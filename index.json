[{"content":"16-Startup-Metrics-Andreessen-Horowitz # #2 Recurring Revenue vs. Total Revenue # Investors more highly value companies where the majority of total revenue comes from product revenue (vs. from services). Contribution margin per customer (per month) = revenue from customer minus variable costs associated with a customer. As the company starts to recognize revenue from the software as service, it reduces its deferred revenue balance and increases revenue: for a 24-month deal, as each month goes by deferred revenue drops by 1/24th and revenue increases by 1/24th. This is why investors consider paid CAC [total acquisition cost/ new customers acquired through paid marketing] to be more important than blended CAC in evaluating the viability of a business — it informs whether a company can scale up its user acquisition budget profitably. Investors look at it the following way:\nMonthly unit churn = lost customers/prior month total Retention by cohort Month 1 = 100% of installed base Latest Month = % of original installed base that are still transacting\nIt is also important to differentiate between gross churn and net revenue churn — Gross churn: MRR lost in a given month/MRR at the beginning of the month. As a reminder, here’s a simple calculation: Monthly cash burn = cash balance at the beginning of the year minus cash balance end of the year / 12 It’s also important to measure net burn vs. gross burn: Net burn [revenues (including all incoming cash you have a high probability of receiving) – gross burn] is the true measure of amount of cash your company is burning every month. Investors like to look at monthly GMV, monthly revenue, or new users/customers per month to assess the growth in early stage businesses.\n","date":"16 February 2023","permalink":"/posts/16-startup-metrics-andreessen-horowitz/","section":"Posts","summary":"16-Startup-Metrics-Andreessen-Horowitz # #2 Recurring Revenue vs.","title":"16-Startup-Metrics-Andreessen-Horowitz"},{"content":"2-How-VC-works-A-Beginner-s-Guide-Simplanatio # Venture capital is the investments of private capital in private companies, ie, companies not listed on the stock exchange. They are ready to invest in a VC fund because of the high returns expected from it compared to other options they have (public equity markets, debt, real estate, etc).\nThe first 2-3 years, Bala\u0026rsquo;s effort goes into identifying and investing in startups The next 3-4 years go into building them The last 2-3 years is when Bala tries to \u0026rsquo;exit\u0026rsquo; the investments and make money Bala can also show that he\u0026rsquo;ll include\nClauses for voting rights on key decisions like selling the company or when to IPO Anti-dilution clauses - if the startup raises the next round of funding at a lower valuation, the # of shares owned by the fund will be adjusted so that the fund continues to own the same % of the startup as before the round When you go through the list of portfolio companies on a VC\u0026rsquo;s website, remember that they could be investments from different funds, each of which have different goals :)\nVC is f**king tough # With high risk comes high returns. #2%20How%20VC%20works%20-%20A%20Beginner\u0026rsquo;s%20Guide%20-%20Simplanatio%20ff137c29a3d94403953d5ffc4f56a4a7/https3A2F2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com2Fpublic2Fimages2F774e5abf-3310-42af-9df1-aaba66f69057_1920x1080.jpeg Continuing Bala\u0026rsquo;s example\u0026hellip; Bala ended up raising $10M and invested that money in 25 companies. To generate a 25% annualized return, Bala\u0026rsquo;s fund has to at least 10x its original size, ie, the sum total of all the investments has to be at $100M (10x over 10 years = 25% year-on-year growth) Now, this would have been simple if all the 25 startups grew 10x in those 4-5 years after Bala invested.\n","date":"16 February 2023","permalink":"/posts/2-how-vc-works-a-beginner-s-guide-simplanatio/","section":"Posts","summary":"2-How-VC-works-A-Beginner-s-Guide-Simplanatio # Venture capital is the investments of private capital in private companies, ie, companies not listed on the stock exchange.","title":"2-How-VC-works-A-Beginner-s-Guide-Simplanatio"},{"content":"2001-03147-Age-Partitioned-Bloom-Filters # [2001.03147] Age-Partitioned Bloom Filters # Created: January 11, 2020 12:51 AM Tags: Data URL: https://arxiv.org/abs/2001.03147\nBloom filters (BF) are widely used for approximate membership queries over a set of elements. BF variants allow removals, sets of unbounded size or querying a sliding window over an unbounded stream. However, for this last case the best current approaches are dictionary based (e.g., based on Cuckoo Filters or TinyTable), and it may seem that BF-based approaches will never be competitive to dictionary-based ones. In this paper we present Age-Partitioned Bloom Filters, a BF-based approach for duplicate detection in sliding windows that not only is competitive in time-complexity, but has better space usage than current dictionary-based approaches (e.g., SWAMP), at the cost of some moderate slack. APBFs retain the BF simplicity, unlike dictionary-based approaches, important for hardware-based implementations, and can integrate known improvements such as double hashing or blocking. We present an Age-Partitioned Blocked Bloom Filter variant which can operate with 2-3 cache-line accesses per insertion and around 2-4 per query, even for high accuracy filters.\n","date":"16 February 2023","permalink":"/posts/2001-03147-age-partitioned-bloom-filters/","section":"Posts","summary":"2001-03147-Age-Partitioned-Bloom-Filters # [2001.","title":"2001-03147-Age-Partitioned-Bloom-Filters"},{"content":"3-Things-to-Avoid-When-Setting-Up-an-Amazon-Redshi # 3 Things to Avoid When Setting Up an Amazon Redshift Cluster | intermix.io # Created: February 6, 2020 2:11 PM URL: https://www.intermix.io/blog/3-mistakes-redshift-setup/ ! 3-Things-to-Avoid-When-Setting-Up-an-Amazon-Redshift-Cluster-2-1024x768.jpg\n","date":"16 February 2023","permalink":"/posts/3-things-to-avoid-when-setting-up-an-amazon-redshi/","section":"Posts","summary":"3-Things-to-Avoid-When-Setting-Up-an-Amazon-Redshi # 3 Things to Avoid When Setting Up an Amazon Redshift Cluster | intermix.","title":"3-Things-to-Avoid-When-Setting-Up-an-Amazon-Redshi"},{"content":"3-tricks-to-start-working-despite-not-feeling-like # 3 tricks to start working despite not feeling like it # Created: January 21, 2020 6:39 PM Tags: Productivity URL: https://www.deprocrastination.co/blog/3-tricks-to-start-working-despite-not-feeling-like-it Ever wish you felt like creating that presentation? It\u0026rsquo;s easy to start when we feel like it. But here\u0026rsquo;s the thing: **we don\u0026rsquo;t have to feel like it to start. **When you get tired of thinking about a piece of work and feeling bad for not finishing it yet, go \u0026ldquo;screw it, let\u0026rsquo;s do it\u0026rdquo; and start with something, anything. **\nStart sloppy # Another trick to start sloppy - you might have high expectations of your finished work. **When you know that you don\u0026rsquo;t have to make the greatest thing ever right from the start, it\u0026rsquo;s easier to start. **\nStart small # You probably don\u0026rsquo;t feel like creating a 20 slide presentation right now from scratch and presenting it in 2 hours.\n","date":"16 February 2023","permalink":"/posts/3-tricks-to-start-working-despite-not-feeling-like/","section":"Posts","summary":"3-tricks-to-start-working-despite-not-feeling-like # 3 tricks to start working despite not feeling like it # Created: January 21, 2020 6:39 PM Tags: Productivity URL: https://www.","title":"3-tricks-to-start-working-despite-not-feeling-like"},{"content":"3-Ways-to-Change-a-Users-Default-Shell-in-Linux # 3 Ways to Change a Users Default Shell in Linux # Created: April 26, 2020 11:45 PM URL: https://www.tecmint.com/change-a-users-default-shell-in-linux/ In this article, we will describe how to change a user’s shell in Linux. Read Also: 5 Most Frequently Used Open Source Shells for Linux There are several reasons for changing a user’s shell in Linux including the following: 1. When creating user accounts with the useradd or adduser utilities, the --shell flag can be used to specify the name of a user’s login shell other than that specified in the respective configuration files.\n1. usermod Utility # usermod is a utility for modifying a user’s account details, stored in the /etc/passwd file and the -s or --shell option is used to change the user’s login shell. In this example, we’ll first check user tecmint’s account information to view his default login shell and then change its login shell from /bin/sh to /bin/bash as follows. 3%20Ways%20to%20Change%20a%20Users%20Default%20Shell%20in%20Linux%207f26076b89524234a53cc7c75eb29d7e/Change-User-Shell-using-Usermod.png Change User Shell using Usermod\n2. chsh Utility # chsh is a command line utility for changing a login shell with the -s or –shell option like this. Change User Shell in /etc/passwd File In this method, simply open the /etc/passwd file using any of your favorite command line text editors and change a specific users shell.\n","date":"16 February 2023","permalink":"/posts/3-ways-to-change-a-users-default-shell-in-linux/","section":"Posts","summary":"3-Ways-to-Change-a-Users-Default-Shell-in-Linux # 3 Ways to Change a Users Default Shell in Linux # Created: April 26, 2020 11:45 PM URL: https://www.","title":"3-Ways-to-Change-a-Users-Default-Shell-in-Linux"},{"content":"311-OPCD-Calls-2012-Present-Socrata-API-Foundry # 311 OPCD Calls (2012-Present) | Socrata API Foundry # Created: April 6, 2020 9:29 AM URL: https://dev.socrata.com/foundry/data.nola.gov/2jgv-pqrq\n","date":"16 February 2023","permalink":"/posts/311-opcd-calls-2012-present-socrata-api-foundry/","section":"Posts","summary":"311-OPCD-Calls-2012-Present-Socrata-API-Foundry # 311 OPCD Calls (2012-Present) | Socrata API Foundry # Created: April 6, 2020 9:29 AM URL: https://dev.","title":"311-OPCD-Calls-2012-Present-Socrata-API-Foundry"},{"content":"4-questions-to-help-accurately-scope-analytics-eng # One common example: your company might have marketing data stored in Marketo, but it is ingested into your data warehouse through an existing Salesforce integration. The two-hop integration will have more opportunities for failure, and the data will likely not be as granular or complete as the primary source data. According to Kimball, it’s important to “stick to rich, expressive, atomic-level data that’s closely connected to the original source and collection process.” I couldn’t agree more: granular data gives you options, aggregated data is limiting. One of the most common examples of this in practice is the difference between Google Analytics data exported via the API and Google Analytics 360 data loaded automatically into Bigquery. Ideally, you want to have access to the absolute most granular data available from the source data system. It’s important to actually learn about the source data systems whose data you’re interacting with. As you bring each one of these systems into your pipeline and your data model, you’ll need at least one key to join the data in.\n","date":"16 February 2023","permalink":"/posts/4-questions-to-help-accurately-scope-analytics-eng/","section":"Posts","summary":"4-questions-to-help-accurately-scope-analytics-eng # One common example: your company might have marketing data stored in Marketo, but it is ingested into your data warehouse through an existing Salesforce integration.","title":"4-questions-to-help-accurately-scope-analytics-eng"},{"content":"A-2020-Vision-of-Linear-Algebra-MIT-OpenCourseWare # A 2020 Vision of Linear Algebra | MIT OpenCourseWare # Created: May 12, 2020 6:46 AM URL: https://ocw.mit.edu/resources/res-18-010-a-2020-vision-of-linear-algebra-spring-2020/ ! A%202020%20Vision%20of%20Linear%20Algebra%20MIT%20OpenCourseWare%200d5c26ec03b346699239ca248c093d33/RES-18-010S20.jpg Professor Strang introduces his new vision of how to teach linear algebra. (Image by MIT OCW)\nResource Features # Course Description # These six brief videos, recorded in 2020, contain ideas and suggestions from Professor Strang about the recommended order of topics in teaching and learning linear algebra. The key point is to start right in with the columns of a matrix A and the multiplication Ax that combines those columns. That leads to The Column Space of a Matrix and the idea of independent columns and the factorization A = CR that tells so much about A. The remaining videos outline very briefly the full course: The Big Picture of Linear Algebra; Orthogonal Vectors; Eigenvalues \u0026amp; Eigenvectors; and Singular Values \u0026amp; Singular Vectors. You can see this new idea developing in the first video lecture of Professor Strang’s 2019 course 18.065 Matrix Methods in Data Analysis, Signal Processing, and Machine Learning.\n","date":"16 February 2023","permalink":"/posts/a-2020-vision-of-linear-algebra-mit-opencourseware/","section":"Posts","summary":"A-2020-Vision-of-Linear-Algebra-MIT-OpenCourseWare # A 2020 Vision of Linear Algebra | MIT OpenCourseWare # Created: May 12, 2020 6:46 AM URL: https://ocw.","title":"A-2020-Vision-of-Linear-Algebra-MIT-OpenCourseWare"},{"content":"A-Beginner-s-Guide-to-Data-Engineering-Part-II # From collecting raw data and building data warehouses to applying Machine Learning, we saw why data engineering plays a critical role in all of these areas. First, I will introduce the concept of Data Modeling, a design process where one carefully defines table schemas and data relations to capture business metrics and dimensions. **\nData Modeling, Normalization, and Star Schema # To give an example of the design decisions involved, we often need to decide the extent to which tables should be normalized. This design focuses on building normalized tables, specifically fact and dimension tables. The star schema organized table in a star-like pattern, with a fact table at the center, surrounded by dim tables\nFact \u0026amp; Dimension Tables # To understand how to build denormalized tables from fact tables and dimension tables, we need to discuss their respective roles in more detail:\nFact tables typically contain point-in-time transactional data. Below is a simple example of how fact tables and dimension tables (both are normalized tables) can be joined together to answer basic analytics question such as how many bookings occurred in the past week in each market. Shrewd users can also imagine that if additional metrics m_a, m_b, m_c and dimensions dim_x, dim_y, dim_z are projected in the final SELECT clause, a denormalized table can be easily built from these normalized tables. ","date":"16 February 2023","permalink":"/posts/a-beginner-s-guide-to-data-engineering-part-ii/","section":"Posts","summary":"A-Beginner-s-Guide-to-Data-Engineering-Part-II # From collecting raw data and building data warehouses to applying Machine Learning, we saw why data engineering plays a critical role in all of these areas.","title":"A-Beginner-s-Guide-to-Data-Engineering-Part-II"},{"content":"A-Guide-for-Customer-Retention-Analysis-with-SQL # A Guide for Customer Retention Analysis with SQL - Stats and Bots # Created: February 21, 2020 12:21 PM URL: https://blog.statsbot.co/customer-retention-analysis-93af9daee46b\n","date":"16 February 2023","permalink":"/posts/a-guide-for-customer-retention-analysis-with-sql/","section":"Posts","summary":"A-Guide-for-Customer-Retention-Analysis-with-SQL # A Guide for Customer Retention Analysis with SQL - Stats and Bots # Created: February 21, 2020 12:21 PM URL: https://blog.","title":"A-Guide-for-Customer-Retention-Analysis-with-SQL"},{"content":"A-Map-of-Every-Building-in-America-The-New-York # A Map of Every Building in America - The New York Times # Created: December 4, 2019 10:25 AM Tags: Data, Maps URL: https://www.nytimes.com/interactive/2018/10/12/us/map-of-every-building-in-the-united-states.html ! On this page you will find maps showing almost every building in the United States. Every black speck on the map below is a building, reflecting the built legacy of the United States.\nHistory Made Apparent # The nation’s expansion shows itself: The clustered development of the original colonies flowed west, with scattered cities and towns linked, like beads on a string, by rivers, highways and railroads.\nTraces of Distant Culture # South of New Orleans This arrangement of buildings along a narrow spit of land on either side of a Louisiana bayou shows the imprint of the region’s history under France: “long lot” development, which stretched skinny holdings laterally away from important waterways.\nA City, by Design # Washington, D.C. Many cities evolve over time, but some are designed from scratch. Unlike places such as the Flint area — with its carefully plotted grids — places like the landscape east of Lancaster, Pa., have a long history of organic settlement.\n","date":"16 February 2023","permalink":"/posts/a-map-of-every-building-in-america-the-new-york/","section":"Posts","summary":"A-Map-of-Every-Building-in-America-The-New-York # A Map of Every Building in America - The New York Times # Created: December 4, 2019 10:25 AM Tags: Data, Maps URL: https://www.","title":"A-Map-of-Every-Building-in-America-The-New-York"},{"content":"A-New-Data-Analytics-Company-From-A-Boisterous-Sta # A%20New%20Data%20Analytics%20Company%20From%20A%20Boisterous%20Sta%2081c5dbbaca8e432dbaee5e2580afb856/960x0.jpg Peter Bailis, founder and CEO of Sisu Peter Bailis can’t resist dropping more than a dozen f-bombs as he explains how his startup Sisu aims to upend traditional business intelligence and reshape the way companies act on the massive amounts of information—about products, sales, customers, etc.—that they generate and store. “It’s a product problem plus a tech problem.” While an endless number of tools help companies store, sort and visualize their tsunami of data, he says Sisu goes a step further. “And no one can match our speed or the result quality.” Bailis, a former Stanford researcher and assistant professor currently on leave, founded Sisu last year, but the so-called operational analytics company is emerging from stealth Wednesday with a $52.5 million Series B round of funding led by NEA, with participation from Andreessen Horowitz and Green Bay Ventures, bringing its total funding to $66.7 million. Instead, it processes the massive amounts of data related to a given goal (increase conversion rates) and ranks which factors are currently affecting it, effectively telling companies why a metric is changing. A%20New%20Data%20Analytics%20Company%20From%20A%20Boisterous%20Sta%2081c5dbbaca8e432dbaee5e2580afb856/960x0%201.jpg Sisu customers can, for example, drill down on which factors are affecting a key performance \u0026hellip; [+] Ben Horowitz, who led his firm’s investments in Sisu’s Series A and B, says that both Bailis’ tech chops and his approach to making machine learning usable, impressed him. “There are two ways companies are going about the market: They’re either basically building more graphs and pretty pictures, or they’re applying deep learning for predictions,” Horowitz says, “What Sisu built doesn’t look like anything else out there: It just shows you why things are happening.” Gartner analyst W. Roy Schulte says that the old approach to business intelligence, where analysts check dashboards once a day, has evolved as storing and analyzing data has become more inexpensive and companies can pump out reports in nearly real-time. The name of the company comes from a Finnish concept that roughly translates to tenacity and determination, which Bailis says captures the both the technological hurdle of sorting through massive amounts of data as well as the platform’s objectives.\n","date":"16 February 2023","permalink":"/posts/a-new-data-analytics-company-from-a-boisterous-sta/","section":"Posts","summary":"A-New-Data-Analytics-Company-From-A-Boisterous-Sta # A%20New%20Data%20Analytics%20Company%20From%20A%20Boisterous%20Sta%2081c5dbbaca8e432dbaee5e2580afb856/960x0.","title":"A-New-Data-Analytics-Company-From-A-Boisterous-Sta"},{"content":"Ab-testing # Ab testing # Created: July 17, 2020 11:43 PM URL: https://engineering.upside.com/playbook-for-practical-product-experimentation-ee98e5e31860 playbook-for-practical-product-experimentation-ee98e5e31860\n","date":"16 February 2023","permalink":"/posts/ab-testing/","section":"Posts","summary":"Ab-testing # Ab testing # Created: July 17, 2020 11:43 PM URL: https://engineering.","title":"Ab-testing"},{"content":"Adding-Faust-to-your-Existing-Architecture-Robin # Adding Faust to your Existing Architecture - Robinhood Engineering # Created: June 12, 2020 10:58 PM URL: https://robinhood.engineering/adding-faust-to-your-existing-architecture-39b0ebd1f7c9 A few weeks ago we open sourced Faust, a Python stream processing library that we built at Robinhood to make it extremely easy to build and deploy traditionally complex streaming architectures. We can install aredis and Faust using pip:\npip install aredis pip install faust Upon installing the dependencies, let’s first define our Faust application, Kafka topic and models:\nimport datetime import faustclass Activity(faust.Record, isodates=True): user: str message: str timestamp: datetime.datetimeapp = faust.App(\u0026#34;redis_example\u0026#34;, broker=\u0026#34;kafka://localhost:9092\u0026#34;) activities_topic = app.topic(\u0026#34;feed_activities\u0026#34;, value_type=Activity) We can now create an agent which reads feed activities coming in through this topic, and adds the messages to the user’s Redis sorted set as follows:\n@app.agent(activities_topic) async def save_activities(activities): async for activity in activities: client = aredis.StrictRedis(host=\u0026#34;localhost\u0026#34;, port=6379) await client.zadd(activity.user, activity.timestamp, activity.message) As shown above, we use Redis as you would use it in any app. Below is an example of how we use the Python aiohttp library from a Faust streaming app for one of our use cases at Robinhood. We create an agent which processes orders and uses a third part HTTP API to send order confirmation emails to our customers:\nasync def send_confirmation(order): url = f\u0026#34;https://emailer.robinhood.com/\u0026#34; data = { \u0026#34;user\u0026#34;: order.user_id, \u0026#34;subject\u0026#34;: \u0026#34;Order Confirmation\u0026#34;, \u0026#34;body\u0026#34; f\u0026#34;Order: {order.quantity} shares of {order.symbol}\u0026#34;, } async with aiohttp.ClientSession() as session: await session.post(url, json=data)@app.agent(orders_topic) async def add_symbol(orders): async for order in orders: await send_confirmation(order) A lot of our internal services export REST APIs. Again, as before, let us install the Python library we will use to query InfluxDB:\npip install aioinflux We now create an agent which looks at the orders topic from above and looks at the time series in InfluxDB for the particular stock to get the price at which the order executed was the price in the market at the time.\n@app.agent(orders_topic) async def add_symbol(orders): async for order in orders: client = aioinflux.InfluxDBClient() query = f\u0026#34;SELECT price FROM marketdata WHERE symbol = {order.symbol} AND timestamp \u0026lt;= {order.timestamp} ORDER BY DESC LIMIT 1\u0026#34; order.market_price = await client.query(query) await quality_topic.send(key=order.id, value=order) The ability to easily integrate streaming apps with InfluxDB lets us solve problems where we need to work with multiple time series.\nasync def get_top_result(query_string): client = elasticsearch_async.AsyncElasticsearch() query = {\u0026#34;query\u0026#34;: query_string} resp = await client.search(index=\u0026#34;search_index\u0026#34;, body=query) return resp[\u0026#34;hits\u0026#34;][\u0026#34;hits\u0026#34;]@app.agent(search_queries_topic) async def add_top_search_result(search_queries): async for query in search_queries: query.top_result = await get_top_result(query.query_string) await top_results_topic.send(key=query.top_result, value=query) Faust makes it easy for us to build architectures where we use Elasticsearch as our data store of choice.\n","date":"16 February 2023","permalink":"/posts/adding-faust-to-your-existing-architecture-robin/","section":"Posts","summary":"Adding-Faust-to-your-Existing-Architecture-Robin # Adding Faust to your Existing Architecture - Robinhood Engineering # Created: June 12, 2020 10:58 PM URL: https://robinhood.","title":"Adding-Faust-to-your-Existing-Architecture-Robin"},{"content":"Advanced-PostgreSQL-Data-Types # The implementation may vary somewhat between systems, but generally there are standard ways you’ll want to process and analyze these types of data (e.g. perform mathematical calculations, find the length of a character string, cast from one type to another, etc).\nArray type # Arrays are likely something familiar, but in case you’re new to programming: it’s a data type meant to hold a collection of things. In Postgres, however, the array elements must all be of the same type - the table definition alludes to it:\nCREATE TABLE countries_visited ( person_name text, countries char(2)[] ); As we can see above with the countries column, the array declaration must have the type name of the values that array will contain. Another good rule of thumb might be, if there are places in your application code where you’re using arrays and you often find yourself fetching entire data sets, storing the data as an array type could save you one more join against a lookup table.\nRange type # You might say that a “range” can describe some set of values, i.e. when something is “within a range,” it is part of that set. Our second course on data types in the Crunchy Data interactive learning portal focuses on the above three types plus XML. It lets you play around with these data types with a little sample data, and it also introduces you to some helpful functions and operators that come with these data types.\n","date":"16 February 2023","permalink":"/posts/advanced-postgresql-data-types/","section":"Posts","summary":"Advanced-PostgreSQL-Data-Types # The implementation may vary somewhat between systems, but generally there are standard ways you’ll want to process and analyze these types of data (e.","title":"Advanced-PostgreSQL-Data-Types"},{"content":"Agile-as-Trauma-Dorian-Taylor # Agile as Trauma — Dorian Taylor # Created: October 16, 2021 12:37 AM URL: https://doriantaylor.com/agile-as-trauma agile-as-trauma\n","date":"16 February 2023","permalink":"/posts/agile-as-trauma-dorian-taylor/","section":"Posts","summary":"Agile-as-Trauma-Dorian-Taylor # Agile as Trauma — Dorian Taylor # Created: October 16, 2021 12:37 AM URL: https://doriantaylor.","title":"Agile-as-Trauma-Dorian-Taylor"},{"content":"AIP-Improvement-Proposals # AIP Improvement Proposals # Created: July 4, 2021 8:25 PM URL: https://google.aip.dev/\nWelcome # AIPs are design documents that summarize Google\u0026rsquo;s API design decisions.\nCurious about the basics? # AIPs are a combination of design guidance and a system we use to manage and track that guidance. Learn more about how the AIP program works in the first AIP! Contribute to the project »\nWant to use AIPs for your organization? # AIPs are designed to be useful outside of Google. Learn more » Except as otherwise noted, the content of this page is licensed under the Creative Commons Attribution 4.0 License, and code samples are licensed under the Apache 2.0 License.\n","date":"16 February 2023","permalink":"/posts/aip-improvement-proposals/","section":"Posts","summary":"AIP-Improvement-Proposals # AIP Improvement Proposals # Created: July 4, 2021 8:25 PM URL: https://google.","title":"AIP-Improvement-Proposals"},{"content":"AirflowETL # Airflow is a platform to schedule and monitor workflows and in this post I will show you how to use it to extract the daily weather in New York from the OpenWeatherMap API, convert the temperature to Celsius and load the data in a simple PostgreSQL database. Specifically I transform and load the following into the database,\nthe city name the country name the latitude and longitude of the city the date the API call was made the humidity the pressure the minimum temperature of the day the maximum temperature of the day the current temperature a description of the weather This script is stored in a file name makeTable.py and can be run using the command, python makeTable.py From the appropriate directory and before we set up our Airflow job . We can now install airflow with PostgreSQL using pip:\npip install airflow[postgres] We then initialize the metadata database by typing,\nairflow initdb Out of the box, Airflow uses a SQLite database, which you should outgrow fairly quickly since no parallelization is possible using this database backend.\nAn Example ETL Pipeline With Airflow # Let\u0026rsquo;s go over an example of an Airflow DAG to that calls the OpenWeatherMap API daily to get weather in Brooklyn, NY and stores the data in the Postgres database that we created. First you can see if there is Python syntax error by \u0026ldquo;compiling it,\u0026rdquo;\npython dag_def_.py You can then test an individual task within a dag by using the command,\nairflow test You can also test the whole dag by doing a backfill,\nAirflow backfill -s -e Sometimes, in order to notify Airflow of an update you may need to delete the .pyc files or even the DAGs themselves. If you need to delete a dag, first delete the DAG data from the metadata_db database:\nUse the UI -\u0026gt; Browse -\u0026gt; Dag Runs -\u0026gt; Then delete them all. Then you can delete DAGs by clearing the task instance states:\nairflow clear Airflow is an extremely useful tool for building data pipelines and scheduling jobs in Python.\n","date":"16 February 2023","permalink":"/posts/airflowetl/","section":"Posts","summary":"AirflowETL # Airflow is a platform to schedule and monitor workflows and in this post I will show you how to use it to extract the daily weather in New York from the OpenWeatherMap API, convert the temperature to Celsius and load the data in a simple PostgreSQL database.","title":"AirflowETL"},{"content":"Algorithms-interviews-theory-vs-practice # Algorithms interviews: theory vs. practice # Created: May 23, 2020 11:28 AM URL: https://danluu.com/algorithms-interviews/ When I ask people at trendy big tech companies why algorithms quizzes are mandatory, the most common answer I get is something like \u0026ldquo;we have so much scale, we can\u0026rsquo;t afford to have someone accidentally write an O(n^2) algorithm and bring the site down\u0026rdquo;1. Both the examples in this post as well as the ones I haven’t included have these properties:\nThe example could be phrased as an interview question If phrased as an interview question, you\u0026rsquo;d expect most (and probably) all people on the relevant team to get the right answer in the timeframe of an interview The cost savings from fixing the example is worth more annually than my lifetime earnings to date The example persisted for long enough that it\u0026rsquo;s reasonable to assume that it wouldn\u0026rsquo;t have been discovered otherwise At the start of this post, we noted that people at big tech companies commonly claim that they have to do algorithms interviews since it\u0026rsquo;s so costly to have inefficiencies at scale. I actually worked at a company that used the strategy of \u0026ldquo;don\u0026rsquo;t ask algorithms questions in interviews, but do incentivize things that are globally good for the company\u0026rdquo;. Another trend at the time was for behavioral interviews and a number of companies I interviewed with had 100% behavioral interviews with zero technical interviews. Some companies will give very large out-of-band bonuses to people regularly, but that work wasn\u0026rsquo;t for a company that does a lot of that, so there\u0026rsquo;s nothing the company could do to indicate that it valued additional work once someone did \u0026ldquo;enough\u0026rdquo; work to get the best possible rating on a performance review. But even for companies that do, most people don\u0026rsquo;t have jobs where they\u0026rsquo;re designing high-scale algorithms (maybe they did at Google circa 2003, but from what I\u0026rsquo;ve seen at three different big tech companies, most people\u0026rsquo;s jobs are pretty light on algorithms work). [return] The reason it\u0026rsquo;s arguably zero is that the only software interview where I inarguably got a \u0026ldquo;real\u0026rdquo; interview and was coming in cold was at Google, but that only happened because the interviewers that were assigned interviewed me for the wrong ladder \u0026ndash; I was interviewing for a hardware position, but I was being interviewed by software folks, so I got what was basically a standard software interview except that one interviewer asked me some questions about state machine and cache coherence (or something like that). ","date":"16 February 2023","permalink":"/posts/algorithms-interviews-theory-vs-practice/","section":"Posts","summary":"Algorithms-interviews-theory-vs-practice # Algorithms interviews: theory vs.","title":"Algorithms-interviews-theory-vs-practice"},{"content":"Amazon-Redshift-introduces-support-for-materialize # Amazon Redshift introduces support for materialized views (preview) # Created: January 28, 2020 8:27 PM Tags: Data URL: https://aws.amazon.com/about-aws/whats-new/2019/11/amazon-redshift-introduces-support-for-materialized-views-preview/\n","date":"16 February 2023","permalink":"/posts/amazon-redshift-introduces-support-for-materialize/","section":"Posts","summary":"Amazon-Redshift-introduces-support-for-materialize # Amazon Redshift introduces support for materialized views (preview) # Created: January 28, 2020 8:27 PM Tags: Data URL: https://aws.","title":"Amazon-Redshift-introduces-support-for-materialize"},{"content":"America-s-Patriotic-Victory-Gardens-HISTORY # In March of 1917¬—just weeks before the United States entered the war—Charles Lathrop Pack organized the National War Garden Commission to encourage Americans to contribute to the war effort by planting, fertilizing, harvesting and storing their own fruits and vegetables so that more food could be exported to our allies. Promoted through propaganda posters advocating that civilians “Sow the seeds of victory” by planting their own vegetables, the war garden movement (as it was originally known) was spread by word of mouth through numerous women’s clubs, civic associations and chambers of commerce, which actively encouraged participation in the campaign. In addition to the appeal to men and women, the federal Bureau of Education initiated a U.S. School Garden Army (USSGA) to mobilize children to enlist as “soldiers of the soil.” As a result of these combined efforts, 3 million new garden plots were planted in 1917 and more than 5.2 million were cultivated in 1918, which generated an estimated 1.45 million quarts of canned fruits and vegetables. Through the distribution of several million government-sponsored pamphlets, fledgling farmers were advised to maximize their garden’s productivity by practicing succession planting, and were encouraged to record the germination rates of seeds, along with any diseases or insects they may have encountered, in order to minimize waste and improve their garden’s output the following year. Throughout both world wars, the Victory Garden campaign served as a successful means of boosting morale, expressing patriotism, safeguarding against food shortages on the home front, and easing the burden on the commercial farmers working arduously to feed troops and civilians overseas. In 1942, roughly 15 million families planted victory gardens; by 1944, an estimated 20 million victory gardens produced roughly 8 million tons of food—which was the equivalent of more than 40 percent of all the fresh fruits and vegetables consumed in the United States. Although the government’s promotion of victory gardens ended with the war, a renaissance movement has sprouted up in recent years in support of self-sufficiency and eating seasonally to improve health through local, organic farming and sustainable agriculture.\n","date":"16 February 2023","permalink":"/posts/america-s-patriotic-victory-gardens-history/","section":"Posts","summary":"America-s-Patriotic-Victory-Gardens-HISTORY # In March of 1917¬—just weeks before the United States entered the war—Charles Lathrop Pack organized the National War Garden Commission to encourage Americans to contribute to the war effort by planting, fertilizing, harvesting and storing their own fruits and vegetables so that more food could be exported to our allies.","title":"America-s-Patriotic-Victory-Gardens-HISTORY"},{"content":"American-Trestle-Table-Popular-Woodworking-Magazin # American%20Trestle%20Table%20Popular%20Woodworking%20Magazin%2042d9ddc982d540d0983ec53791c56f15/American_Trestle_Table_Page_1_Image_0001-300x237.jpg *One of the oldest designs for a dining table is also one of the most highly engineered and contemporary. Still, despite their spare charm and long history, there are some things about the dimensions of trestle tables that don’t conform to our typical expectations for tables. American%20Trestle%20Table%20Popular%20Woodworking%20Magazin%2042d9ddc982d540d0983ec53791c56f15/American_Trestle_Table_Page_2_Image_0001-150x150.jpg And the trestle form frequently looks quite fragile, which seems at odds with the fact that these tables are typically the centerpiece of a casual dining area. For many years, I’ve wanted to build a trestle table to replace the store-bought, white-pine apron table my wife and I got soon after college. The pine apron table was a testament to everything I disliked about commercial furniture: The top was pieced together using narrow, knotty and poorly matched boards. American%20Trestle%20Table%20Popular%20Woodworking%20Magazin%2042d9ddc982d540d0983ec53791c56f15/American_Trestle_Table_Page_2_Image_0002-150x150.jpg But if I hated our dining table, then I was equally afraid of the trestle table I wanted to build, which looked narrow, tippy and ready to collapse, so I put it off for 15 years. Each end of a trestle table has a foot, leg and brace.\n","date":"16 February 2023","permalink":"/posts/american-trestle-table-popular-woodworking-magazin/","section":"Posts","summary":"American-Trestle-Table-Popular-Woodworking-Magazin # American%20Trestle%20Table%20Popular%20Woodworking%20Magazin%2042d9ddc982d540d0983ec53791c56f15/American_Trestle_Table_Page_1_Image_0001-300x237.","title":"American-Trestle-Table-Popular-Woodworking-Magazin"},{"content":"And-voila-Jupyter-Blog # The execution model of voilà is the following: upon connection to a notebook URL, voilà launches the kernel for that notebook, and runs all the cells as it populates the notebook model with the outputs. Voilà can render custom Jupyter widget libraries, including (but not limited to) bqplot, ipyleafet, ipyvolume, ipympl, ipysheet, plotly, ipywebrtc, etc. Together with ipympl, voilà is actually a simple means to render interactive matplotlib figures in a standalone web application: Voilà can be used to produce applications with any Jupyter kernel. A voilà template is actually a folder placed in the standard directoryPREFIX/share/jupyter/voila/templates and which may include\nnbconvert templates (the jinja templates used to transform the notebook into HTML) static resources custom tornado templates such as 404.html etc. The directory structure for a voilà template is the following: PREFIX/share/jupyter/voila/templates/template_name/|├── conf.json # Template configuration file├── nbconvert_templates/ # Custom nbconvert templates├── static/ # Static directory└── templates/ # Custom tornado templates The voilà template system can be used to completely override the behavior of the front-end. * Beyond the voila command-line utility, the voilà package also include a Jupyter server extension, so that voilà dashboards can be served alongside the Jupyter notebook application. Current work streams include better integration with JupyterHub for publicly sharing dashboard between users, as well as JupyterLab extensions (a voilà \u0026ldquo;preview\u0026rdquo; extension for notebooks, and a WYSIWYG editor for dashboard layouts).\n","date":"16 February 2023","permalink":"/posts/and-voila-jupyter-blog/","section":"Posts","summary":"And-voila-Jupyter-Blog # The execution model of voilà is the following: upon connection to a notebook URL, voilà launches the kernel for that notebook, and runs all the cells as it populates the notebook model with the outputs.","title":"And-voila-Jupyter-Blog"},{"content":"Andrew-Chen-on-Marketplaces # Andrew Chen on Marketplaces # Created: May 25, 2020 1:08 AM URL: about:reader?url=https%3A%2F%2Fstripe.com%2Fatlas%2Fguides%2Fandrew-chen-marketplaces\n","date":"16 February 2023","permalink":"/posts/andrew-chen-on-marketplaces/","section":"Posts","summary":"Andrew-Chen-on-Marketplaces # Andrew Chen on Marketplaces # Created: May 25, 2020 1:08 AM URL: about:reader?","title":"Andrew-Chen-on-Marketplaces"},{"content":"Apache-Airflow-on-Docker-for-Complete-Beginners # But over time, OLAP usually starts to become too burdensome to run on your production tables:\nOLAP queries are more computationally expensive (aggregations, joins) OLAP often requires intermediate steps like data cleaning and featurization Analytics usually runs at regular time intervals, while OLTP is usually event based (e.g. a user does something, so we hit the database) For these reasons and more, Bill Inmon, Barry Devlin, and Paul Murphy developed the concept of a Data Warehouse in the 1980’s. Some of the major issues that data teams face scheduling ETL jobs with Cron: ETL jobs fail all the time for a million reasons, and Cron makes it very difficult to debug for a number of reasons ETL tends to have a lot of dependencies (on past jobs, for example) and Cron isn’t built to account for that Data is getting larger, and modern distributed data stacks (HDFS, Hive, Presto) don’t always work well with Cron Unsurprisingly, data teams have been trying to find more sophisticated ways to schedule and run ETL jobs. Airflow lets you: Design complex, sophisticated ETL jobs with multiple layers of dependencies Schedule jobs to run at any time and wait for their dependencies to finish, with the option to run distributed workloads (using Celery) Monitor all of your jobs, know exactly where and when they failed, and get detailed logs on what the errors were It’s important to note that Airflow is not an ETL tool, even though that happens to be what it’s used most for. Some really clutch features that you might find yourself using: Seeing the structure of your DAG in a graph format Checking on all of your DAG runs and seeing when they failed Adding and editing Connections Looking at how long your tasks typically take in one of your DAGs The Webserver definitely gives Cron a run for its money, and is probably the most compelling feature Airflow has to offer over Cron for beginners. It also tells the container to immediately run airflow initdb, airflow webserver, and airflow scheduler, so you don’t have to run those manually. 2) Decide on how you want to deploy and test The typical structure for building the Airflow Docker Image, which is also how this repo is designed, is two fold: A bunch of local files exist that are your “source of truth” When Docker Images are built, all of the local data and files are copied over to containers What this means that you’d make edits to any local files like script/entrypoint.sh or config/airflow.cfg, and then build your image. Here’s the way that puckel/docker-airflow does it: The Fernet Key is generated with a Python command and exported as an environment variable in entrypoint.sh In the airflow.cfg file, the fernet_key parameter is set to $FERNET_KEY, which is the name of the environment variable exported above There’s an open issue (created by yours truly) with this process, though: if you build your Docker Image through plain old Docker Build and Docker Run commands, it doesn’t work. ","date":"16 February 2023","permalink":"/posts/apache-airflow-on-docker-for-complete-beginners/","section":"Posts","summary":"Apache-Airflow-on-Docker-for-Complete-Beginners # But over time, OLAP usually starts to become too burdensome to run on your production tables:","title":"Apache-Airflow-on-Docker-for-Complete-Beginners"},{"content":"API-Description-XKCD-AnyAPI-Documentation # API Description - XKCD | AnyAPI Documentation # Created: June 15, 2020 8:46 PM Tags: Api, Documentation, Technical writing URL: https://any-api.com/xkcd_com/xkcd_com/docs/API_Description\n","date":"16 February 2023","permalink":"/posts/api-description-xkcd-anyapi-documentation/","section":"Posts","summary":"API-Description-XKCD-AnyAPI-Documentation # API Description - XKCD | AnyAPI Documentation # Created: June 15, 2020 8:46 PM Tags: Api, Documentation, Technical writing URL: https://any-api.","title":"API-Description-XKCD-AnyAPI-Documentation"},{"content":"API-Routing-Layers-in-fintech # Some examples of this are Alloy (aggregating several identity verification vendors and enabling you to access them from a single API) and Payitoff (aggregating several student loan programs and enabling you to access them all from a single API). For the fintech building on an routing layer, you might pay a slightly higher per unit price to consume the service (because the routing layer marks up the cost of the underlying vendor) but the overall organizational cost is lower, simply because you don’t have to distort your org by taking on non-core work. API%20Routing%20Layers%20in%20fintech%208184cab869324516bfc6424774a9a74d/segment.png Segment was the first API routing layer I remember seeing.\nOne API, multiple vendors # The common thread amongst API routing layers I’ve seens is that they enable a single integration, that allows you to programmatically access multiple vendors without having to do an additional integration. API%20Routing%20Layers%20in%20fintech%208184cab869324516bfc6424774a9a74d/alloy.png\nAdvantages for early stage product teams # When done well, an API routing layer gives an early stage product team material advantages. Advantages:\nEasily switch between KYC vendors Reduce costs through switching Increasing pass rates by running customers through multiple vendors Companies: Alloy Payroll linking \u0026amp; income verification Aggregated Services: ADP, Gusto, Paylocity, Gusto, Trinet, Paychex, Paycor. Some financial services subdomains with no routing layers include: Card manufacturing Idemia, CPI, G\u0026amp;D, Perfect Plastic, Arroweye Advantages: faster card delivery, redundancy Merchant payment processing Stripe, Adyen, Braintree, Worldpay, Globalpayments, Bolt Advantages: cost, acceptance, redundancy Credit checks for loan underwriting Vendors: Experian, Transunion, Equifax Advantages: cost, coverage Bank account linking Plaid, Yodlee, Quovo, Finicity Advantages: Coverage, reliability Bank account issuing Every BAAS platform: Q2, Marqeta, Synapse, Galileo, i2c, Advantages: Issuing cost, Reliability Bank underwriting Radius, Evolve, Regions, Sutton Advantages: bank account portability Outside of financial services, there are a few gargantuan domains, such as healthcare, construction, logistics, natural resources, and others that have a high degree of fragmentation and variability between legacy vendors. ","date":"16 February 2023","permalink":"/posts/api-routing-layers-in-fintech/","section":"Posts","summary":"API-Routing-Layers-in-fintech # Some examples of this are Alloy (aggregating several identity verification vendors and enabling you to access them from a single API) and Payitoff (aggregating several student loan programs and enabling you to access them all from a single API).","title":"API-Routing-Layers-in-fintech"},{"content":"Ask-HN-Burning-Out-Hacker-News # Non-founder team lead is unhappy some minor issues have slipping in the chaotic process of getting this to market. Founders are very please, but getting poor feedback from team lead. Team lead has hindered more than helped, gives conflicting advice, blows hot and cold, has created a toxic environment etc. Admittedly some minor things have slipped through the cracks, but much of this comes from my team lead playing politics, creating silos and conflicts etc. My team lead is grinding me down with their constant nitpicking and I really just want to go and do something else, anything else, work in a bar or something. Lots of others have take issue with my team lead as well, so its not just me. I don’t think I’d come across very well (or as sharp as I usually) in interviews at the moment without a break.\n","date":"16 February 2023","permalink":"/posts/ask-hn-burning-out-hacker-news/","section":"Posts","summary":"Ask-HN-Burning-Out-Hacker-News # Non-founder team lead is unhappy some minor issues have slipping in the chaotic process of getting this to market.","title":"Ask-HN-Burning-Out-Hacker-News"},{"content":"Ask-HN-Do-you-maintain-a-log-of-what-you-learn-How # Ask HN: Do you maintain a log of what you learn? # How often do you go over them? | Hacker News Created: May 11, 2020 4:40 PM Tags: Learning, Writing URL: https://news.ycombinator.com/item?id=23142252\n","date":"16 February 2023","permalink":"/posts/ask-hn-do-you-maintain-a-log-of-what-you-learn-how/","section":"Posts","summary":"Ask-HN-Do-you-maintain-a-log-of-what-you-learn-How # Ask HN: Do you maintain a log of what you learn?","title":"Ask-HN-Do-you-maintain-a-log-of-what-you-learn-How"},{"content":"Ask-HN-Name-one-idea-that-changed-your-life-Hacker # Ask HN: Name one idea that changed your life | Hacker News # Created: May 6, 2020 9:25 PM URL: https://news.ycombinator.com/item?id=23092657 Nick Cave, on writer\u0026rsquo;s block: \u0026ldquo;My advice to you is to change your basic relationship to songwriting. You are not the ‘Great Creator’ of your songs, you are simply their servant, and the songs will come to you when you have adequately prepared yourself to receive them. They are not inside you, unable to get out; rather, they are outside of you, unable to get in.\u0026rdquo; https://www.theredhandfiles.com/do-u-have-any-spare-lyrics/\n","date":"16 February 2023","permalink":"/posts/ask-hn-name-one-idea-that-changed-your-life-hacker/","section":"Posts","summary":"Ask-HN-Name-one-idea-that-changed-your-life-Hacker # Ask HN: Name one idea that changed your life | Hacker News # Created: May 6, 2020 9:25 PM URL: https://news.","title":"Ask-HN-Name-one-idea-that-changed-your-life-Hacker"},{"content":"Ask-HN-What-makes-a-good-technical-leader-any-re # Ask HN: What makes a good technical leader – any recommended books? # | Hacker News Created: July 7, 2020 12:51 PM Tags: Leadership, Technical writing URL: https://news.ycombinator.com/item?id=23759547 I just became a technical lead (spiced with a little product manager) for a team who is working with machine-learning/deep-learning technologies (I have ~6 years of background in this field). I feel like I am performing well, but there is a lot of room for improvement on: how to plan for the future, how to communicate success, how to assign engineers and researchers to tasks, how to define tasks, etc. Do you have any recommendation on what should I do, read to become better and better every day? I want my team to be successful, and to show the improvements we make to the company.\n","date":"16 February 2023","permalink":"/posts/ask-hn-what-makes-a-good-technical-leader-any-re/","section":"Posts","summary":"Ask-HN-What-makes-a-good-technical-leader-any-re # Ask HN: What makes a good technical leader – any recommended books?","title":"Ask-HN-What-makes-a-good-technical-leader-any-re"},{"content":"Automation-and-Make # Automation and Make # Created: March 22, 2020 2:43 AM URL: https://swcarpentry.github.io/make-novice/ Make is a tool which can run commands to read files, process these files in some way, and write out the processed files. For example, in software development, Make is used to compile source code into executable programs or libraries, but Make can also be used to:\nrun analysis scripts on raw data files to get data files that summarize the raw data; run visualization scripts on data files to produce plots; and to parse and combine text files and plots to create papers. Make is called a build tool - it builds data files, plots, papers, programs or libraries. Make tracks the dependencies between the files it creates and the files used to create these. If one of the original files (e.g. a data file) is changed, then Make knows to recreate, or update, the files that depend upon this file (e.g. a plot). Some previous experience with using the shell to list directories, create, copy, remove and list files and directories, and run simple scripts is necessary. Setup In order to follow this lesson, you will need to download some files.\n","date":"16 February 2023","permalink":"/posts/automation-and-make/","section":"Posts","summary":"Automation-and-Make # Automation and Make # Created: March 22, 2020 2:43 AM URL: https://swcarpentry.","title":"Automation-and-Make"},{"content":"AWS-re-Invent-2019-Deep-dive-and-best-practices-fo # AWS re:Invent 2019: Deep dive and best practices for Amazon Redshift (ANT418) - YouTube # Created: January 28, 2020 8:47 PM Tags: Data URL: https://www.youtube.com/watch?v=lj8oaSpCFTc\u0026amp;feature=youtu.be https://www.youtube.com/watch?v=lj8oaSpCFTc\u0026amp;feature=youtu.be\n","date":"16 February 2023","permalink":"/posts/aws-re-invent-2019-deep-dive-and-best-practices-fo/","section":"Posts","summary":"AWS-re-Invent-2019-Deep-dive-and-best-practices-fo # AWS re:Invent 2019: Deep dive and best practices for Amazon Redshift (ANT418) - YouTube # Created: January 28, 2020 8:47 PM Tags: Data URL: https://www.","title":"AWS-re-Invent-2019-Deep-dive-and-best-practices-fo"},{"content":"Ben-Horowitz-s-Best-Startup-Advice-Product-Hunt # Wherever he goes, entrepreneurs flock to him like fly to flypaper, hoping for a few minutes to pitch their companies or talk about how his NYT bestselling book, The Hard Thing About Hard Things, changed the way they think about startups. Find the thing you’re great at, put that into the world, contribute to others, help the world be better.” While his advice is often unconventional, it makes so much sense that it’s hard not to listen. Recently, Horowitz joined the Product Hunt for a LIVE Chat — by the way, you can find the full schedule of upcoming LIVE Chats here: In his answers to the community’s questions, his sincerity is almost palpable. — Ben Center** Start a company or join a startup and learn what the process of building a company feels like.\nBlake McDowall** I wrote a post on that called Why Founders Fail. Stella Tran** The book that most inspired my book was Andy Grove’s High Output Management. Join Product Hunt for future LIVE Chats with Ashton Kutcher, Naval Ravikant, Om Malik, Kara Swisher, Dave McClure, and many more. ","date":"16 February 2023","permalink":"/posts/ben-horowitz-s-best-startup-advice-product-hunt/","section":"Posts","summary":"Ben-Horowitz-s-Best-Startup-Advice-Product-Hunt # Wherever he goes, entrepreneurs flock to him like fly to flypaper, hoping for a few minutes to pitch their companies or talk about how his NYT bestselling book, The Hard Thing About Hard Things, changed the way they think about startups.","title":"Ben-Horowitz-s-Best-Startup-Advice-Product-Hunt"},{"content":"Best-of-SaaStr-SaaStr # Best of SaaStr | SaaStr # Created: May 16, 2020 7:55 AM URL: https://www.saastr.com/best-of-saastr/ ! Screen-Shot-2018-07-07-at-5.10.40-PM.png On Building A World-Class Sales Team: On Getting from 10 Customers to Initial Traction (~$1.5m): On Getting from Initial Traction (~$1.5m) to Initial Scale ($10m) and Beyond At Scale On The Journey On Mergers \u0026amp; Acquisitions (To Sell – Or Not? ), IPOs and Exits: On SaaS Start-Ups: On Marketing, Leads and Partners: On Pricing and Driving Up The Deal Size: On Customer Success, Upsells and Retention: Competition: Hiring and Retention: Metrics and Operations: Venture Capital: Product: Some of Best of SaaStr Annual videos: Some of the best videos on scaling with Jason:\n","date":"16 February 2023","permalink":"/posts/best-of-saastr-saastr/","section":"Posts","summary":"Best-of-SaaStr-SaaStr # Best of SaaStr | SaaStr # Created: May 16, 2020 7:55 AM URL: https://www.","title":"Best-of-SaaStr-SaaStr"},{"content":"Binomial-options-pricing-model-Wikipedia # The binomial model was first proposed by William Sharpe in the 1978 edition of Investments (ISBN 013504605X),[1] and formalized by Cox, Ross and Rubinstein in 1979[2] and by Rendleman and Bartter in that same year. [citation needed] For options with several sources of uncertainty (e.g., real options) and for options with complicated features (e.g., Asian options), binomial methods are less practical due to several difficulties, and Monte Carlo option models are commonly used instead. At each step, it is assumed that the underlying instrument will move up or down by a specific factor ( Binomial%20options%20pricing%20model%20-%20Wikipedia%20464f7993a1864d5ab474168e8056c9b4/c3e6bb763d22c20916ed4f0bb6bd49d7470cffd8 or Binomial%20options%20pricing%20model%20-%20Wikipedia%20464f7993a1864d5ab474168e8056c9b4/e85ff03cbe0c7341af6b982e47e9f90d235c66ab ) per step of the tree (where, by definition, Binomial%20options%20pricing%20model%20-%20Wikipedia%20464f7993a1864d5ab474168e8056c9b4/9418b55d44983bad84c6530b9368538a9892b9ef and Binomial%20options%20pricing%20model%20-%20Wikipedia%20464f7993a1864d5ab474168e8056c9b4/5f15d45850d0fb6f9b86b6ff899f71e679e67374 ). Option up Option down The following formula to compute the expectation value is applied at each node: , or Binomial%20options%20pricing%20model%20-%20Wikipedia%20464f7993a1864d5ab474168e8056c9b4/b75e23dc7ad848aee9b22bb4cef7cb97ae667b83 Binomial%20options%20pricing%20model%20-%20Wikipedia%20464f7993a1864d5ab474168e8056c9b4/00282af864f696abbe09d86960fe7d10ed44f0f4 where is the option\u0026rsquo;s value for the node at time t, Binomial%20options%20pricing%20model%20-%20Wikipedia%20464f7993a1864d5ab474168e8056c9b4/6586a79a20630949cb9f3809b10bc9ba637166cb Binomial%20options%20pricing%20model%20-%20Wikipedia%20464f7993a1864d5ab474168e8056c9b4/b20367c858b407ee650081aad55d73bc9bfb1850 is chosen such that the related binomial distribution simulates the geometric Brownian motion of the underlying stock with parameters r and σ, Binomial%20options%20pricing%20model%20-%20Wikipedia%20464f7993a1864d5ab474168e8056c9b4/30baa98e7511726459fb5f5fd0de92a05366b7a5 q is the dividend yield of the underlying corresponding to the life of the option. The aside algorithm demonstrates the approach computing the price of an American put option, although is easily generalized for calls and for European and Bermudan options:\nRelationship with Black–Scholes[edit] # Similar assumptions underpin both the binomial model and the Black–Scholes model, and the binomial model thus provides a discrete time approximation to the continuous process underlying the Black–Scholes model. [5] [4] In addition, when analyzed as a numerical procedure, the CRR binomial method can be viewed as a special case of the explicit finite difference method for the Black–Scholes PDE; see finite difference methods for option pricing.\nImplied binomial tree Edgeworth binomial tree [edit] # External links[edit] # The Binomial Model for Pricing Options, Prof. Thayer Watkins Binomial Option Pricing (PDF), Prof. Robert M. Conroy Binomial Option Pricing Model by Fiona Maclachlan, The Wolfram Demonstrations Project On the Irrelevance of Expected Stock Returns in the Pricing of Options in the Binomial Model: A Pedagogical Note by Valeri Zakamouline A Simple Derivation of Risk-Neutral Probability in the Binomial Option Pricing Model by Greg Orosi ","date":"16 February 2023","permalink":"/posts/binomial-options-pricing-model-wikipedia/","section":"Posts","summary":"Binomial-options-pricing-model-Wikipedia # The binomial model was first proposed by William Sharpe in the 1978 edition of Investments (ISBN 013504605X),[1] and formalized by Cox, Ross and Rubinstein in 1979[2] and by Rendleman and Bartter in that same year.","title":"Binomial-options-pricing-model-Wikipedia"},{"content":"Bringing-Data-Sources-Together-with-PipelineWise # Bringing Data Sources Together with PipelineWise # Created: December 5, 2019 2:04 PM Tags: Data URL: https://tech.transferwise.com/bringing-data-sources-together-with-pipelinewise/?utm_campaign=Up%20%26%20Running%20Weekly\u0026amp;utm_medium=email\u0026amp;utm_source=Revue%20newsletter TransferWise is open sourcing it’s data replication framework. PipelineWise is a Data Pipeline Framework using the Singer.io specification to replicate data from various sources to various destinations.\nDefining the Requirements # The tool had to be able to replicate hundreds of data sources into a centralised analytics data store where data analysts can do further transformations and can get real insights from data. The following minimal conditions had to be met:\nCompatibility with various data sources and analytics data stores as target Change Data Capture (CDC) mechanism wherever it’s possible to keep the lag as low as possible Maximum security and self hosted solution with the capability to obfuscate and mask PII and sensitive data at load time Apply schema changes automatically Scalability Avoiding vendor lock-in; accessing the source code to develop new features and fix issues quickly Keeping configuration as code Our first approach was to look for a data transportation product that could satisfy our known use cases and also pass the rigorous requirements defined by our security team. Team Effort and Running on Production # PipelineWise was created by our Analytics Platform team in close cooperation with our Data Analysts, and some of the product teams as the main consumers of the data. PipelineWise now meets all the above initial requirements and is used to replicate hundreds of GB of data every day from 120 microservices, 1500+ tables and a bunch of external tools into our Snowflake data warehouse, with only minutes of lag. Bringing%20Data%20Sources%20Together%20with%20PipelineWise%20aeefac37300e452e8ceb4cea5e01d2d9/pipelinewise-monitoring-grafana.png Monitoring with Grafana: Replicating 120 data sources, 1500+ tables into Snowflake with PipelineWise on 3 nodes of c5.2xlarge EC2 instances\nLimitations # Like any other tools PipelineWise also has limitations:\nNot Real-Time: The currently supported target connectors are micro-batch oriented. ","date":"16 February 2023","permalink":"/posts/bringing-data-sources-together-with-pipelinewise/","section":"Posts","summary":"Bringing-Data-Sources-Together-with-PipelineWise # Bringing Data Sources Together with PipelineWise # Created: December 5, 2019 2:04 PM Tags: Data URL: https://tech.","title":"Bringing-Data-Sources-Together-with-PipelineWise"},{"content":"Builder-Design-Pattern # Builder Design Pattern # Created: June 16, 2020 2:18 PM URL: https://sourcemaking.com/design_patterns/builder ! home-tb1.png\n","date":"16 February 2023","permalink":"/posts/builder-design-pattern/","section":"Posts","summary":"Builder-Design-Pattern # Builder Design Pattern # Created: June 16, 2020 2:18 PM URL: https://sourcemaking.","title":"Builder-Design-Pattern"},{"content":"Building-a-Mature-Analytics-Workflow # The center of gravity in mature analytics organizations has shifted away from proprietary, end-to-end tools towards more composable solutions made up of:\ndata integration scripts and/or tools, high-performance analytic databases, SQL, R, and/or Python, and visualization tools. We believe a mature analytics team’s techniques and workflow should have the following collaboration features: Version Control # Analytic code — whether it’s Python, SQL, Java, or anything else — should be version controlled.\nIf analytics is core to the success of an organization, the code, processes, and tooling required to produce that analysis are core organizational investments.We believe a mature analytics organization’s workflow should have the following characteristics so as to protect and grow that investment:\nEnvironments # Analytics requires multiple environments.\nAnalytics workflows require automated tools # Frequently, much of an analytic workflow is manual. Today, we’re announcing the release of the first version of our initial set of tools:\ndbt (data build tool) is a deployment tool for data models. Future versions of dbt will a) extend it to become a package management system, enabling an open source analytics ecosystem, b) automatically run tests on top of data models, and c) extend it to other analytic databases. Analyst Collective is sponsored by RJMetrics Pipeline, a tool that helps data engineers and analysts consolidate data into Amazon Redshift. ","date":"16 February 2023","permalink":"/posts/building-a-mature-analytics-workflow/","section":"Posts","summary":"Building-a-Mature-Analytics-Workflow # The center of gravity in mature analytics organizations has shifted away from proprietary, end-to-end tools towards more composable solutions made up of:","title":"Building-a-Mature-Analytics-Workflow"},{"content":"Building-a-simple-VPN-with-WireGuard-with-a-Raspbe # Building a simple VPN with WireGuard with a Raspberry Pi as Server // Andreas Happe # Created: January 30, 2020 9:07 AM Tags: Projects, Tech URL: https://snikt.net/blog/2020/01/29/building-a-simple-vpn-with-wireguard-with-a-raspberry-pi-as-server/ Now that wireguard will be part of the upcoming Linux 5.6 Kernel it’s time to see how to best integrate it with my Raspberry Pi based LTE-Router/Access Point Setup.\nWhat is my scenario? # This will be the VPN server (called edgewalker in this post)\nAn Android Phone that should use the VPN for all communication when connected An Linux Laptop that should use the VPN only accessing network services that are exposed to the VPN Each device connected to the VPN should be able to connect to all other devices, e.g., my phone should be able to connect to a webserver running on the laptop as long as both are part of the VPN network. Would I have read the manual I would have done the right steps: Untitled On the Raspberry Pi I am using Raspbian Buster, this distribution already included the wireguard package, I installed it with: Untitled On the Android Phone, I used the Google App Store to install the WireGuard VPN Application. Creating a configuration file for the VPN Server (Raspberry Pi) # Configuration was quite easy, I just created the following file at /etc/wireguard/wg0.conf: Untitled Some notes:\nPlease fill in the values from the created key files I am creating a VPN network that uses 10.200.200.0/24 for its internal IP range my server uses wwan0 as external network interface in the PostUp/PostDown-Commands, please adapt that to use your network-facing interface (might be eth0) It’s easy to bring the VPN network up with the following command: Untitled One small thing: I am using dnsmasq as DNS server and have bound it to the network interface br0. In dnsmasq you do this by adding a new config line to /etc/dnsmasq.conf with the network interface, e.g.: Untitled In addition I’ve added some iptable rules to allow traffic to the listening UDP port (51280): Untitled Now that everything works, we can utilize systemd to automatically start the VPN tunnel: Untitled Mostly the Laptop setup consists of creating a matching configuration file in /etc/wireguard/wg0.conf on the Laptop: Untitled Some notes: edgewalker should be the public IP-address or public hostname of the VPN server By setting AllowedIPs to 10.200.200.0/24 we are only using the VPN for accessing the internal VPN network. We prepare the following file (let’s call it mobile.conf) on the server through ssh: Untitled In contrast to the laptop setup we are forcing the mobile device to use our VPN server as DNS server (that’s the DNS setting) as well as using the newly VPN tunnel for all traffic (by using 0.0.0.0/0 as wildcard for AllowedIPs). ","date":"16 February 2023","permalink":"/posts/building-a-simple-vpn-with-wireguard-with-a-raspbe/","section":"Posts","summary":"Building-a-simple-VPN-with-WireGuard-with-a-Raspbe # Building a simple VPN with WireGuard with a Raspberry Pi as Server // Andreas Happe # Created: January 30, 2020 9:07 AM Tags: Projects, Tech URL: https://snikt.","title":"Building-a-simple-VPN-with-WireGuard-with-a-Raspbe"},{"content":"Building-Analytics-at-500px-Samson-Hu-Medium # A place where:\nEveryone understands company metrics and knows how their work fits into the measurement framework We measure the impact of decisions and acknowledge their success or failure Teams use self service tools to answer their own questions with data We can predict the future by digging into the data and finding leading indicators I knew that it wouldn’t be easy. The Dimensional Data Warehouse Schema # The schemas of data warehouses have two types of tables:\nFact tables record measurements in time of a process. ETL # In order to populate our data warehouse with data from our production databases and logs, we apply a process called ETL. An ETL script that has to turn messy production data into clean data warehouse data will naturally be extremely messy.\nThe Finished Data Warehouse # So that’s our data model — stored on Amazon Redshift, and fed new data every night by ETL pipelines run on Luigi. You can think of three funnels at 500px:\nvisitor -\u0026gt; signup -\u0026gt; daily active -\u0026gt; daily engaged -\u0026gt; paid subscriber visitor -\u0026gt; signup -\u0026gt; photo upload -\u0026gt; photo submit to marketplace -\u0026gt; photo sold on marketplace visitor -\u0026gt; signup -\u0026gt; purchase photo from marketplace Each team owns different parts of this funnel for different products: The marketing teams own (page views) and the top and bottom (revenue) of this funnel The product teams has less of an emphasis on top of funnel metrics The development teams (web and mobile), want to see the entire funnel with respect to their own products. There’s so much room for failure in this system: Logs could get parsed wrong, and you undercount or over-count an event The log sender on an API server could fail and you don’t notice and you miss a fraction of your events There might be network issues one day, and 10% of your log entries might fail to send to S3 Some metrics that are important might be hard to query in the database and you might pull the wrong number etc This doesn’t include hard fail events like your ETL’s failing and the metrics not being refreshed. ","date":"16 February 2023","permalink":"/posts/building-analytics-at-500px-samson-hu-medium/","section":"Posts","summary":"Building-Analytics-at-500px-Samson-Hu-Medium # A place where:","title":"Building-Analytics-at-500px-Samson-Hu-Medium"},{"content":"Building-Serverless-Python-Apps-Using-AWS-Chalice # By the end of this tutorial, you’ll be able to:\nDiscuss the benefits of a serverless architecture Explore Chalice, a Python serverless framework Build a full blown serverless app for a real world use case Deploy to Amazon Web Services (AWS) Lambda Compare Pure and Lambda functions Free Bonus: 5 Thoughts On Python Mastery, a free course for Python developers that shows you the roadmap and the mindset you\u0026rsquo;ll need to take your Python skills to the next level. Getting Started With AWS Chalice # Chalice, a Python Serverless Microframework developed by AWS, enables you to quickly spin up and deploy a working serverless app that scales up and down on its own as required using AWS Lambda. Updating policy for IAM role: hello-world-dev Creating lambda function: hello-world-dev Creating Rest API Resources deployed:\nLambda ARN: arn:aws:lambda:ap-south-1:679337104153:function:hello-world-dev Rest API URL: https://fqcdyzvytc.execute-api.ap-south-1.amazonaws.com/api/ **Note**: The generated ARN and API URL in the above snippet will vary from user to user. First, let’s include all the import statements: from os import environ as env\n3rd party imports # from chalice import Chalice, Response from twilio.rest import Client from twilio.base.exceptions import TwilioRestException\nTwilio Config # ACCOUNT_SID = env.get(\u0026lsquo;ACCOUNT_SID\u0026rsquo;) AUTH_TOKEN = env.get(\u0026lsquo;AUTH_TOKEN\u0026rsquo;) FROM_NUMBER = env.get(\u0026lsquo;FROM_NUMBER\u0026rsquo;) TO_NUMBER = env.get(\u0026lsquo;TO_NUMBER\u0026rsquo;)\nNext, you’ll encapsulate the Twilio API and use it to send SMS: app = Chalice(app_name=\u0026lsquo;sms-shooter\u0026rsquo;)\nCreate a Twilio client using account_sid and auth token # tw_client = Client(ACCOUNT_SID, AUTH_TOKEN) @app.route(\u0026rsquo;/service/sms/send\u0026rsquo;, methods=[\u0026lsquo;POST\u0026rsquo;]) def send_sms(): request_body = app.current_request.json_body if request_body: try: msg = tw_client.messages.create( from_=FROM_NUMBER, body=request_body[\u0026lsquo;msg\u0026rsquo;], to=TO_NUMBER) if msg.sid: return Response(status_code=201, headers={\u0026lsquo;Content-Type\u0026rsquo;: \u0026lsquo;application/json\u0026rsquo;}, body={\u0026lsquo;status\u0026rsquo;: \u0026lsquo;success\u0026rsquo;, \u0026lsquo;data\u0026rsquo;: msg.sid, \u0026lsquo;message\u0026rsquo;: \u0026lsquo;SMS successfully sent\u0026rsquo;}) else: return Response(status_code=200, headers={\u0026lsquo;Content-Type\u0026rsquo;: \u0026lsquo;application/json\u0026rsquo;}, body={\u0026lsquo;status\u0026rsquo;: \u0026lsquo;failure\u0026rsquo;, \u0026lsquo;message\u0026rsquo;: \u0026lsquo;Please try again!!!\u0026rsquo;}) Updating policy for IAM role: sms-shooter-dev Creating lambda function: sms-shooter-dev Creating Rest API Resources deployed:\nLambda ARN: arn:aws:lambda:ap-south-1:679337104153:function:sms-shooter-dev Rest API URL: https://qtvndnjdyc.execute-api.ap-south-1.amazonaws.com/api/ **Note**: The above command succeeds, and you have your API URL in the output as expected. Updating policy for IAM role: sms-shooter-dev Updating lambda function: sms-shooter-dev Updating rest API Resources deployed: - Lambda ARN: arn:aws:lambda:ap-south-1:679337104153:function:sms-shooter-dev - Rest API URL: https://fqcdyzvytc.execute-api.ap-south-1.amazonaws.com/api/ Do a sanity check by making a curl request to the generated API endpoint:\n$ curl -H \u0026#34;Content-Type: application/json\u0026#34; -X POST -d \u0026#39;{\u0026#34;msg\u0026#34;: \u0026#34;hey mate!!!\u0026#34;}\u0026#39; Now to make this work, we need to make changes to `app.py` as well: Core imports # from chalice import Chalice, Response from twilio.base.exceptions import TwilioRestException\nApp level imports # from chalicelib import sms app = Chalice(app_name=\u0026lsquo;sms-shooter\u0026rsquo;) @app.route(\u0026rsquo;/\u0026rsquo;) def index(): return {\u0026lsquo;hello\u0026rsquo;: \u0026lsquo;world\u0026rsquo;} @app.route(\u0026rsquo;/service/sms/send\u0026rsquo;, methods=[\u0026lsquo;POST\u0026rsquo;]) def send_sms(): request_body = app.current_request.json_body if request_body: try: resp = sms.send(request_body) if resp: return Response(status_code=201, headers={\u0026lsquo;Content-Type\u0026rsquo;: \u0026lsquo;application/json\u0026rsquo;}, body={\u0026lsquo;status\u0026rsquo;: \u0026lsquo;success\u0026rsquo;, \u0026lsquo;data\u0026rsquo;: resp.sid, \u0026lsquo;message\u0026rsquo;: \u0026lsquo;SMS successfully sent\u0026rsquo;}) else: return Response(status_code=200, headers={\u0026lsquo;Content-Type\u0026rsquo;: \u0026lsquo;application/json\u0026rsquo;}, body={\u0026lsquo;status\u0026rsquo;: \u0026lsquo;failure\u0026rsquo;, \u0026lsquo;message\u0026rsquo;: \u0026lsquo;Please try again!!!\u0026rsquo;})\n","date":"16 February 2023","permalink":"/posts/building-serverless-python-apps-using-aws-chalice/","section":"Posts","summary":"Building-Serverless-Python-Apps-Using-AWS-Chalice # By the end of this tutorial, you’ll be able to:","title":"Building-Serverless-Python-Apps-Using-AWS-Chalice"},{"content":"Business-Model-Library-Reason-Street # Business%20Model%20Library%20Reason%20Street%2044f6b760abdf4ab99a1a68bdffeacd2c/11_SaaS.png Learn More ! Business%20Model%20Library%20Reason%20Street%2044f6b760abdf4ab99a1a68bdffeacd2c/12_hardware-as-a-service.png Learn More ! Business%20Model%20Library%20Reason%20Street%2044f6b760abdf4ab99a1a68bdffeacd2c/02_Data-as-a-service.png Learn More ! Business%20Model%20Library%20Reason%20Street%2044f6b760abdf4ab99a1a68bdffeacd2c/25_revenue_share.png Learn More ! Business%20Model%20Library%20Reason%20Street%2044f6b760abdf4ab99a1a68bdffeacd2c/03_Free_Data-as-a-service.png Learn More ! Business%20Model%20Library%20Reason%20Street%2044f6b760abdf4ab99a1a68bdffeacd2c/09_two-sided_marketplace.png Learn More ! Business%20Model%20Library%20Reason%20Street%2044f6b760abdf4ab99a1a68bdffeacd2c/07_advertising.png Learn More Let Us Know\n","date":"16 February 2023","permalink":"/posts/business-model-library-reason-street/","section":"Posts","summary":"Business-Model-Library-Reason-Street # Business%20Model%20Library%20Reason%20Street%2044f6b760abdf4ab99a1a68bdffeacd2c/11_SaaS.","title":"Business-Model-Library-Reason-Street"},{"content":"Business-Models # Business Models # Created: December 8, 2019 10:20 AM Tags: Finance, Startup URL: https://news.ycombinator.com/item?id=21723189 Indeed. Some of these models are partially-applied functions - they\u0026rsquo;re missing the part that actually makes them money. For instance:\nNetwork effects (can come in different forms such as direct, two-sided, data): Facebook — the value of the product increases as more people use it Network effects keeps the users in, money is made by ads and surveillance. Crowdsourcing: YouTube — aggregate content/product from users individuals and distributes them at scale Really, it\u0026rsquo;s also a form of network effects (viewers go where most content is; creators go where most viewers are), and again, money is made by ads and surveillance. Also, missing models:\nRegulatory arbitrage: Uber - conquering markets by breaking local laws faster than the regulators can react. Also2: Open-source: Bitcoin Bitcoin is not a company and does not have a business model. \u0026ldquo;Donations\u0026rdquo; and \u0026ldquo;Paid support\u0026rdquo; are.\n","date":"16 February 2023","permalink":"/posts/business-models/","section":"Posts","summary":"Business-Models # Business Models # Created: December 8, 2019 10:20 AM Tags: Finance, Startup URL: https://news.","title":"Business-Models"},{"content":"Call-for-Service-2020-Socrata-API-Foundry # Call for Service 2020 | Socrata API Foundry # Created: April 6, 2020 9:29 AM URL: https://dev.socrata.com/foundry/data.nola.gov/hp7u-i9hf\n","date":"16 February 2023","permalink":"/posts/call-for-service-2020-socrata-api-foundry/","section":"Posts","summary":"Call-for-Service-2020-Socrata-API-Foundry # Call for Service 2020 | Socrata API Foundry # Created: April 6, 2020 9:29 AM URL: https://dev.","title":"Call-for-Service-2020-Socrata-API-Foundry"},{"content":"Census # Census # Created: January 8, 2020 10:10 AM Tags: Startup Merge data from different sources to build a single record of everything you know about each customer. Define health scores in a single place to share across your Marketing, Sales, and Success. Easily create new syncs between your data and your CRM, Marketing, Support, and more. Smart integrations only update when records actually change, and automatically handles new fields as your dataset expands. Census%208491b10eccc44144b07b0c5e88d282ed/publish.png Don\u0026rsquo;t get stuck waiting on the engineering backlog. Syncs can be updated as your business needs change and new data becomes important. Census%208491b10eccc44144b07b0c5e88d282ed/notify-errors.png\nLearn how Census can help your revenue teams focus on the right opportunities at the right time. # ","date":"16 February 2023","permalink":"/posts/census/","section":"Posts","summary":"Census # Census # Created: January 8, 2020 10:10 AM Tags: Startup Merge data from different sources to build a single record of everything you know about each customer.","title":"Census"},{"content":"Chris-Albon # Chris Albon # Created: February 21, 2020 12:05 PM URL: https://chrisalbon.com/ I am a data scientist with a decade of experience applying statistical learning, artificial intelligence, and software engineering to political, social, and humanitarian efforts \u0026ndash; from election monitoring to disaster relief. I lead the data science team at Devoted Health, helping fix America\u0026rsquo;s health care system. Learning machine learning? Check out my Machine Learning Flashcards or my book, Machine Learning With Python Cookbook.\n","date":"16 February 2023","permalink":"/posts/chris-albon/","section":"Posts","summary":"Chris-Albon # Chris Albon # Created: February 21, 2020 12:05 PM URL: https://chrisalbon.","title":"Chris-Albon"},{"content":"Churn # Churn%203075f664cd834b5b8e5d6924f4ad1f2f/0j3iK6M7Jt1TCV-NX.png *If a user has a constant churn probability over time, this implies that customer lifetimes come from an Exponential distribution. If the churn probability gets lower the longer the customer has been subscribed, you could model that as c/(t+1), where t is the timestep (e.g. number of days the customer has been subscribed), and c is some constant. Churn%203075f664cd834b5b8e5d6924f4ad1f2f/0_ceJxW5UQmjQze15.png *The Lomax distribution can express churn probabilities that get lower with time. Keep in mind, in each of the examples below we simulate lifetimes from the same customer lifetime distribution, and this distribution does not change over time. Multiply this by what you make per customer per day, and you have your Customer Lifetime Value. Keep in mind that the typical customers (found by the median) stick around equally long in either company, but it’s the rare long term customers that shift the Lifetime Customer Value massively in favor of the orange company. ](http://fooledbyrandomness.com/DarwinCollege.pdf) So if you have Pareto 80/20 distributed customer lifetimes, **you need 100 billion customers before the sample mean lifetime is accurate.\n","date":"16 February 2023","permalink":"/posts/churn/","section":"Posts","summary":"Churn # Churn%203075f664cd834b5b8e5d6924f4ad1f2f/0j3iK6M7Jt1TCV-NX.","title":"Churn"},{"content":"Clearing-Programming-Interview-Assignment-RSS-Feed # Install the necessary libraries for python:\ndocker exec rss-python conda update -c base -c defaults conda docker exec rss-python conda install beautifulsoup4 lxml psycopg2 docker exec rss-python conda install -c conda-forge apscheduler Let’s start building our data model now… After reviewing some of the posts, I realised there needs to be three entities:\nposts: Keeping entry of each post published on the feed2. Please use this github link for the exact create table queries (as they are pretty straightforward) The tables can be created in one shot by running: docker exec -it rss-postgres psql -U postgres -d audioboom -f create_db.sql Helper modulesThree scripts apart from the main script were created to abstract some of the underlying functionality with function calls:\ncontent_fetcher.py: Use to semi-robustly handle a web-page request and return it’s content2. update_feed_data(feed,conn):** requests the BeautifulSoup object for the given url and attempts to process any record in it: def update_feed_data(feed,conn): content = get_soup(feed) print(f\u0026#34;Processing Records for : {feed}\u0026#34;) records = content.find_all(\u0026#39;item\u0026#39;) process_records(records,conn) return Again, following the functional paradigm, it does its job by retrieving the BeautifulSoup object and passes the content for further processing. 5. process_records(content, conn**):** It processes records incrementally and pushes them for persistence (capturing in DB):\ndef process_records(content,conn): record_count = len(content) current_max = get_max_records(conn,get_max_query) print(\u0026#39;Current Max : \u0026#39;,current_max) records = {} if record_count == current_max: print(\u0026#34;No new records found!!\u0026#34;) **6. persist_tastse_of_india_record(***conn***,***data***):** It tries to persist each component of a post separately (based on the entities defined) def persist_taste_of_india_record(conn,data): persist_record(conn,data,\u0026lsquo;posts\u0026rsquo;) persist_record(conn,data[\u0026lsquo;itunes\u0026rsquo;],\u0026lsquo;itunes_data\u0026rsquo;) for media in data[\u0026lsquo;media\u0026rsquo;]: persist_record(conn,media,\u0026lsquo;media\u0026rsquo;) conn.commit() return True\n**conn.commit()** is necessary, else the changes in the database aren’t permanent and will be lost once the session expires. **7. persist_record(***conn***,***data***,***tb_name***):** Execute the insert query based on the object type: def persist_record(conn,data,tb_name): query_param = tuple( list(map(lambda k : data[k],col_list[tb_name]))) execute_query(conn,query_strings[tb_name],query_param) return\n***query_param*** simply stores the values in the column order in a tuple. **Executing and scheduling it:** The script is completed by invoking the begin function and scheduling it for once everyday execution as follows: if name == \u0026lsquo;main\u0026rsquo;: feed_url = \u0026lsquo;https://audioboom.com/channels/4930693.rss' db_credentials = \u0026lsquo;connection.json\u0026rsquo; print(\u0026lsquo;Main Script Running\u0026hellip;\u0026rsquo;) begin(feed_url,db_credentials) scheduler = BlockingScheduler() scheduler.add_job(begin, \u0026lsquo;interval\u0026rsquo;,[feed_url,db_credentials], hours=24) try: scheduler.start() except Exception as e: print(\u0026lsquo;Stopping Schedule!!\u0026rsquo;)\n","date":"16 February 2023","permalink":"/posts/clearing-programming-interview-assignment-rss-feed/","section":"Posts","summary":"Clearing-Programming-Interview-Assignment-RSS-Feed # Install the necessary libraries for python:","title":"Clearing-Programming-Interview-Assignment-RSS-Feed"},{"content":"CodeSandbox-Online-IDE-for-Rapid-Web-Development # CodeSandbox: Online IDE for Rapid Web Development # Created: May 25, 2020 11:02 AM URL: https://codesandbox.io/ ! banner.png\nAn instant IDE and prototyping tool for rapid web development. # ","date":"16 February 2023","permalink":"/posts/codesandbox-online-ide-for-rapid-web-development/","section":"Posts","summary":"CodeSandbox-Online-IDE-for-Rapid-Web-Development # CodeSandbox: Online IDE for Rapid Web Development # Created: May 25, 2020 11:02 AM URL: https://codesandbox.","title":"CodeSandbox-Online-IDE-for-Rapid-Web-Development"},{"content":"cohost-Systems-Engineering-or-why-james-gets-p # Once your project has multiple people working on it, sometimes with widely different experiences (e.g. the \u0026lsquo;software folks\u0026rsquo; and the \u0026lsquo;hardware folks\u0026rsquo; and the \u0026lsquo;manager folks\u0026rsquo; and the \u0026lsquo;product folks\u0026rsquo; and so on), the concepts of \u0026ldquo;what are you building\u0026rdquo; and \u0026ldquo;how are you building it\u0026rdquo; seem to get away from teams and organizations.\nWhy aren\u0026rsquo;t more people doing \u0026ldquo;systems engineering\u0026rdquo;? # They might think about breaking a problem down in to parts as an individual engineer solving problems, but don\u0026rsquo;t have a lot of tools for breaking down \u0026ldquo;team sized problems\u0026rdquo;. \u0026ldquo;Classical engineering\u0026rdquo; folks in electrical and mechanical domains tend to have seen some of the processes and formality around systems engineering before, but haven\u0026rsquo;t necessarily internalized why certain approaches exist. This gets a little weird in the time between \u0026ldquo;the requirements say what the system will do\u0026rdquo;, and \u0026ldquo;the requirements say what the system does do\u0026rdquo;. In the ideal world, you should be able to:\nHave someone come in for their first day Know what they need access to, before they start Point them at one document (which might link to other ones) They should be able to read this document They should know generally how to start working with your team Again, you can capture this information however you want. These documents end up acting a bit like \u0026ldquo;process requirements\u0026rdquo;: They should be a living document over time Everyone should know where to find them If there\u0026rsquo;s a disagreement, the \u0026ldquo;process requirements\u0026rdquo; should have the answer, or if it doesn\u0026rsquo;t, you should come to a decision and write it down Incomplete is okay (but improve it as you go! ","date":"16 February 2023","permalink":"/posts/cohost-systems-engineering-or-why-james-gets-p/","section":"Posts","summary":"cohost-Systems-Engineering-or-why-james-gets-p # Once your project has multiple people working on it, sometimes with widely different experiences (e.","title":"cohost-Systems-Engineering-or-why-james-gets-p"},{"content":"Collecting-data-from-a-leaflet-map-with-flask-St # Collecting data from a leaflet map with flask - Stack Overflow # Created: February 27, 2020 2:27 PM URL: https://stackoverflow.com/questions/52172010/collecting-data-from-a-leaflet-map-with-flask ! apple-touch-icon@2.png I\u0026rsquo;m trying to create a webpage with flask. The webpage includes a leaflet map where I can click on the map to create a marker which opens a popup window with a link. The link is supposed to open a new page where I can see the longitude and latitude. I\u0026rsquo;m currently struggeling on how to send the leaflet coordinates from my js to flask and then to the second route. Can someone explain to me what I\u0026rsquo;m doing wrong? Python file:\nfrom flask import Flask, render_template, url_for, request, redirect app = Flask(__name__) @app.route(\u0026#39;/\u0026#39;, methods=[\u0026#34;GET\u0026#34;,\u0026#34;POST\u0026#34;]) def mainpage(): if request.method == \u0026#34;POST\u0026#34;: longitude = request.form[\u0026#34;longitude\u0026#34;] latitude = request.form[\u0026#34;latitude\u0026#34;] return lredirect(url_for(\u0026#34;form\u0026#34;, longitude=longitude, latitude=latitude)) return render_template(\u0026#34;main.html\u0026#34;) @app.route(\u0026#39;/form\u0026#39;) def form(): return render_template(\u0026#34;form.html\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: app.run(debug=True) main.html file:\n","date":"16 February 2023","permalink":"/posts/collecting-data-from-a-leaflet-map-with-flask-st/","section":"Posts","summary":"Collecting-data-from-a-leaflet-map-with-flask-St # Collecting data from a leaflet map with flask - Stack Overflow # Created: February 27, 2020 2:27 PM URL: https://stackoverflow.","title":"Collecting-data-from-a-leaflet-map-with-flask-St"},{"content":"Column-oriented-DBMS # Column-oriented DBMS # Created: April 20, 2020 7:50 AM URL: http://people.brandeis.edu/~nga/papers/VLDB05.pdf VLDB05.pdf\n","date":"16 February 2023","permalink":"/posts/column-oriented-dbms/","section":"Posts","summary":"Column-oriented-DBMS # Column-oriented DBMS # Created: April 20, 2020 7:50 AM URL: http://people.","title":"Column-oriented-DBMS"},{"content":"Combining-multiple-tables-with-valid-from-to-date # Combining multiple tables with valid from/to date ranges into a single dimension | ORAYLIS # Created: February 1, 2020 2:17 PM URL: https://www.oraylis.de/blog/combining-multiple-tables-with-valid-from-to-date-ranges-into-a-single-dimension Data Integration (ETL - ELT) Data Modelling Data Warehouse Microsoft APS Dimensional modeling Tracking historical changes within a dimension is a common task in data warehousing and well covered by Ralph Kimball’s slowly changing dimension (SCD) methods. But the valid from/to dates are usually not a good idea for joining fact data to the associated dimensions because this would result in range lookups (ETL) or date range (between) joins (in SQL or ELT). Combining%20multiple%20tables%20with%20valid%20from%20to%20date%20%2074c784b6dd2f4768bc7932ad8a5bc58f/2014_11_image_thumb5.png The tables reflect a very simple human resources model of four tables, a base table Employee and three detail tables, all joined by the EmployeeNo-field. The following query for example checks if there are overlapping date ranges in the Employee table by using window functions to retrieve the previous and next date boundaries:\n, lag([ValidTo],1) over (partition by [EmployeeNo] order by [ValidFrom]) PrevValidTo , lead([ValidFrom],1) over (partition by [EmployeeNo] order by [ValidFrom]) NextValidFrom where (PrevValidTo is not null and PrevValidTo\u0026gt;ValidFrom) or (NextValidFrom is not null and NextValidFrom and \u0026lt; in the where condition to a \u0026lt;\u0026gt;, i.e. Running this check on all the four tables from above shows that the data is consistent (no faulty rows returned from the query above). At first, as the resulting valid from/to dates need to reflect all date ranges from all of the four tables, I start by collecting all of those dates: This gives a list of all valid from/to-dates by employee from all of the four tables with duplicates being removed (since I used a union, not a union all). So, now we can join the four tables with the date range table making sure to include the proper date range in the join condition. This could even make it a good idea to include valid from/to dates from other associated tables even if no other information from those tables is yet being used in the data warehouse. ","date":"16 February 2023","permalink":"/posts/combining-multiple-tables-with-valid-from-to-date/","section":"Posts","summary":"Combining-multiple-tables-with-valid-from-to-date # Combining multiple tables with valid from/to date ranges into a single dimension | ORAYLIS # Created: February 1, 2020 2:17 PM URL: https://www.","title":"Combining-multiple-tables-with-valid-from-to-date"},{"content":"Compost-Products-REOTEMP-Instruments # Compost Products - REOTEMP Instruments # Created: March 7, 2020 9:02 AM URL: https://reotemp.com/compost/ ! Compost%20Products%20-%20REOTEMP%20Instruments%20840f9cdd638f408db85feb351b05e46e/reotemp-compost-logo.png REOTEMP is a family owned company that understands your goal is to create high quality compost quickly and efficiently. You can measure how your compost is doing by looking at key pieces of information like compost temperature, % oxygen, and moisture content.\nWhat types of composting instruments does REOTEMP offer? # REOTEMP offer both high quality industrial compost thermometers for municipal and city operations and backyard compost thermometers for the beginner backyard composter. To measure relative moisture, REOTEMP offers heavy-duty compost moisture meters in short and long-stem varieties which can be used for gardening or composting.\nPurchase Through Our Secure Online Store # ","date":"16 February 2023","permalink":"/posts/compost-products-reotemp-instruments/","section":"Posts","summary":"Compost-Products-REOTEMP-Instruments # Compost Products - REOTEMP Instruments # Created: March 7, 2020 9:02 AM URL: https://reotemp.","title":"Compost-Products-REOTEMP-Instruments"},{"content":"Compound-Back-Exercises-Legion-Athletics # These are kinds of free weight back exercises that give you a full back workout in less time than other back exercises, and allow you to lift heavier weights more safely. T-Bar Row](https://legionathletics.com/best-back-exercises/)\nAn Example Back Workout There are several muscles that make up the bulk of the back: Trapezius (traps) Rhomboids Teres major and minor Infraspinatus Latissimus dorsi (lats) Erector spinae (iliocostalis, longissimus, and spinalis muscles) When people refer to the upper back or thoracic spine, they’re referring mainly to the trapezius, lats, rhomboids, teres muscles, and infraspinatus. By “heavy,” I mean doing back exercises with weights in the range of 75 to 85% of your one-rep max (1RM), or in the range of 8 to 10 (~75%) to 4 to 6 (~85%) reps. You don’t have to do your full back workout in a low rep range—high-rep sets also have a place—but the majority of your time should be spent doing free weight back exercises with heavy weights. Why: The deadlift is hands down the best all-around back exercise you can do because it trains every muscle in your posterior chain (the muscles on the back side of your body). Push your hips back, arch your lower back slightly, keep your shoulders back and down, and make sure your head is in a neutral position. This taxes your back muscles slightly differently to other free weight back exercises. Deadlift: 3 sets of 4 to 6 reps Barbell Row: 3 sets of 4 to 6 reps One-Arm Dumbbell Row: 3 sets of 8 to 10 reps Pull-up or chin-up: 3 sets of 8 to 10 reps\nIf you want more back workouts that include all of the best free weight back exercises you can do, check out this article:\n3 Supplements for Better Back Workouts # I saved this for last because it’s the least important.\n","date":"16 February 2023","permalink":"/posts/compound-back-exercises-legion-athletics/","section":"Posts","summary":"Compound-Back-Exercises-Legion-Athletics # These are kinds of free weight back exercises that give you a full back workout in less time than other back exercises, and allow you to lift heavier weights more safely.","title":"Compound-Back-Exercises-Legion-Athletics"},{"content":"Consistency-Models # Consistency Models # Created: June 18, 2020 3:07 PM URL: https://jepsen.io/consistency This clickable map (adapted from Bailis, Davidson, Fekete et al and Viotti \u0026amp; Vukolic) shows the relationships between common consistency models for concurrent systems. We say “logically single-threaded” to emphasize that while a process can only do one thing at a time, its implementation may be spread across multiple threads, operating system processes, or even physical nodes—just so long as those components provide the illusion of a coherent singlethreaded program. To model this, we say that each operation has an invocation time and, should it complete, a strictly greater completion time, both given by an imaginary2, perfectly synchronized, globally accessible clock. 4 Since operations take time, two operations might overlap in time. If an operation does not complete for some reason (perhaps because it timed out or a critical component crashed) that operation has no completion time, and must, in general, be considered concurrent with every operation after its invocation. Some papers represent this as a set of operations, where each operation includes two numbers, representing their invocation and completion time; concurrent structure is inferred by comparing the time windows between processes. ↩ This magical synchronized clock doesn’t actually need to exist, but some consistency models imply that if such a clock were to exist, some operations would happen before others.\n","date":"16 February 2023","permalink":"/posts/consistency-models/","section":"Posts","summary":"Consistency-Models # Consistency Models # Created: June 18, 2020 3:07 PM URL: https://jepsen.","title":"Consistency-Models"},{"content":"Consulting-Mistakes # Consulting Mistakes # Created: December 7, 2019 1:48 PM Tags: Consulting, Self URL: https://news.ycombinator.com/item?id=21728436 What was the specific mistake, what did you learn from it, and how did you change the way you do business as a result? For example, I was told by someone who started a project without taking an advance, only for the client to decide not to pay. So he now refuses to take on a project without at least 30% advance. What lessons did you learn the hard way? I\u0026rsquo;m setting up my own consulting practice: kartick.org and would like to learn from other people\u0026rsquo;s mistakes rather than repeating them myself.\n","date":"16 February 2023","permalink":"/posts/consulting-mistakes/","section":"Posts","summary":"Consulting-Mistakes # Consulting Mistakes # Created: December 7, 2019 1:48 PM Tags: Consulting, Self URL: https://news.","title":"Consulting-Mistakes"},{"content":"COVID-19-Community-Mobility-Reports # COVID-19 Community Mobility Reports # Created: April 3, 2020 12:26 PM URL: https://www.google.com/covid19/mobility/ ! social_image.png As global communities respond to COVID-19, we\u0026rsquo;ve heard from public health officials that the same type of aggregated, anonymized insights we use in products such as Google Maps could be helpful as they make critical decisions to combat COVID-19. These Community Mobility Reports aim to provide insights into what has changed in response to policies aimed at combating COVID-19. The reports chart movement trends over time by geography, across different categories of places such as retail and recreation, groceries and pharmacies, parks, transit stations, workplaces, and residential.\n","date":"16 February 2023","permalink":"/posts/covid-19-community-mobility-reports/","section":"Posts","summary":"COVID-19-Community-Mobility-Reports # COVID-19 Community Mobility Reports # Created: April 3, 2020 12:26 PM URL: https://www.","title":"COVID-19-Community-Mobility-Reports"},{"content":"Create-a-macOS-Menu-Bar-App-with-Python-Pomodoro # Create a macOS Menu Bar App with Python (Pomodoro Timer) – Camillo Visini # Created: July 13, 2020 2:44 AM URL: https://camillovisini.com/create-macos-menu-bar-app-pomodoro/ ! As an example, we will create a 🍅 pomodoro app, which you can use to boost your productivity and manage your time from the convenience of your menu bar. We will be using the following software:\nPython 3 and PyCharm as an IDE Rumps → Ridiculously Uncomplicated macOS Python Statusbar apps py2app → For creating standalone macOS apps from Python code *(how cool is that? Enter the following in your terminal: pip3 install -U py2app # this will install py2app using pip, or to upgrade to the latest released version of py2app pip3 install -U rumps # this will install rumps using pip, or to upgrade to the latest released version of rumps Step 2: Basic example # Open the project directory in your favorite editor or IDE – in my case PyCharm – by typing:\ncharm . In pomodoro.py we need to set up the following boilerplate code in order to get started:\nimport rumps class PomodoroApp(object): def __init__(self): self.app = rumps.App(\u0026#34;Pomodoro\u0026#34;, \u0026#34;🍅\u0026#34;) def run(self): self.app.run() if __name__ == \u0026#39;__main__\u0026#39;: app = PomodoroApp() app.run() If you execute the python program using python3 pomodoro.py, you will notice a new addition to your menu bar – albeit with limited functionality, as you can see… A barely functional menu bar app\u0026hellip;\nStep 3: Full implementation # Now we will implement the actual functionality of our pomodoro menu bar app – see the complete code for pomodoro.py below:\nimport rumps class PomodoroApp(object): def __init__(self): self.config = { \u0026#34;app_name\u0026#34;: \u0026#34;Pomodoro\u0026#34;, \u0026#34;start\u0026#34;: \u0026#34;Start Timer\u0026#34;, \u0026#34;pause\u0026#34;: \u0026#34;Pause Timer\u0026#34;, \u0026#34;continue\u0026#34;: \u0026#34;Continue Timer\u0026#34;, \u0026#34;stop\u0026#34;: \u0026#34;Stop Timer\u0026#34;, \u0026#34;break_message\u0026#34;: \u0026#34;Time is up! Take a break :)\u0026#34;, \u0026#34;interval\u0026#34;: 1500 } self.app = rumps.App(self.config[\u0026#34;app_name\u0026#34;]) self.timer = rumps.Timer(self.on_tick, 1) self.interval = self.config[\u0026#34;interval\u0026#34;] self.set_up_menu() self.start_pause_button = rumps.MenuItem(title=self.config[\u0026#34;start\u0026#34;], callback=self.start_timer) self.stop_button = rumps.MenuItem(title=self.config[\u0026#34;stop\u0026#34;], callback=None) self.app.menu = [self.start_pause_button, self.stop_button] def set_up_menu(self): self.timer.stop() self.timer.count = 0 self.app.title = \u0026#34;🍅\u0026#34; def on_tick(self, sender): time_left = sender.end - sender.count mins = time_left // 60 if time_left \u0026gt;= 0 else time_left // 60 + 1 secs = time_left % 60 if time_left \u0026gt;= 0 else (-1 * time_left) % 60 if mins == 0 and time_left \u0026lt; 0: rumps.notification(title=self.config[\u0026#34;app_name\u0026#34;], subtitle=self.config[\u0026#34;break_message\u0026#34;], message=\u0026#39;\u0026#39;) self.stop_timer() self.stop_button.set_callback(None) else: self.stop_button.set_callback(self.stop_timer) self.app.title = \u0026#39;{:2d}:{:02d}\u0026#39;.format(mins, secs) sender.count += 1 def start_timer(self, sender): if sender.title.lower().startswith((\u0026#34;start\u0026#34;, \u0026#34;continue\u0026#34;)): if sender.title == self.config[\u0026#34;start\u0026#34;]: self.timer.count = 0 self.timer.end = self.interval sender.title = self.config[\u0026#34;pause\u0026#34;] self.timer.start() else: sender.title = self.config[\u0026#34;continue\u0026#34;] self.timer.stop() def stop_timer(self): self.set_up_menu() self.stop_button.set_callback(None) self.start_pause_button.title = self.config[\u0026#34;start\u0026#34;] def run(self): self.app.run() if __name__ == \u0026#39;__main__\u0026#39;: app = PomodoroApp() app.run() By executing the python program again using python3 pomodoro.py, you can see our menu bar app in its full glory: Now we\u0026rsquo;re getting there!\nStep 4: Create macOS app from our python code # In setup.py we need to add the following code, which provides all the necessary instructions to create the application bundle (app name, app version, app icon, etc.)\n","date":"16 February 2023","permalink":"/posts/create-a-macos-menu-bar-app-with-python-pomodoro/","section":"Posts","summary":"Create-a-macOS-Menu-Bar-App-with-Python-Pomodoro # Create a macOS Menu Bar App with Python (Pomodoro Timer) – Camillo Visini # Created: July 13, 2020 2:44 AM URL: https://camillovisini.","title":"Create-a-macOS-Menu-Bar-App-with-Python-Pomodoro"},{"content":"Creating-Data-Vault-Point-In-Time-and-Dimension-ta # Creating Data Vault Point-In-Time and Dimension tables: merging historical data sources - Roelant Vos # Created: February 1, 2020 2:17 PM URL: http://roelantvos.com/blog/creating-data-vault-point-in-time-and-dimension-tables-merging-historical-data-sources/ Edit 2019-09-05: I reviewed this paper on relevance today and made a few minor tweaks.\nMerging time-variant data # Beyond creating Hubs, Links and Satellites and current-state (Type 1) views off a Data Vault model, one of the most common requirements is the ability to represent a complete history of changes for a specific business entity (Hub, Link or groups of those). This post outlines how merging time-variant data can be applied to Data Vault in order to create Point-In-Time (PIT) and Dimension tables. To keep some differentiation between the two let’s agree that a PIT table combines history from its direct surrounding tables (all Satellites for a Hub for example).\nJoining historised data sets together # The basic approach to join two time-variant tables together is to join them on their shared key (CUSTOMER_SK in the example below) as well as their Effective and Expiry Date/Times.\nMaking joining easier when you have multiple tables in scope # The technique explained in the previous section is geared towards combining two time-variant sources and it is not very straightforward (transparent) to add more time-variant tables using this approach. The join uses the same mechanism as above (greatest of the two effective dates \u0026lt; smallest of the two expiry dates) but the big difference is that each time-variant table is joined against the central ‘range’ set, making this a lot easier to configure and extend.\n","date":"16 February 2023","permalink":"/posts/creating-data-vault-point-in-time-and-dimension-ta/","section":"Posts","summary":"Creating-Data-Vault-Point-In-Time-and-Dimension-ta # Creating Data Vault Point-In-Time and Dimension tables: merging historical data sources - Roelant Vos # Created: February 1, 2020 2:17 PM URL: http://roelantvos.","title":"Creating-Data-Vault-Point-In-Time-and-Dimension-ta"},{"content":"CTEs-and-Window-Functions-Unleashing-the-Power-of # Ultimately you’ll need to read and refer to the PostgreSQL documentation on Window Functions and Window Function Calls, along with the tutorial when using them in your own queries. Window functions are a special class of analytic functions that are applied to windows of rows. The frame is another logical concept only used by functions that are relative to the frame (like first_value / last_value) I think of window functions as falling into two categories:\nFunctions that are also available as traditional analytics functions, such as count, sum, avg, etc. For functions that are also available when using GROUP BY, the primary advantage of using them with window functions is it becomes possible to do multiple different grouping operations in a single query. Window Function Examples: Using Multiple Traditional Aggregates # The following query illustrates the use of multiple count functions over different partitions to compute the percent of reservations that a given restaurant accounts for by locality (city). The things to note in this query are:\nUse of DISTINCT: Since window functions append columns to each row, without a DISTINCT operator, the query will give you back 1 row for every row in the join. I’ve touched on two of the most powerful features for Redshift analytics, window functions and CTEs, but there’s a lot more functionality in Postgres, much of which is also in RedShift. ","date":"16 February 2023","permalink":"/posts/ctes-and-window-functions-unleashing-the-power-of/","section":"Posts","summary":"CTEs-and-Window-Functions-Unleashing-the-Power-of # Ultimately you’ll need to read and refer to the PostgreSQL documentation on Window Functions and Window Function Calls, along with the tutorial when using them in your own queries.","title":"CTEs-and-Window-Functions-Unleashing-the-Power-of"},{"content":"Custom-Storage-Adapters-for-Ghost # Custom Storage Adapters for Ghost # Created: April 22, 2020 7:06 AM URL: https://ghost.org/docs/concepts/storage-adapters/ It\u0026rsquo;s possible to send your publication\u0026rsquo;s images to a 3rd party service, CDN or database using a custom storage module.\nUsing a custom storage adapter # By default Ghost stores images on your filesystem. In order to use a custom storage adapter, your custom configuration file needs to be updated to provide config for your new storage module and set it as active:\nstorage: { active: \u0026#39;my-module\u0026#39;, \u0026#39;my-module\u0026#39;: { key: \u0026#39;abcdef\u0026#39; } } The storage block should have 2 items:\nAn active key, which contains the name* of your module A key which reflects the name* of your module, containing any config your module needs Available custom storage adapters # local-file-store (default) saves images to the local filesystem http-store passes image requests through to an HTTP endpoint s3-store saves to Amazon S3 and proxies requests to S3 s3-store saves to Amazon S3 and works with 0.10+ qn-store saves to Qiniu ghost-cloudinary-store saves to Cloudinary ghost-storage-cloudinary saves to Cloudinary with RetinaJS support upyun-ghost-store saves to Upyun ghost-upyun-store saves to Upyun ghost-google-drive saves to Google Drive ghost-azure-storage saves to Azure Storage ghost-imgur saves to Imgur google-cloud-storage saves to Google Cloud Storage ghost-oss-store saves to Aliyun OSS ghost-b2 saves to Backblaze B2 ghost-github saves to GitHub pages-store saves to GitHub Pages or other pages service, e.g. Coding Pages WebDAV Storage saves to a WebDAV server. ghost-qcloud-cos saves to Tencent Cloud COS. Creating a custom storage adapter # In order to replace the storage module, use these requirements. Inside of content/adapters/storage create a file or a folder: content/adapters/storage/my-module.js or content/adapters/storage/my-module - if using a folder, create a file called index.js inside it\nBase adapter class inheritance # A custom storage adapter must inherit from your base storage adapter.\n\u0026#39;use strict\u0026#39;; var BaseAdapter = require(\u0026#39;ghost-storage-base\u0026#39;); class MyCustomAdapter extends BaseAdapter{ constructor() { super(); } } module.exports = MyCustomAdapter; Required methods # Your custom storage adapter must implement five required functions:\nsave - The .save() method stores the image and returns a promise which resolves the path from which the image should be requested in future. exists - Used by the base storage adapter to check whether a file exists or not serve - Ghost calls .serve() as part of its middleware stack, and mounts the returned function as the middleware for serving images delete read \u0026#39;use strict\u0026#39;; var BaseAdapter = require(\u0026#39;ghost-storage-base\u0026#39;); class MyCustomAdapter extends BaseAdapter{ constructor() { super(); } exists() { } save() { } serve() { return function customServe(req, res, next) { next(); } } delete() { } read() { } } module.exports = MyCustomAdapter; Summary # You have discovered how to use a custom storage module to replace the storage layer which handles images with custom code.\n","date":"16 February 2023","permalink":"/posts/custom-storage-adapters-for-ghost/","section":"Posts","summary":"Custom-Storage-Adapters-for-Ghost # Custom Storage Adapters for Ghost # Created: April 22, 2020 7:06 AM URL: https://ghost.","title":"Custom-Storage-Adapters-for-Ghost"},{"content":"Data-Engineer-Auth0 # N+1 \u0026gt; N. The Data engineer will help build, scale and maintain the entire data platform. The ideal candidate will have a deep technical understanding, hands-on experience in distributed computing, big data, ETL, dimensional modeling , columnar databases and data visualization. This job plays a key role in data infrastructure, analytics projects, and systems design and development. You should be passionate for continuous learning, experimenting, applying and contributing towards cutting edge open source Data technologies and software paradigms.\nResponsibilities: # Contributing at a senior-level to the data platform design by implementing a solid, robust, extensible design that supports key business flows. Performing all of the necessary data transformations to populate data lake. Experience with Data Warehouse design, ETL (Extraction, Transformation \u0026amp; Load), architecting efficient software designs for DW platform. ","date":"16 February 2023","permalink":"/posts/data-engineer-auth0/","section":"Posts","summary":"Data-Engineer-Auth0 # N+1 \u0026gt; N.","title":"Data-Engineer-Auth0"},{"content":"Data-Science-in-Production # Instead of doing that, we\u0026rsquo;ll have a server running that\u0026rsquo;s providing the service of listening to incoming requests (with model input data) and responding with the model\u0026rsquo;s output, which could be passed on to whatever the next step is (emailing to business stakeholder, passing it into another application, or maybe writing it to a database for further consumption down the line). We\u0026rsquo;ll create an app.py file in our directory so that we end up with this directory structure: ├── housing_app │ ├── app.py │ └── .venv In that file, we can copy/paste the following\nfrom flask import Flask app = Flask(__name__) @app.route(\u0026#39;/\u0026#39;) def hello_world(): return \u0026#39;Hello, World!\u0026#39; ### Flask Basics Getting back to business, let\u0026#39;s re-examine our `app.py`: app.py # from flask import Flask app = Flask(name) @app.route(\u0026rsquo;/\u0026rsquo;) def hello_world(): return \u0026lsquo;Hello, World!\u0026rsquo; Here\u0026rsquo;s what that could look like:\n#app.py from flask import Flask, request import pandas as pd import pickle app = Flask(__name__) @app.route(\u0026#39;/predict\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def predict(): if request.method == \u0026#39;POST\u0026#39;: # turn json format into pandas dataframe df = pd.DataFrame.from_records(request.json) # make predictions and create a df out of them pred = house_price_model.predict(df[[\u0026#39;age\u0026#39;]])) pred = pd.DataFrame(pred, columns=[\u0026#39;predicted_price\u0026#39;]) # combine with original data to return it all together pred = pd.concat([df, pred], axis=1) return(str(pred)) @app.before_first_request def load_model(): global house_price_model with open(\u0026#39;house_price_model\u0026#39;, \u0026#39;rb\u0026#39;) as handle: house_price_model = (pickle.load(handle)) We\u0026rsquo;ve made several changes to our app.py that we need to look at. We\u0026rsquo;ve done this because each directory within the ~/housing_app directory now represents a Docker container - we\u0026rsquo;ll have a web container that contains our Flask web application, and we have a new nginx container that is going to contain an nginx instance. There\u0026rsquo;s a lot we can do here, but for now we\u0026rsquo;ve left it pretty simple, with a few key commands:\nbuild: this tells Docker which directory to build the container from expose: this makes port 8000 available to other containers - this is key if containers need to interact with each other command: this is the final command that is run after the container is created - for out web container we need to create the container, but then also need the Flask application to run, using gunicorn ports: unlike expose this is making port 80 (for HTTP) available to us as the users, rather than available to other containers links: this creates a link between containers so that we can refer to \u0026lsquo;web\u0026rsquo; simply as \u0026lsquo;web\u0026rsquo; and not worry about the actual IP address Deployment # We\u0026rsquo;re finally here!\ndocker-machine create -d virtualbox housing-app-vm Once it\u0026rsquo;s done you can check that your new VM exists by doing:\ndocker-machine ls Now that it\u0026rsquo;s created we just need to make it our current active VM by executing:\neval \u0026#34;$(docker-machine env housing-app-vm)\u0026#34; With our housing_app_vm VM created and running, we\u0026rsquo;ll execute the following from ~/housing_app (the location of docker-compose.yml):\ndocker-compose build docker-compose up -d We\u0026rsquo;ll see a lot of magic happening in the terminal, but when it\u0026rsquo;s done, our containers will be running on our VM and we\u0026rsquo;ll be able to interact with them.\n","date":"16 February 2023","permalink":"/posts/data-science-in-production/","section":"Posts","summary":"Data-Science-in-Production # Instead of doing that, we\u0026rsquo;ll have a server running that\u0026rsquo;s providing the service of listening to incoming requests (with model input data) and responding with the model\u0026rsquo;s output, which could be passed on to whatever the next step is (emailing to business stakeholder, passing it into another application, or maybe writing it to a database for further consumption down the line).","title":"Data-Science-in-Production"},{"content":"Dbt-consulting-legal-form # Dbt consulting legal form # Created: April 22, 2020 12:11 PM URL: https://www.getdbt.com/docs/DataProcessingAddendum.pdf DataProcessingAddendum.pdf\n","date":"16 February 2023","permalink":"/posts/dbt-consulting-legal-form/","section":"Posts","summary":"Dbt-consulting-legal-form # Dbt consulting legal form # Created: April 22, 2020 12:11 PM URL: https://www.","title":"Dbt-consulting-legal-form"},{"content":"Dbt-Jinja-template # Dbt Jinja template # Created: April 27, 2020 10:10 AM URL: https://github.com/fishtown-analytics/dbt/blob/f07face7c00b070518148e2fb34068dfedfe7fe2/core/dbt/clients/_jinja_blocks.py#L198\n","date":"16 February 2023","permalink":"/posts/dbt-jinja-template/","section":"Posts","summary":"Dbt-Jinja-template # Dbt Jinja template # Created: April 27, 2020 10:10 AM URL: https://github.","title":"Dbt-Jinja-template"},{"content":"Deploying-Models-with-Flask-And-Docker-datascience # Deploying Models with Flask And Docker : datascience # Created: February 21, 2020 12:04 PM URL: https://www.reddit.com/r/datascience/comments/a80y1j/deploying_models_with_flask_and_docker/ ! new-icon.png A place for data science practitioners and professionals to discuss and debate data science career questions.\nThe weekly sticky post meant for any questions about getting started, studying, or transitioning into the data science field. N free videos, Y free book, Z free courses, etc\u0026hellip; No Surveys Remember the reddit self-promotion rule of thumb: \u0026ldquo;\u0026ldquo;For every 1 time you post self-promotional content, 9 other posts (submissions or comments) should not contain self-promotional content.\u0026rdquo;\u0026rdquo; ","date":"16 February 2023","permalink":"/posts/deploying-models-with-flask-and-docker-datascience/","section":"Posts","summary":"Deploying-Models-with-Flask-And-Docker-datascience # Deploying Models with Flask And Docker : datascience # Created: February 21, 2020 12:04 PM URL: https://www.","title":"Deploying-Models-with-Flask-And-Docker-datascience"},{"content":"Design-Patterns-in-Python # Design Patterns in Python # Created: June 21, 2020 6:25 PM URL: https://stackabuse.com/design-patterns-in-python/\nIntroduction # Design Patterns are reusable models for solving known and common problems in software architecture. A good presentation of a design pattern should include:\nName Motivating problem Solution Consequences Equivalent Problems # If you were thinking that that\u0026rsquo;s a pretty fuzzy concept, you\u0026rsquo;d be right. Patterns that could be applied to these sorts of problems are what we can meaningfully dub design patterns.\nDesign Patterns in Python # Traditionally, design patterns have been classified into three main categories: Creational, Structural, and Behavioral. There are also Python-specific design patterns that are created specifically around the problems that the structure of the language itself provides or that deal with problems in special ways that are only allowed because of the structure of the language.\nCreational Design Patterns # Structural Design Patterns # Behavioral Design Patterns # ***Coming soon!\nGlobal Object Pattern Prebound Method Pattern Sentinel Object Pattern See Also # ","date":"16 February 2023","permalink":"/posts/design-patterns-in-python/","section":"Posts","summary":"Design-Patterns-in-Python # Design Patterns in Python # Created: June 21, 2020 6:25 PM URL: https://stackabuse.","title":"Design-Patterns-in-Python"},{"content":"Design-Thinking-101 # History of Design Thinking # It is a common misconception that design thinking is new. Cue design thinking, a formalized framework of applying the creative design process to traditional business problems.\nWhat — Definition of Design Thinking # Design thinking is an ideology supported by an accompanying process. As Don Norman preaches, “we need more design doing.” Design thinking does not free you from the actual design doing. There are numerous reasons to engage in design thinking, enough to merit a standalone article, but in summary, design thinking achieves all these advantages at the same time:\nIt is a user-centered process that starts with user data, creates design artifacts that address real and not imaginary user needs, and then tests those artifacts with real users. Design%20Thinking%20101%20897efda989854c0ea65be076b32dbc83/designthinking_illustration_final2-02.png Scalability — Think Bigger # The packaged and accessible nature of design thinking makes it scalable. Learn more about design thinking in the full-day course Generating Big Ideas with Design Thinking.\n","date":"16 February 2023","permalink":"/posts/design-thinking-101/","section":"Posts","summary":"Design-Thinking-101 # History of Design Thinking # It is a common misconception that design thinking is new.","title":"Design-Thinking-101"},{"content":"Developing-a-Single-Page-App-with-Flask-and-Vue-js # Install: Enable the Bootstrap Vue library in client/src/main.js:\nPOST Route # Server # Update the existing route handler to handle POST requests for adding a new book: Update the imports: With the Flask server running, you can test the POST route in a new terminal tab: You should see: You should also see the new book in the response from the http://localhost:5000/books endpoint. Developing%20a%20Single%20Page%20App%20with%20Flask%20and%20Vue%20js%20900a7939c8244b579780a034209512c4/add-new-book.gif\nAlert Component # Next, let\u0026rsquo;s add an Alert component to display a message to the end user after a new book is added. Developing%20a%20Single%20Page%20App%20with%20Flask%20and%20Vue%20js%20900a7939c8244b579780a034209512c4/alert-2.png To make it dynamic, so that a custom message is passed down, use a binding expression in Books.vue: Add the message to the data options, in Books.vue as well: Then, within addBook, update the message: Finally, add a v-if, so the alert is only displayed if showMessage is true: Add showMessage to the data: Update addBook again, setting showMessage to true: Test it out! Handle cancel button click\n(1) Add modal and form # First, add a new modal to the template, just below the first modal: Add the form state to the data part of the script section:\nChallenge: Instead of using a new modal, try using the same modal for handling both POST and PUT requests.\n(2) Handle update button click # Update the \u0026ldquo;update\u0026rdquo; button in the table: Add a new method to update the values in editForm: Then, add a method to handle the form submit:\n(3) Wire up AJAX request # (4) Alert user # Update updateBook:\n(5) Handle cancel button click # Add method: Update initForm: Make sure to review the code before moving on. Developing%20a%20Single%20Page%20App%20with%20Flask%20and%20Vue%20js%20900a7939c8244b579780a034209512c4/update-book.gif\nDELETE Route # Server # Update the route handler:\nClient # Update the \u0026ldquo;delete\u0026rdquo; button like so: Add the methods to handle the button click and then remove the book: Now, when the user clicks the delete button, the onDeleteBook method is fired, which, in turn, fires the removeBook method. Developing%20a%20Single%20Page%20App%20with%20Flask%20and%20Vue%20js%20900a7939c8244b579780a034209512c4/developing_spa_flask_vue.png\nDeveloping a Single Page App with Flask and Vue.js # The following is a step-by-step walkthrough of how to set up a basic CRUD app with Vue and Flask.\n","date":"16 February 2023","permalink":"/posts/developing-a-single-page-app-with-flask-and-vue-js/","section":"Posts","summary":"Developing-a-Single-Page-App-with-Flask-and-Vue-js # Install: Enable the Bootstrap Vue library in client/src/main.","title":"Developing-a-Single-Page-App-with-Flask-and-Vue-js"},{"content":"Dia-taxis # Diátaxis # Created: October 26, 2022 1:03 AM URL: https://diataxis.fr/ A systematic framework for technical documentation authoring. The Diátaxis framework aims to solve the problem of structure in technical documentation. It adopts a systematic approach to understanding the needs of documentation users in their cycle of interaction with a product. Diátaxis identifies four modes of documentation - tutorials, how-to guides, technical reference and explanation. Dia%CC%81taxis%20469b500281904446b4fe5b7867e08d79/diataxis.png Technical documentation should be structured explicitly around these four types, and should keep them all separate and distinct from each other.\n—Adam Schwartz (@AdamSchwartz)\nDiátaxis promises to make documentation and projects better, and the teams that work with them more successful. Diátaxis is proven in practice across a wide variety of fields and applications, in large and small, open and proprietary documentation projects.\n","date":"16 February 2023","permalink":"/posts/dia-taxis/","section":"Posts","summary":"Dia-taxis # Diátaxis # Created: October 26, 2022 1:03 AM URL: https://diataxis.","title":"Dia-taxis"},{"content":"Distill-Why-do-we-need-Flask-Celery-and-Redis-w # ** In the next section, we’ll discuss the various components of Mcdonald’s task queue and how they map to the three technologies above.\nDiving into Mcdonald’s Task Queue # In the Mcdonalds near our office, there are three major components that are in play:\nThe Ate/Kuya cashier: they’re the ones who talk to customers, take their orders, and give them their reference numbers (remember that in the Mcdo near our apartment, they’re also the ones who prepares the meal, which is inefficient). The database behind the LED screen: the LED screen displays information on the customers’ reference numbers and order status, but we know that it’s job is to only show information. The cashier takes their order, put it in the database queue (with a PENDING status), so that free workers can take them on. [Distill%20Why%20do%20we%20need%20Flask,%20Celery,%20and%20Redis%20(w%204b3cfc055f3f44beb14348e7163ece7b/task_queue_02.svg](Distill%20Why%20do%20we%20need%20Flask,%20Celery,%20and%20Redis%20(w%204b3cfc055f3f44beb14348e7163ece7b/task_queue_02.svg) We consider ourselves as the Client, for we’re the ones asking for a service or making a Request. The components of the Mcdonald’s task queue: cashier, worker, database behind LED screen How these components look like in more general terms: application, worker, database backend. The App takes the request, put it in the database queue (with a PENDING status), so that Celery workers can take them on. ","date":"16 February 2023","permalink":"/posts/distill-why-do-we-need-flask-celery-and-redis-w/","section":"Posts","summary":"Distill-Why-do-we-need-Flask-Celery-and-Redis-w # ** In the next section, we’ll discuss the various components of Mcdonald’s task queue and how they map to the three technologies above.","title":"Distill-Why-do-we-need-Flask-Celery-and-Redis-w"},{"content":"DISTINCT-ON-The-confusing-unique-and-useful-featu # **\nThe DISTINCT ON clause will only return the first row based on the DISTINCT ON(column) and ORDER BY clause provided in the query.\nIs there any difference between the results of DISTINCT and DISTINCT ON queries? # DISTINCT%20ON%20The%20confusing,%20unique%20and%20useful%20featu%200254dbd04e6143d5af762bdaf289732b/distinct-on-query-output-yogesh-chauhan-1.jpg DISTINCT ON query: Output: ! DISTINCT%20ON%20The%20confusing,%20unique%20and%20useful%20featu%200254dbd04e6143d5af762bdaf289732b/distinct-on-query-output-yogesh-chauhan-2.jpg ! DISTINCT%20ON%20The%20confusing,%20unique%20and%20useful%20featu%200254dbd04e6143d5af762bdaf289732b/distinct-on-query-output-yogesh-chauhan-6.jpg So, if we add all the columns to the GROUP BY clause then we will basically get the same results as just simple DISTINCT query. DISTINCT%20ON%20The%20confusing,%20unique%20and%20useful%20featu%200254dbd04e6143d5af762bdaf289732b/distinct-on-query-output-yogesh-chauhan-8.jpg So, we need to write down query like this: SELECT country, COUNT(contact_name), COUNT(company_name) FROM customers GROUP BY country; Output: ! DISTINCT%20ON%20The%20confusing,%20unique%20and%20useful%20featu%200254dbd04e6143d5af762bdaf289732b/distinct-on-query-output-yogesh-chauhan-9.jpg So, as we can see in the screenshot above, we are getting the number of rows using GROUP BY but if we want the first result from all those groups, we need to use DISTINCT ON and that\u0026rsquo;s why I consider it a powerful feature of Postgres.\n","date":"16 February 2023","permalink":"/posts/distinct-on-the-confusing-unique-and-useful-featu/","section":"Posts","summary":"DISTINCT-ON-The-confusing-unique-and-useful-featu # **","title":"DISTINCT-ON-The-confusing-unique-and-useful-featu"},{"content":"Do-Things-that-Don-t-Scale # Do Things that Don\u0026rsquo;t Scale # Created: February 29, 2020 10:50 AM URL: http://paulgraham.com/ds.html Untitled\n","date":"16 February 2023","permalink":"/posts/do-things-that-don-t-scale/","section":"Posts","summary":"Do-Things-that-Don-t-Scale # Do Things that Don\u0026rsquo;t Scale # Created: February 29, 2020 10:50 AM URL: http://paulgraham.","title":"Do-Things-that-Don-t-Scale"},{"content":"Dockerize-the-jaffle-shop-by-davehowell-Pull-Req # Dockerize the jaffle shop by davehowell · Pull Request #26 · fishtown-analytics/jaffle_shop # Created: April 27, 2020 10:45 AM URL: https://github.com/fishtown-analytics/jaffle_shop/pull/26/files 41717489 Untitled\n","date":"16 February 2023","permalink":"/posts/dockerize-the-jaffle-shop-by-davehowell-pull-req/","section":"Posts","summary":"Dockerize-the-jaffle-shop-by-davehowell-Pull-Req # Dockerize the jaffle shop by davehowell · Pull Request #26 · fishtown-analytics/jaffle_shop # Created: April 27, 2020 10:45 AM URL: https://github.","title":"Dockerize-the-jaffle-shop-by-davehowell-Pull-Req"},{"content":"Does-Radiant-Floor-Cooling-Make-Sense-GreenBuild # GreenBuildingAdvisor Created: May 23, 2020 11:45 AM URL: https://www.greenbuildingadvisor.com/article/does-radiant-floor-cooling-make-sense Does%20Radiant%20Floor%20Cooling%20Make%20Sense%20-%20GreenBuild%20525f42befcb747bd9779c12b6357e0f7/eyJlZGl0cyI6eyJyZXNpemUiOnsid2lkdGgiOjEwODB9fSwiYnVja2V0IjoiZ3JlZW5idWlsZGluZ2Fkdmlzb3IuczMudGF1bnRvbmNsb3VkLmNvbSIsImtleSI6ImFwcFwvdXBsb2Fkc1wvMjAxOFwvMDhcLzA3MjMwNDU1XC9TcG90bGlnaHQgY3JvcHBlZF8wLW1haW4tNzAweDU1Ny5qcGcifQ Radiant heating and cooling: Justin Gibbs shared this photo of Warmboard panels that will be used in his radiant heating and cooling system. “There are a number of reasons why I am still looking at radiant floor heating, most of which are not technical justifications but more because we really really want it … Having made a decision on radiant floor heating, does it make sense to go with radiant floor cooling?” In areas with high summer humidity, water vapor can condense on cold water lines. Similarly, water can accumulate on flooring cooled by a radiant floor cooling system. Some amount of latent cooling with air coils may need to be included if the design water temperatures are otherwise too low.” Radiant cooling panels in the ceiling or walls are more effective per square foot than radiant floor systems, he adds, due to the tendency of cold air to pool near the floor rather than mix with room air. Gibbs adds that floors must be insulated when the house has a radiant heating (and cooling) system, and in his case forgetting to put that into the budget proved to be a “sizable omission.” Is the insulation really necessary? # If you want a radiant floor to also serve as a radiant ceiling for the space below, insulation gets in the way.” Gibbs says he had similar concerns before he spoke with the system designer. “It is climate, system, and enclosure dependent…[and so here are] some examples: “For cold, dry climates and good-to-great buildings with sensible cooling flux under 13 Btu/hr/ft2, fabricate the floor cooling system with masonry type surface (tile, stained concrete, terrazzo etc.\n","date":"16 February 2023","permalink":"/posts/does-radiant-floor-cooling-make-sense-greenbuild/","section":"Posts","summary":"Does-Radiant-Floor-Cooling-Make-Sense-GreenBuild # GreenBuildingAdvisor Created: May 23, 2020 11:45 AM URL: https://www.","title":"Does-Radiant-Floor-Cooling-Make-Sense-GreenBuild"},{"content":"Don-t-nest-your-curlies # Don\u0026rsquo;t nest your curlies # Created: March 6, 2020 10:01 AM URL: https://docs.getdbt.com/docs/dont-nest-your-curlies Don\u0026rsquo;t Nest Your Curlies\nIf dbt errors out earlyand your models are building quite poorlydon\u0026rsquo;t post to the slackjust take a step backand check if you\u0026rsquo;re nesting your curlies.\nWhen writing jinja code in a dbt project, it may be tempting to nest expressions inside of each other. Take this example:\n{{ dbt_utils.date_spine( datepart=\u0026#34;day\u0026#34;, start_date=[ USE JINJA HERE ] ) }} To nest a jinja expression inside of another jinja expression, simply place the desired code (without curly brackets) directly into the expression. Once we\u0026rsquo;ve denoted that we\u0026rsquo;re inside a jinja expression (using the {{ syntax), no further curly brackets are required inside of the jinja expression. {{ dbt_utils.date_spine( datepart=\u0026ldquo;day\u0026rdquo;, start_date=\u0026quot;{{ var(\u0026lsquo;start_date\u0026rsquo;) }}\u0026quot; ) }}\nThere is one exception to this rule: curlies inside of curlies are acceptable in hooks (ie. Code like this is both valid, and encouraged: {{ config(post_hook=\u0026ldquo;grant select on {{ this }} to role bi_role\u0026rdquo;) }}\nSo why are curlies inside of curlies allowed in this case? \u0026#34;table\u0026#34;....` being executed against the database. ","date":"16 February 2023","permalink":"/posts/don-t-nest-your-curlies/","section":"Posts","summary":"Don-t-nest-your-curlies # Don\u0026rsquo;t nest your curlies # Created: March 6, 2020 10:01 AM URL: https://docs.","title":"Don-t-nest-your-curlies"},{"content":"Duplicating-a-repository-GitHub-Help # Duplicating a repository - GitHub Help # Created: January 30, 2020 8:55 PM URL: https://help.github.com/en/github/creating-cloning-and-archiving-repositories/duplicating-a-repository GitHub.com Creating, cloning, and archiving repositories Creating a repository on GitHub Duplicating a repository\nDuplicating a repository # To duplicate a repository without forking it, you can run a special clone command, then mirror-push to the new repository. Mac Windows Linux\nIn this article # Mirroring a repository Mirroring a repository that contains Git Large File Storage objects Mirroring a repository in another location Before you can duplicate a repository and push to your new copy, or mirror, of the repository, you must create the new repository on GitHub. Open Terminal. old-repository Mirroring a repository that contains Git Large File Storage objects # Replace the example username with the name of the person or organization who owns the repository, and replace the example repository name with the name of the repository you\u0026rsquo;d like to duplicate. Push the repository\u0026rsquo;s Git Large File Storage objects to your mirror.\nold-repository Mirroring a repository in another location # If you want to mirror a repository in another location, including getting updates from the original, you can clone a mirror and periodically push the changes.\n","date":"16 February 2023","permalink":"/posts/duplicating-a-repository-github-help/","section":"Posts","summary":"Duplicating-a-repository-GitHub-Help # Duplicating a repository - GitHub Help # Created: January 30, 2020 8:55 PM URL: https://help.","title":"Duplicating-a-repository-GitHub-Help"},{"content":"Effortless-API-Request-Caching-with-Python-Redis # Effortless API Request Caching with Python \u0026amp; Redis | Red’s Digressions # Created: May 25, 2020 3:09 PM URL: https://rednafi.github.io/digressions/python/database/2020/05/25/python-redis-cache.html Recently, I was working with MapBox’s Route Optimization API. Here the requested coordinate-string will be the key and the response will be the corresponding value\nSetting a timeout on the records Serving new requests from cache if the records exist Only send a new request to MapBox API if the response is not cached and then add that response to cache Setting Up Redis \u0026amp; RedisInsight # To proceed with the above workflow, you’ll need to install and setup Redis database on your system.\n# docker-compose.yml version: \u0026#34;3.2\u0026#34; services: redis: container_name: redis-cont image: \u0026#34;redis:alpine\u0026#34; command: redis-server --requirepass ubuntu environment: - REDIS_PASSWORD=ubuntu - REDIS_REPLICATION_MODE=master ports: - \u0026#34;6379:6379\u0026#34; volumes: # save redisearch data to your current working directory - ./redis-data:/data command: # Save if 100 keys are added in every 10 seconds - \u0026#34;--save 10 100\u0026#34; # Set password - \u0026#34;--requirepass ubuntu\u0026#34; redisinsight: # redis db visualization dashboard container_name: redisinsight-cont image: redislabs/redisinsight ports: - 8001:8001 volumes: - redisinsight:/db volumes: redis-data: redisinsight: The above docker-compose file has two services, redis and redisinsight.\ndef route_optima(coordinates: str) -\u0026gt; dict: # First it looks for the data in redis cache data = get_routes_from_cache(key=coordinates) # If cache is found then serves the data from cache if data is not None: data = json.loads(data) data[\u0026#34;cache\u0026#34;] = True return data else: # If cache is not found then sends request to the MapBox API data = get_routes_from_api(coordinates) # This block sets saves the respose to redis and serves it directly if data[\u0026#34;code\u0026#34;] == \u0026#34;Ok\u0026#34;: data[\u0026#34;cache\u0026#34;] = False data = json.dumps(data) state = set_routes_to_cache(key=coordinates, value=data) if state is True: return json.loads(data) return data This part of the code wraps the original Route Optimization API and exposes that as a new endpoint.\ncoordinates = \u0026ldquo;90.3866,23.7182;90.3742,23.7461\u0026rdquo; # return route_optima(coordinates)\napp.py # import json import sys from datetime import timedelta import httpx import redis from fastapi import FastAPI def redis_connect() -\u0026gt; redis.client.Redis: try: client = redis.Redis( host=\u0026ldquo;localhost\u0026rdquo;, port=6379, password=\u0026ldquo;ubuntu\u0026rdquo;, db=0, socket_timeout=5, ) ping = client.ping() if ping is True: return client except redis.AuthenticationError: print(\u0026ldquo;AuthenticationError\u0026rdquo;) sys.exit(1) client = redis_connect() def get_routes_from_api(coordinates: str) -\u0026gt; dict: \u0026ldquo;\u0026ldquo;\u0026ldquo;Data from mapbox api.\u0026rdquo;\u0026rdquo;\u0026rdquo; state = client.setex(key, timedelta(seconds=3600), value=value,) return state def route_optima(coordinates: str) -\u0026gt; dict:\nFirst it looks for the data in redis cache # data = get_routes_from_cache(key=coordinates)\nIf cache is found then serves the data from cache # if data is not None: data = json.loads(data) data[\u0026ldquo;cache\u0026rdquo;] = True return data else:\nIf cache is not found then sends request to the MapBox API # data = get_routes_from_api(coordinates)\nThis block sets saves the respose to redis and serves it directly # if data[\u0026ldquo;code\u0026rdquo;] == \u0026ldquo;Ok\u0026rdquo;: data[\u0026ldquo;cache\u0026rdquo;] = False data = json.dumps(data) state = set_routes_to_cache(key=coordinates, value=data) if state is True: return json.loads(data) return data app = FastAPI() @app.get(\u0026quot;/route-optima/{coordinates}\u0026quot;) def view(coordinates: str) -\u0026gt; dict: \u0026ldquo;\u0026ldquo;\u0026ldquo;This will wrap our original route optimization API and incorporate Redis Caching.\ncoordinates = \u0026ldquo;90.3866,23.7182;90.3742,23.7461\u0026rdquo; # return route_optima(coordinates)\nYou can copy the complete code to a file named `app.py` and run the app using the command below (assuming redis, redisinsight is running and you’ve installed the dependencies beforehand): uvicorn app.app:app \u0026ndash;host 0.0.0.0 \u0026ndash;port 5000 \u0026ndash;reload\nThis will run a local server where you can send new request with coordinates. ","date":"16 February 2023","permalink":"/posts/effortless-api-request-caching-with-python-redis/","section":"Posts","summary":"Effortless-API-Request-Caching-with-Python-Redis # Effortless API Request Caching with Python \u0026amp; Redis | Red’s Digressions # Created: May 25, 2020 3:09 PM URL: https://rednafi.","title":"Effortless-API-Request-Caching-with-Python-Redis"},{"content":"Emotional-Debt # Emotional Debt # Created: April 17, 2020 9:43 PM URL: https://news.ycombinator.com/item?id=22875923\nI saw some TV show where some guy said he flew more than a hundred missions in the Air Force and was shot at in all of them. With nerves of steel, he made a killing. I think the trouble with anecdotes is there is always a counter anecdote. Someone who had spent a fair amount of time in the military, including combat, decided to become a doctor later in life. Nothing in the military/combat prepared him for this. I don\u0026rsquo;t know how common this sentiment is in the medical industry, but I do remember a medical student friend of mine talking about how in smaller cities, surgeons work hard to make the lives of other surgeons difficult, in the hopes that they will leave and they can thus charge hospitals more to perform surgeries (even though the need for more surgeons existed). Anyway, I don\u0026rsquo;t see this as a \u0026ldquo;first world problem\u0026rdquo; or a \u0026ldquo;good problem to have\u0026rdquo;.\n","date":"16 February 2023","permalink":"/posts/emotional-debt/","section":"Posts","summary":"Emotional-Debt # Emotional Debt # Created: April 17, 2020 9:43 PM URL: https://news.","title":"Emotional-Debt"},{"content":"ETL-and-ELT-design-patterns-for-lake-house-archite # ETL and ELT design patterns for lake house architecture using Amazon Redshift: Part 2 | AWS Big Data Blog # Created: January 28, 2020 8:42 PM Tags: Data URL: https://aws.amazon.com/blogs/big-data/etl-and-elt-design-patterns-for-lake-house-architecture-using-amazon-redshift-part-2/?nc1=b_rp Part 1 of this multi-post series, ETL and ELT design patterns for lake house architecture using Amazon Redshift: Part 1, discussed common customer use cases and design best practices for building ELT and ETL data processing pipelines for data lake architecture using Amazon Redshift Spectrum, Concurrency Scaling, and recent support for data lake export. You also want to share the data with other AWS services such as Athena with its pay-per-use and serverless ad hoc and on-demand query model, AWS Glue and Amazon EMR for performing ETL operations on the unloaded data and data integration with your other datasets (such as ERP, finance, or third-party data) stored in your data lake, and Amazon SageMaker for machine learning. Now that you have an external data catalog in AWS Glue named etlblogpost, create an external schema in the persistent cluster named eltblogpost using the following SQL from your SQL Workbench/J (replace **):\ncreate external schema spectrum_eltblogpost from data catalog database \u0026#39;eltblogpost\u0026#39; iam_role \u0026#39;arn:aws:iam:::role/redshift-elt-test-role\u0026#39; create external database if not exists; Using Spectrum, you can now query the three AWS Glue catalog tables you set up earlier. \u0026ldquo;monthly_revenue_by_region_manufacturer_category_brand\u0026rdquo; where year = \u0026lsquo;1992\u0026rsquo; and month = \u0026lsquo;March\u0026rsquo; and supplier_region = \u0026lsquo;AFRICA\u0026rsquo; order by revenue desc limit 10; brand | category | manufacturer | revenue \u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;-+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026ndash; MFGR#1313 | MFGR#13 | MFGR#1 | 5170356068 MFGR#5325 | MFGR#53 | MFGR#5 | 5106463527 MFGR#3428 | MFGR#34 | MFGR#3 | 5055551376 MFGR#2425 | MFGR#24 | MFGR#2 | 5046250790 MFGR#4126 | MFGR#41 | MFGR#4 | 5037843130 MFGR#219 | MFGR#21 | MFGR#2 | 5018018040 MFGR#159 | MFGR#15 | MFGR#1 | 5009626205 MFGR#5112 | MFGR#51 | MFGR#5 | 4994133558 MFGR#5534 | MFGR#55 | MFGR#5 | 4984369900 MFGR#5332 | MFGR#53 | MFGR#5 | 4980619214\n- Monthly revenue for the region `AMERICA` for the year 1995 across all brands: SELECT month, sum(revenue) revenue FROM \u0026ldquo;spectrum_eltblogpost\u0026rdquo;. \u0026ldquo;yearly_revenue_by_city\u0026rdquo; where supplier_city in (\u0026lsquo;ETHIOPIA 4\u0026rsquo;) and year between \u0026lsquo;1992\u0026rsquo; and \u0026lsquo;1995\u0026rsquo; and month = \u0026lsquo;December\u0026rsquo; group by year, supplier_city order by year, supplier_city; year | supplier_city | revenue \u0026mdash;\u0026ndash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;+\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; 1992 | ETHIOPIA 4 | 91006583025 1993 | ETHIOPIA 4 | 90617597590 1994 | ETHIOPIA 4 | 92015649529 1995 | ETHIOPIA 4 | 89732644163\nWhen the data is in S3 and cataloged in the AWS Glue catalog, you can query the same catalog tables using Athena, AWS Glue, Amazon EMR, Amazon SageMaker, [Amazon QuickSight](https://aws.amazon.com/quicksight/), and many more AWS services that have seamless integration with S3. You want to query the unloaded data from your data lake using Redshift Spectrum if you have an existing cluster, Athena with its pay-per-use and serverless ad hoc and on-demand query model, AWS Glue and Amazon EMR for performing ETL operations on the unloaded data and data integration with your other datasets in your data lake, and Amazon SageMaker for machine learning. Connect to the cluster from the SQL Workbench/J.To use Redshift Spectrum for querying data from data lake (S3), you need to have the following: - An Amazon Redshift cluster and a SQL client (SQL Workbench/J or another tool of your choice) that can connect to your cluster and execute SQL commands. ","date":"16 February 2023","permalink":"/posts/etl-and-elt-design-patterns-for-lake-house-archite/","section":"Posts","summary":"ETL-and-ELT-design-patterns-for-lake-house-archite # ETL and ELT design patterns for lake house architecture using Amazon Redshift: Part 2 | AWS Big Data Blog # Created: January 28, 2020 8:42 PM Tags: Data URL: https://aws.","title":"ETL-and-ELT-design-patterns-for-lake-house-archite"},{"content":"example-dags-salesforce-data-processing-py-at-mast # example-dags/salesforce_data_processing.py at master · astronomerio-archive/example-dags · GitHub # Created: February 21, 2020 12:09 PM URL: https://github.com/astronomerio-archive/example-dags/blob/master/salesforce_to_slack/salesforce_data_processing.py 40177851 GitHub is home to over 40 million developers working together to host and review code, manage projects, and build software together. Sign up Untitled\n","date":"16 February 2023","permalink":"/posts/example-dags-salesforce-data-processing-py-at-mast/","section":"Posts","summary":"example-dags-salesforce-data-processing-py-at-mast # example-dags/salesforce_data_processing.","title":"example-dags-salesforce-data-processing-py-at-mast"},{"content":"Explained-Visually # Explained Visually # Created: July 13, 2020 9:29 AM URL: https://setosa.io/ev/ Explained Visually (EV) is an experiment in making hard ideas intuitive inspired the work of Bret Victor\u0026rsquo;s Explorable Explanations. Sign up to hear about the latest.\n","date":"16 February 2023","permalink":"/posts/explained-visually/","section":"Posts","summary":"Explained-Visually # Explained Visually # Created: July 13, 2020 9:29 AM URL: https://setosa.","title":"Explained-Visually"},{"content":"Explorable-Explanations # Proposition 21: Vehicle License Fee for State Parks # The way it is now: California has 278 state parks, including state beaches and historic parks. An explanation requires an author, to interpret the results of the model, and present them to the reader via language and graphics. Contextual Information As much as we might wish authors to write explorable explanations, many won\u0026rsquo;t. And even authors with good intentions can\u0026rsquo;t predict everything that the reader will want to explore. How can active readers ask questions and question assumptions while **reading normal text? Treating the author\u0026rsquo;s text as a base layer for hosting the reader\u0026rsquo;s own explorations. ** How do we make readers demand explorable explanations, and reject static text?\n","date":"16 February 2023","permalink":"/posts/explorable-explanations/","section":"Posts","summary":"Explorable-Explanations # Proposition 21: Vehicle License Fee for State Parks # The way it is now: California has 278 state parks, including state beaches and historic parks.","title":"Explorable-Explanations"},{"content":"Export-MySQL-Data-to-Amazon-S3-Using-AWS-Data-Pipe # Export MySQL Data to Amazon S3 Using AWS Data Pipeline - AWS Data Pipeline # Created: April 18, 2020 3:05 PM URL: https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-copydata-mysql.html This tutorial walks you through the process of creating a data pipeline to copy data (rows) from a table in MySQL database to a CSV (comma-separated values) file in an Amazon S3 bucket and then sending an Amazon SNS notification after the copy activity completes successfully. You will use an EC2 instance provided by AWS Data Pipeline for this copy activity. Pipeline Objects The pipeline uses the following objects:\n","date":"16 February 2023","permalink":"/posts/export-mysql-data-to-amazon-s3-using-aws-data-pipe/","section":"Posts","summary":"Export-MySQL-Data-to-Amazon-S3-Using-AWS-Data-Pipe # Export MySQL Data to Amazon S3 Using AWS Data Pipeline - AWS Data Pipeline # Created: April 18, 2020 3:05 PM URL: https://docs.","title":"Export-MySQL-Data-to-Amazon-S3-Using-AWS-Data-Pipe"},{"content":"Extract-HubSpot-data-for-analytics-Fivetran-implem # Extract HubSpot data for analytics | Fivetran implementation # Created: April 18, 2020 6:11 PM URL: https://fivetran.com/docs/applications/hubspot HubSpot is an inbound marketing and sales software that helps companies attract visitors, convert leads, and close customers.\nFeatures # Untitled\nSetup guide # Follow our step-by-step HubSpot setup guide to connect HubSpot with your destination using Fivetran connectors.\nHubSpot Marketing Schema # HubSpot Sales/CRM Schema # Deleted Rows # The HubSpot API doesn\u0026rsquo;t make deleted records available so Fivetran can\u0026rsquo;t mark deleted records in the destination warehouse.\nUpdate Frequency # Because the HubSpot API does not offer a mechanism to capture deletes, Fivetran infers deletes for the tables listed below. We perform a full import of these tables once a day, compare them to the previous version, and mark deletes using the _fivetran_deleted system column:\nCONTACT_LIST CONTACT_LIST_MEMBER DEAL_PIPELINE DEAL_PIPELINE_STAGE FORM HubSpot Developer Preview API # We maintain the following tables using the HubSpot developer preview API, so there are sometimes structural changes in these tables. We\u0026rsquo;re always happy to help with any other questions you might have! [Send us an email.\n","date":"16 February 2023","permalink":"/posts/extract-hubspot-data-for-analytics-fivetran-implem/","section":"Posts","summary":"Extract-HubSpot-data-for-analytics-Fivetran-implem # Extract HubSpot data for analytics | Fivetran implementation # Created: April 18, 2020 6:11 PM URL: https://fivetran.","title":"Extract-HubSpot-data-for-analytics-Fivetran-implem"},{"content":"Failing-Strategies # Failing%20Strategies%204f40da57c29e4b91ada38c335794c470/nov17-08-135629213-Lobo-Press-1200x675.jpg Many strategy execution processes fail because the firm does not have something worth executing. Many strategies fail to get implemented, despite the ample efforts of hard-working people, because they do not represent a set of clear choices. ** Another reason many implementation efforts fail is that executives see it as a pure top-down, two-step process: “The strategy is made; now we implement it.” That’s unlikely to work. Stanford professor Robert Burgelman said, “Successful firms are characterized by maintaining bottom-up internal experimentation and selection processes while simultaneously maintaining top-driven strategic intent.” This is quite a mouthful, but what Burgelman meant is that you indeed need a clear, top-down strategic direction (such as Hornby’s set of choices). Its top-down strategy was clear: (1) to be on the forefront of (2) semiconductor technology and (3) to be aimed at the memory business (not coincidentally a set of three clear choices!). Identifying and countering the bad habits that keep your strategy from getting executed is not an easy process, but — as I elaborate on in my book Breaking Bad Habits — there are various practices you can build into your organization to make it work. For a successful strategy implementation process, however, it is useful to put the default the other way around: Change it unless it is crystal clear that the old way is substantially better.\n","date":"16 February 2023","permalink":"/posts/failing-strategies/","section":"Posts","summary":"Failing-Strategies # Failing%20Strategies%204f40da57c29e4b91ada38c335794c470/nov17-08-135629213-Lobo-Press-1200x675.","title":"Failing-Strategies"},{"content":"Family-Sharing-Apple-Support # Family Sharing makes it easy for up to six family members to share App Store, music, movie, TV, and book purchases, an Apple Music Family Subscription, an Apple Arcade subscription, an Apple News+ subscription, an Apple TV+ subscription, and an iCloud storage plan — all without sharing each other\u0026rsquo;s Apple accounts. You can also set up Family Sharing on your iPhone, iPad, or iPod touch when you sign up for an Apple Music Family Subscription, an Apple Arcade subscription, an Apple News+ subscription, an Apple TV+ subscription, or when you buy an iCloud storage plan to share with your family. With Family Sharing, you can share an Apple Music Family Subscription, an Apple Arcade subscription, an Apple News+ subscription, and an Apple TV+ subscription — all without sharing each other\u0026rsquo;s accounts. https://support.apple.com/library/content/dam/edam/applecare/images/en_US/appleid/ios13-iphone-xs-find-my-people-family.jpg With Family Sharing, you can start sharing your location with the rest of the family automatically. https://support.apple.com/library/content/dam/edam/applecare/images/en_US/iOS/ios13-ipad-pro-iphone-xs-screentime-hero.jpg With Family Sharing, you can share music, movies, apps, and more with family — and it now works with Screen Time on iOS and macOS. Or if you\u0026rsquo;re new to Family Sharing, tap Set up Screen Time for Family and follow the instructions to add a child and set up your family. To use Screen Time with Family Sharing, you need to be the family organizer or parent/guardian in your family group, on iOS 12 or later or macOS Catalina.\n","date":"16 February 2023","permalink":"/posts/family-sharing-apple-support/","section":"Posts","summary":"Family-Sharing-Apple-Support # Family Sharing makes it easy for up to six family members to share App Store, music, movie, TV, and book purchases, an Apple Music Family Subscription, an Apple Arcade subscription, an Apple News+ subscription, an Apple TV+ subscription, and an iCloud storage plan — all without sharing each other\u0026rsquo;s Apple accounts.","title":"Family-Sharing-Apple-Support"},{"content":"Feeding-data-to-AWS-Redshift-with-Airflow-Speake # Feeding data to AWS Redshift with Airflow - Speaker Deck # Created: February 21, 2020 12:17 PM URL: https://speakerdeck.com/fmarani/feeding-data-to-aws-redshift-with-airflow ! slide_0.jpg Talk I gave at EuroPython 2017\nWant more? # ","date":"16 February 2023","permalink":"/posts/feeding-data-to-aws-redshift-with-airflow-speake/","section":"Posts","summary":"Feeding-data-to-AWS-Redshift-with-Airflow-Speake # Feeding data to AWS Redshift with Airflow - Speaker Deck # Created: February 21, 2020 12:17 PM URL: https://speakerdeck.","title":"Feeding-data-to-AWS-Redshift-with-Airflow-Speake"},{"content":"Financial-Distancing-How-Venture-Capital-Follows-t # Financial Distancing: How Venture Capital Follows the Economy Down and Curtails Innovation # Created: May 18, 2020 11:20 PM URL: https://www.nber.org/papers/w27150 Although late-stage venture capital (VC) activity did not change dramatically in the first two months after the COVID-19 pandemic reached the U.S., early-stage VC activity declined by 38%. The particular sensitivity of early-stage VC investment to market conditions—which we show to be common across recessions spanning four decades from 1976 to 2017—raises questions about the pro-cyclicality of VC and its implications for innovation, especially in light of the common narrative that VC is relatively insulated from public markets. We find that the implications for innovation are not benign: innovation conducted by VC-backed firms in recessions is less highly cited, less original, less general, and less closely related to fundamental science. These effects are more pronounced for startups financed by early-stage venture funds. Given the important role that VC plays in financing breakthrough innovations in the economy, our findings have implications for the broader discussion on the nature of innovation across business cycles Machine-readable bibliographic record - MARC, RIS, BibTeX Document Object Identifier (DOI): 10.3386/w27150\n","date":"16 February 2023","permalink":"/posts/financial-distancing-how-venture-capital-follows-t/","section":"Posts","summary":"Financial-Distancing-How-Venture-Capital-Follows-t # Financial Distancing: How Venture Capital Follows the Economy Down and Curtails Innovation # Created: May 18, 2020 11:20 PM URL: https://www.","title":"Financial-Distancing-How-Venture-Capital-Follows-t"},{"content":"Fintech-The-Fourth-Platform-Part-Two # Fintech: The Fourth Platform - Part Two # Created: December 4, 2019 10:32 AM Tags: Finance, Startup URL: about:reader?url=https%3A%2F%2Fwww.forbes.com%2Fsites%2Fmatthewharris%2F2019%2F11%2F22%2Ffintech-the-fourth-platformpart-two%2F%2363853bc55be6#63853bc55be6\n","date":"16 February 2023","permalink":"/posts/fintech-the-fourth-platform-part-two/","section":"Posts","summary":"Fintech-The-Fourth-Platform-Part-Two # Fintech: The Fourth Platform - Part Two # Created: December 4, 2019 10:32 AM Tags: Finance, Startup URL: about:reader?","title":"Fintech-The-Fourth-Platform-Part-Two"},{"content":"Five-principles-that-will-keep-your-data-warehouse # In a data warehouse, you have a lot of objects to name — databases, schemas, relations, columns, users, and shared roles.\nUse a separate user for each human being and application connecting to your data warehouse # Before we dive into this, it’s worth noting that different data warehouses treat users, groups, and roles in very different ways (for anyone else that wants to fall down this rabbit hole, I wrote my findings up on Discourse).\nGrant privileges systematically # Data warehouses provide fine-grained privileges —you can grant any combination of the following privileges to a user or a shared role (in fact, these are just a subset of your options! There’s a balance to be found between being too generous with privileges, where you grant all privileges to the public group, and being too restrictive with privileges, where a super user has to run a series of complex grant statements for every user that wants to run a select statement. We then grant the following privileges to each of the shared roles in our warehouse:\nloader: create-schema transformer: read-schema, create-schema reporter: read-schema (limited to transformed schemas) I’ve done a very implementation-focused writeup of the exact statements involved in setting up a warehouse in this way over on Discourse. The implementation of this varies across data warehouses based on how the warehouse handles inheritance: Postgres: Create a separate role with superuser privileges, and grant the role to individual users as required. Redshift: Create a separate user, _super (e.g. claire_super) for each user that requires superuser privileges (superuser privileges can only be given to users, not groups). ","date":"16 February 2023","permalink":"/posts/five-principles-that-will-keep-your-data-warehouse/","section":"Posts","summary":"Five-principles-that-will-keep-your-data-warehouse # In a data warehouse, you have a lot of objects to name — databases, schemas, relations, columns, users, and shared roles.","title":"Five-principles-that-will-keep-your-data-warehouse"},{"content":"Fivetran-Makes-NetSuite-s-API-Usable-Fivetran-Blog # Fivetran Makes NetSuite’s API Usable | Fivetran Blog # Created: April 20, 2020 1:32 PM URL: https://fivetran.com/blog/netsuite-connector-article Understand the history of the Fivetran NetSuite connector and you\u0026rsquo;ll see why you shouldn\u0026rsquo;t try building your own. Fivetran%20Makes%20NetSuite%E2%80%99s%20API%20Usable%20Fivetran%20Blog%20816601f8816543218e44b0d4b5607703/netsuite-connector-article.png NetSuite is one of the premier providers of ERP software, billing itself as a cloud-based replacement for QuickBooks, Microsoft Dynamics GP, SAP and others. The NetSuite connector is currently a major selling point for Fivetran, not only because of its general usefulness as a piece of software but also because of the complete and utter intractability of its API. As the Fivetran team began to build the NetSuite connector using the standard SOAP API, we quickly realized that there was a three-way disconnect between the software’s UI elements, documentation, and data feed. The Fivetran team is still continually tuning and optimizing the NetSuite connector. The challenges the Fivetran team experienced while building the NetSuite connector have stymied company after company, turning the Fivetran NetSuite connector into a staple among our frustrated customers. By purchasing the Fivetran NetSuite connector, you can have your cake and eat it, too — and obviate the frustration and false steps that would otherwise consume the attention of your engineers.\n","date":"16 February 2023","permalink":"/posts/fivetran-makes-netsuite-s-api-usable-fivetran-blog/","section":"Posts","summary":"Fivetran-Makes-NetSuite-s-API-Usable-Fivetran-Blog # Fivetran Makes NetSuite’s API Usable | Fivetran Blog # Created: April 20, 2020 1:32 PM URL: https://fivetran.","title":"Fivetran-Makes-NetSuite-s-API-Usable-Fivetran-Blog"},{"content":"Flask-and-Firebase-and-Pyrebase-Oh-my-upperlin # Connecting to your Firebase DB # To connect to your firebase database, you’ll need to set up a dictionary variable with your configuration data in your routes.py file. Back to the routes.py file — once you’ve set up your config variable correctly, add the following code underneath: This code connects to our project using our configuration data, and then creates a db variable to interact with the Firebase database that we created earlier. Unfortunately that events list will not persist (save) our data, so we\u0026rsquo;re going to replace this with a call to our firebase dbvariable: To test if this worked (you won’t see anything change on the index page since we haven’t updated it yet), head to the Firebase Console and open up your project, clicking on the ‘database’ section. You should see that the event you submitted is now a “node” in your database, under the node “events” (which was created as well since this was your first upload to that node — future submissions to db.child(\u0026quot;events\u0026quot;) will just add on to that node). Next, inside of our index() method we\u0026rsquo;ll create a new variabe called db_events that pulls in the data we want from Firebase (the \u0026ldquo;events\u0026rdquo; node), and we\u0026rsquo;ll pass that in to our template for rendering: Now reload your app and you should hopefully see data getting pulled in from your Firebase database!\nAdd another field to your events form and then make sure that that content gets rendered correctly on your index page Build a similar app to inventory anything else you’re interested in: animals, groceries, todo items, etc.To test if this worked (you won’t see anything change on the index page since we haven’t updated it yet), head to the Firebase Console and open up your project, clicking on the ‘database’ section. You should see that the event you submitted is now a “node” in your database, under the node “events” (which was created as well since this was your first upload to that node — future submissions to db.child(\u0026quot;events\u0026quot;) will just add on to that node). ","date":"16 February 2023","permalink":"/posts/flask-and-firebase-and-pyrebase-oh-my-upperlin/","section":"Posts","summary":"Flask-and-Firebase-and-Pyrebase-Oh-my-upperlin # Connecting to your Firebase DB # To connect to your firebase database, you’ll need to set up a dictionary variable with your configuration data in your routes.","title":"Flask-and-Firebase-and-Pyrebase-Oh-my-upperlin"},{"content":"Flask-Bulma-CSS-BulmaPlay-Open-Source-App-AppSee # Flask Bulma CSS - BulmaPlay Open-Source App | AppSeed App Generator # Created: February 4, 2020 11:25 AM URL: https://flask-bulma-css.appseed.us/ ! Flask%20Bulma%20CSS%20-%20BulmaPlay%20Open-Source%20App%20AppSee%204a41f27a78f044e3b39294dac321378c/bulma-css-app-mockup-low.png Lorem ipsum dolor sit amet, elit deleniti dissentias quo eu, hinc minim appetere te usu, ea case duis scribentur has. Duo te consequat elaboraret, has quando suavitate at. Lorem ipsum dolor sit amet, elit deleniti dissentias quo eu, hinc minim appetere te usu, ea case duis scribentur has. Duo te consequat elaboraret, has quando suavitate at. Lorem ipsum dolor sit amet, elit deleniti dissentias quo eu, hinc minim appetere te usu, ea case duis scribentur has. Duo te consequat elaboraret, has quando suavitate at.\n","date":"16 February 2023","permalink":"/posts/flask-bulma-css-bulmaplay-open-source-app-appsee/","section":"Posts","summary":"Flask-Bulma-CSS-BulmaPlay-Open-Source-App-AppSee # Flask Bulma CSS - BulmaPlay Open-Source App | AppSeed App Generator # Created: February 4, 2020 11:25 AM URL: https://flask-bulma-css.","title":"Flask-Bulma-CSS-BulmaPlay-Open-Source-App-AppSee"},{"content":"Flask-GoogleCharts-Flask-GoogleCharts-0-0-docume # Flask-GoogleCharts — Flask-GoogleCharts 0.0 documentation # Created: March 24, 2020 11:56 AM URL: https://pythonhosted.org/Flask-GoogleCharts/\nSet Up # Google charts are controlled through a GoogleCharts instance:\nfrom flask import Flask from flask_googlecharts import GoogleCharts app = Flask(__name__) charts = GoogleCharts(app) You may also set up the GoogleCharts instance later using the init_app method:\ncharts = GoogleCharts() app = Flask(__name__) charts.init_app(app) Creating Charts # Import a chart type and declare it in your view, and give it a name at a minimum:\nfrom flask_googlecharts import BarChart my_chart = BarChart(\u0026#34;my_chart\u0026#34;) The name you declare will be used to access your chart in the template, and also to name the resulting JavaScript variables and HTML tags, so it must start with a letter and not contain any spaces. You can customize your chart by setting the options argument:\nmy_chart = BarChart(\u0026#34;my_chart\u0026#34;, options={\u0026#39;title\u0026#39;: \u0026#39;My Chart\u0026#39;}) Adding Data to a Chart # If you will be pulling JSON data from another endpoint in your application, just specify the url in the data_url argument:\nmy_chart = BarChart(\u0026#34;my_chart\u0026#34;, options={\u0026#39;title\u0026#39;: \u0026#39;My Chart\u0026#39;}, data_url=url_for(\u0026#39;data\u0026#39;)) You can also populate the chart using the addColumn and addRows methods on the chart:\nhot_dog_chart.add_column(\u0026#34;string\u0026#34;, \u0026#34;Competitor\u0026#34;) hot_dog_chart.add_column(\u0026#34;number\u0026#34;, \u0026#34;Hot Dogs\u0026#34;) hot_dog_chart.add_rows([[\u0026#34;Matthew Stonie\u0026#34;, 62], [\u0026#34;Joey Chestnut\u0026#34;, 60], [\u0026#34;Eater X\u0026#34;, 35.5], [\u0026#34;Erik Denmark\u0026#34;, 33], [\u0026#34;Adrian Morgan\u0026#34;, 31]]) Including Charts in Templates # First, add the chart javascript to your template:\nFlask-GoogleCharts Example {{ charts_init }} You must always include the Google Charts API loader on any pages that include charts. If you will be populating your charts with JSON data from another endpoint, you must also include jQuery. When the application is running in debug mode, GoogleCharts will log a warning if these dependencies are not met. Add the chart HTML to your template:\n{{ charts.my_chart }} ","date":"16 February 2023","permalink":"/posts/flask-googlecharts-flask-googlecharts-0-0-docume/","section":"Posts","summary":"Flask-GoogleCharts-Flask-GoogleCharts-0-0-docume # Flask-GoogleCharts — Flask-GoogleCharts 0.","title":"Flask-GoogleCharts-Flask-GoogleCharts-0-0-docume"},{"content":"flask-jsondash-schemas-md-at-master-christabor-f # Examples # Dashboard configuration Individual dashboard charts data Overrides # Supported.\n{ \u0026#34;data d\u0026#34;: 16, \u0026#34;data e\u0026#34;: 77, \u0026#34;data b\u0026#34;: 87, \u0026#34;data c\u0026#34;: 41, \u0026#34;data a\u0026#34;: 15 } Area # An object with each key corresponding to the line label, and a list of integer values.\nExamples # Dashboard configuration Individual dashboard charts data Overrides # Supported.\nNumber Group # Just like the single number option above, a number group has the same options (color and noformat), and general format, but supports multiple columns for each number (so you can build multiple big display of aggregate values in a single chart):\n[ { \u0026#34;title\u0026#34;: \u0026#34;Number of widgets sold in last day\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;This is a good sign\u0026#34;, \u0026#34;data\u0026#34;: 32515.0, \u0026#34;color\u0026#34;: \u0026#34;green\u0026#34;, }, { \u0026#34;title\u0026#34;: \u0026#34;New customers signed up this week\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;New user accounts created\u0026#34;, \u0026#34;data\u0026#34;: 740, }, { \u0026#34;title\u0026#34;: \u0026#34;Average Daily Users\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;(aka DAU)\u0026#34;, \u0026#34;data\u0026#34;: 541200, }, { \u0026#34;title\u0026#34;: \u0026#34;Max concurrent users this week\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Server load peak\u0026#34;, \u0026#34;data\u0026#34;: 123401, \u0026#34;color\u0026#34;: \u0026#34;orange\u0026#34;, \u0026#34;noformat\u0026#34;: true, }, ] You can also override the column width for each item, via \u0026quot;width\u0026quot;: \u0026quot;30%\u0026quot;.\nExamples # Dashboard configuration Individual dashboard charts data Overrides # Supported.\nExamples # Dashboard configuration Individual dashboard charts data Overrides # Supported. Format should be similar to d3 hierarchical layouts, like:\n{ \u0026#34;children\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;value\u0026#34;: 10 }, { \u0026#34;name\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;value\u0026#34;: 30, \u0026#34;children\u0026#34;: [...] } ] } Examples # Dashboard configuration Individual dashboard charts data Sparklines # Sparklines are \u0026ldquo;mini\u0026rdquo; charts that can be used inline.\n","date":"16 February 2023","permalink":"/posts/flask-jsondash-schemas-md-at-master-christabor-f/","section":"Posts","summary":"flask-jsondash-schemas-md-at-master-christabor-f # Examples # Dashboard configuration Individual dashboard charts data Overrides # Supported.","title":"flask-jsondash-schemas-md-at-master-christabor-f"},{"content":"From-What-is-a-Markov-Model-to-Here-is-how-Mark # From “What is a Markov Model” to “Here is how Markov Models Work” - By # Created: February 21, 2020 12:22 PM URL: https://hackernoon.com/from-what-is-a-markov-model-to-here-is-how-markov-models-work-1ac5f4629b71 To be honest, if you are just looking to answer the age old question of “what is a Markov Model” you should take a visit to Wikipedia (or just check the TLDR 😉), but if you are curious and looking to use some examples to aid in your understanding of what a Markov Model is, why Markov Models Matter, and how to implement a Markov Model stick around :) Show \u0026gt; Tell\nTLDR: “In probability theory, a Markov model is a stochastic model used to model randomly changing systems where it is assumed that future states depend only on the current state not on the events that occurred before it (that is, it assumes the Markov property). From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20529b583894574a42bfbc7a4658520ee2/1gnwXq22M3L3-bBXibqXqaw.png Distribution of keys By looking at the above distribution of keys we could deduce that the key fish comes up 4x as much as any other **key. Lets start from a high level definition of What a Markov Model is (according to Wikipedia): “A Markov model is a stochastic model used to model randomly changing systems where it is assumed that future states depend only on the current state not on the events that occurred before it (that is, it assumes the Markov property). Wow, ok so many keys 🔑 were brought up and dictionaries too if you are curious about the code you should certainly check it out below👇 But otherwise, just recognize that in order to create a more advanced model we need to track what keys proceed other keys and the amount of occurrences of these keys. It has been quite a journey to go from what is a Markov Model to now be talking about how to implement a Markov Model 🌄 From%20%E2%80%9CWhat%20is%20a%20Markov%20Model%E2%80%9D%20to%20%E2%80%9CHere%20is%20how%20Mark%20529b583894574a42bfbc7a4658520ee2/13122645 In my implementation I have a dictionary that stores windows as the key in the key-value pair and then the value for each key is a dictogram. Nth Order Markov Model Structure |**Some of you are definitely curious about how to implement higher order Markov Models so I also included how I went about doing that 😏 ☝️☝️☝️☝️☝️ Very similar to the first order Markov Model, but in this case we store a tuple as the key in the key-value pair in the dictionary. Otherwise, you start the generated data with a starting state (which I generate from valid starts), then you just keep looking at the possible keys (by going into the dictogram for that key) that could follow the current state and make a decision based on probability and randomness (weighted probability).\n","date":"16 February 2023","permalink":"/posts/from-what-is-a-markov-model-to-here-is-how-mark/","section":"Posts","summary":"From-What-is-a-Markov-Model-to-Here-is-how-Mark # From “What is a Markov Model” to “Here is how Markov Models Work” - By # Created: February 21, 2020 12:22 PM URL: https://hackernoon.","title":"From-What-is-a-Markov-Model-to-Here-is-how-Mark"},{"content":"Front-Controller-Microsoft-Docs # You have reviewed the Page Controller pattern, but your page controller classes have complicated logic, are part of a deep inheritance hierarchy, or your application determines the navigation between pages dynamically based on configurable rules. The following forces might persuade you to use Front Controller as opposed to Page Controller: A common implementation of Page Controller involves creating a base class for behavior shared among individual pages. Because Page Controller is implemented with a single object per logical page, it is difficult to consistently apply a particular action across all the pages in a Web application. When implemented with Page Controller, the optional pages would have to be implemented with conditional logic in the base class to select the next page.\nSolution # Front Controller solves the decentralization problem present in Page Controller by channeling all requests through a single controller. As described in the Page Controller pattern, testing the controller may be hindered by the fact that the controller contains code that makes it dependent on the HTTP run-time environment. Page Controller has a single controller object per page as opposed to the single object for all requests.\n","date":"16 February 2023","permalink":"/posts/front-controller-microsoft-docs/","section":"Posts","summary":"Front-Controller-Microsoft-Docs # You have reviewed the Page Controller pattern, but your page controller classes have complicated logic, are part of a deep inheritance hierarchy, or your application determines the navigation between pages dynamically based on configurable rules.","title":"Front-Controller-Microsoft-Docs"},{"content":"Genius-engineer-Claude-Shannon-used-this-problem-s # Genius engineer Claude Shannon used this problem-solving process - Business Insider # Created: February 21, 2020 12:10 PM URL: https://www.businessinsider.com/engineer-claude-shannon-problem-solving-process-2017-7\n","date":"16 February 2023","permalink":"/posts/genius-engineer-claude-shannon-used-this-problem-s/","section":"Posts","summary":"Genius-engineer-Claude-Shannon-used-this-problem-s # Genius engineer Claude Shannon used this problem-solving process - Business Insider # Created: February 21, 2020 12:10 PM URL: https://www.","title":"Genius-engineer-Claude-Shannon-used-this-problem-s"},{"content":"Getting-Ramped-Up-on-Airflow-with-MySQL-S3-Red # Getting Ramped-Up on Airflow with MySQL → S3 → Redshift - By Austin Gibbons # Created: January 28, 2020 8:06 PM Tags: Data URL: https://hackernoon.com/getting-ramped-up-on-airflow-with-mysql-s3-redshift-defcc4522c8c I recently joined Plaid as a data engineer and was getting ramped up on Airflow, a workflow tool that we used to manage ETL pipelines internally. Around the time that I was joining, Plaid was migrating onto Periscope Data for visualizing SQL queries, and my immediate mission became to get more of the data people relied on for analytics insights into our nascent Redshift cluster, the data warehouse we query from Periscope. Getting%20Ramped-Up%20on%20Airflow%20with%20MySQL%20%E2%86%92%20S3%20%E2%86%92%20Red%20f60cc11edad34b0a89feae207ee0d59a/0Dt4pTxhZXWN8CtwF Plaid ETL pipeline circa early 2018\nMotivation # Kindly, my coworker left a more straightforward task to me to help me get ramped up with Airflow — moving data regularly from MySQL to Redshift. ](https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/dp-template-redshift.html) We can pull the schema out of MySQL using a straightforward query:\nselectfromwhere We handled a few types manually, for example instead of moving binary data over we would detect a binary type and instead return a boolean as for whether the column was null or non-null, to avoid having to copy a large amount of binary data over the wire that would be unusable for analytics. Storing the state in the data warehouse manager instead lets us more easily modify the system— automatically adding and removing tables and columns as they get added or dropped from the upstream database and adding custom functionality like setting Redshift sort and distribution keys, and deploying better methodology for our database ingestion. If you’re looking for a cloud provider, we’ve had success using Stitch Data for data processing pipelines, this post from Amazon recommends additional vendors, and Panoply recommends a similar method using mysqldump. Plaid works with many different data sources, and for non-sensitive datasets + 3rd-party data Stitch and Segment have been instrumental in building up data workflows.\n","date":"16 February 2023","permalink":"/posts/getting-ramped-up-on-airflow-with-mysql-s3-red/","section":"Posts","summary":"Getting-Ramped-Up-on-Airflow-with-MySQL-S3-Red # Getting Ramped-Up on Airflow with MySQL → S3 → Redshift - By Austin Gibbons # Created: January 28, 2020 8:06 PM Tags: Data URL: https://hackernoon.","title":"Getting-Ramped-Up-on-Airflow-with-MySQL-S3-Red"},{"content":"Ghost-Theme-Development-Using-Visual-Studio-Code-R # Ghost Theme Development Using Visual Studio Code Remote Development With Containers # Created: April 19, 2020 12:54 PM URL: https://geeklearning.io/visual-studio-code-remote-development-ghost-theme-with-containers/ Earlier, I\u0026rsquo;ve introduced Visual Studio Remote Development and you might be wondering what a practical use case would be. Here is a quick description of the steps it was running:\nBuild the theme Copy it to a .staging directory Start a docker container with the database mounted to a .data and .staging mounted in the theme directory Watch for file changes, and repeat the step above. We will customize it as follow: Install Yarn so we can restore the theme dev dependencies Install gscan so we can validate our theme easily Configure bash as the default shell (who uses sh these days? .devontainer/Dockerfile: FROM ghost # Configure apt ENV DEBIAN_FRONTEND=noninteractive RUN apt-get update \\ \u0026amp;\u0026amp; apt-get -y install --no-install-recommends apt-utils 2\u0026gt;\u0026amp;1 # Verify git and process tools are installed RUN apt-get install -y git procps # Remove outdated yarn from /opt and install via package # so it can be easily updated via apt-get upgrade yarn RUN rm -rf /opt/yarn-* \\ \u0026amp;\u0026amp; rm -f /usr/local/bin/yarn \\ \u0026amp;\u0026amp; rm -f /usr/local/bin/yarnpkg \\ \u0026amp;\u0026amp; apt-get install -y curl apt-transport-https lsb-release \\ \u0026amp;\u0026amp; curl -sS https://dl.yarnpkg.com/$(lsb_release -is | tr \u0026#39;[:upper:]\u0026#39; \u0026#39;[:lower:]\u0026#39;)/pubkey.gpg | apt-key add - 2\u0026gt;/dev/null \\ \u0026amp;\u0026amp; echo \u0026#34;deb https://dl.yarnpkg.com/$(lsb_release -is | tr \u0026#39;[:upper:]\u0026#39; \u0026#39;[:lower:]\u0026#39;)/ stable main\u0026#34; | tee /etc/apt/sources.list.d/yarn.list \\ \u0026amp;\u0026amp; apt-get update \\ \u0026amp;\u0026amp; apt-get -y install --no-install-recommends yarn RUN yarn global add gscan # Clean up RUN apt-get autoremove -y \\ \u0026amp;\u0026amp; apt-get clean -y \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* ENV DEBIAN_FRONTEND=dialog ENV SHELL /bin/bash ENV url http://localhost:3001 Visual Studio Code configuration # Visual Studio Code needs a devcontainer.json file to be able to understand what to do with you dockerfile. .devontainer/devcontainer.json:\n{ \u0026#34;dockerFile\u0026#34;: \u0026#34;Dockerfile\u0026#34;, \u0026#34;extensions\u0026#34;: [], \u0026#34;appPort\u0026#34;: [ \u0026#34;3001:2368\u0026#34; ], \u0026#34;postCreateCommand\u0026#34;: \u0026#34;ln -s /workspaces/geek-learning-io-casper/.data /var/lib/ghost/content/data \u0026amp;\u0026amp; yarn install --production=false\u0026#34; } Tweaking the gulp file # This step has nothing to do with Remote development and is only related to the ghost use case.\nyarn add ps-list @tryghost/admin-api --dev --production=false In gulpfile.js we will add the following imports:\nconst psList = require(\u0026#39;ps-list\u0026#39;); const adminApiClient = require(\u0026#34;@tryghost/admin-api\u0026#34;); const fs = require(\u0026#39;fs\u0026#39;); const path = require(\u0026#39;path\u0026#39;); Then we will replace the end section of the file with the following:\nasync function ghost() { var exec = require(\u0026#34;child_process\u0026#34;).exec; const processes = await psList(); const ghostProcess = processes.filter(x =\u0026gt; x.cmd == \u0026#34;/usr/local/bin/node current/index.js\u0026#34;); if (ghostProcess.length) { process.kill(ghostProcess[0].pid) } exec( \u0026#34;node current/index.js\u0026#34;, { cwd: process.env.GHOST_INSTALL, env: process.env, detached: true }, function callback(error, stdout, stderr) { } ); }; async function deployThemeViaApi(done) { var targetDir = \u0026#39;dist/\u0026#39;; var themeName = require(\u0026#39;./package.json\u0026#39;).name; var filename = themeName + \u0026#39;.zip\u0026#39;; const themePath = path.join(__dirname, targetDir, filename); const url = \u0026#34;http://localhost:2368\u0026#34;; const client = new adminApiClient({ url, key: fs.readFileSync(path.join(__dirname, \u0026#39;.token\u0026#39;), { encoding: \u0026#39;utf8\u0026#39; }), version: \u0026#34;v2\u0026#34; }); await client.themes.upload({ file: themePath }); }; const cssWatcher = () =\u0026gt; watch(\u0026#39;assets/css/**\u0026#39;, css); const hbsWatcher = () =\u0026gt; watch([\u0026#39;*.hbs\u0026#39;, \u0026#39;partials/**/*.hbs\u0026#39;, \u0026#39;!node_modules/**/*.hbs\u0026#39;], hbs); const watcher = parallel(cssWatcher, hbsWatcher); const build = series(css, js); const dev = series(build, serve, watcher); const zipBuild = series(build, zipper); const dockerCssWatcher = () =\u0026gt; watch(\u0026#39;assets/css/**\u0026#39;, series(zipBuild, deployThemeViaApi)); const dockerHbsWatcher = () =\u0026gt; watch([\u0026#39;*.hbs\u0026#39;, \u0026#39;partials/**/*.hbs\u0026#39;, \u0026#39;!node_modules/**/*.hbs\u0026#39;], series(zipBuild, deployThemeViaApi)); const dockerWatcher = parallel(dockerCssWatcher, dockerHbsWatcher); const dockerDev = series(ghost, zipBuild, deployThemeViaApi, dockerWatcher); exports.ghost = ghost; exports.build = build; exports.zip = zipBuild; exports.dev = dev; exports.default = dockerDev; This will change the default gulp task to a task designed to build, zip and deploy the theme to the blog on changes. Ghost%20Theme%20Development%20Using%20Visual%20Studio%20Code%20R%20cfb55d0e495b4904961745c147db7412/aziz-acharki-gXndgCS-CGo-unsplash.jpg Now we can upgrade the readme with the steps to get started, you\u0026rsquo;ll notice that the longest part has to do we initializing ghost :) :\nTo develop on the theme we recommend that you use Visual Studio Code with the Remote development tools * Install [VS Code](https://code.visualstudio.com/) * Install [Remote Developement Extension Pack](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.vscode-remote-extensionpack) * Clone this Repository (if not done already) * Open it * Hit `Ctrl/Command + Shift + P`, type Reopen * `Reopen Folder in container` should appear, Select and Press enter * Wait for the container to be build and start If it\u0026#39;s your first time, start ghost using yarn gulp ghost * Open a browser on `https://localhost:3001` configure your blog and your account. ","date":"16 February 2023","permalink":"/posts/ghost-theme-development-using-visual-studio-code-r/","section":"Posts","summary":"Ghost-Theme-Development-Using-Visual-Studio-Code-R # Ghost Theme Development Using Visual Studio Code Remote Development With Containers # Created: April 19, 2020 12:54 PM URL: https://geeklearning.","title":"Ghost-Theme-Development-Using-Visual-Studio-Code-R"},{"content":"Gitflow-Workflow-Atlassian-Git-Tutorial # A simple way to do this is for one developer to create an empty develop branch locally and push it to the server:\ngit branch develop git push -u origin develop This branch will contain the complete history of the project, whereas master will contain an abridged version. Other developers should now clone the central repository and create a tracking branch for develop. When using the git-flow extension library, executing git flow init on an existing repo will create the develop branch:\n$ git flow init Initialized empty Git repository in ~/project/.git/ No branches exist yet. Branch name for production releases: [master] Branch name for \u0026#34;next release\u0026#34; development: [develop] How to name your supporting branch prefixes? Without the git-flow extensions: git checkout develop git checkout -b release/0.1.0\nWhen using the git-flow extensions: $ git flow release start 0.1.0 Switched to a new branch \u0026lsquo;release/0.1.0\u0026rsquo;\nOnce the release is ready to ship, it will get merged it into `master` and `develop`, then the `release` branch will be deleted. To finish a `release` branch, use the following methods: Without the git-flow extensions: git checkout master git merge release/0.1.0\nOr with the git-flow extension: git flow release finish \u0026lsquo;0.1.0\u0026rsquo;\n## Hotfix Branches ! A `hotfix` branch can be created using the following methods: Without the git-flow extensions: git checkout master git checkout -b hotfix_branch\nWhen using the git-flow extensions: $ git flow hotfix start hotfix_branch\nSimilar to finishing a `release` branch, a `hotfix` branch gets merged into both `master` and `develop.` git checkout master git merge hotfix_branch git checkout develop git merge hotfix_branch git branch -D hotfix_branch\n$ git flow hotfix finish hotfix_branch\n## Example A complete example demonstrating a Feature Branch Flow is as follows. git checkout mastergit checkout -b developgit checkout -b feature_branch# work happens on feature branchgit checkout developgit merge feature_branchgit checkout mastergit merge developgit branch -d feature_branch\nIn addition to the `feature` and `release` flow, a `hotfix` example is as follows: git checkout master git checkout -b hotfix_branch\nwork is done commits are added to the hotfix_branch # git checkout develop git merge hotfix_branch git checkout master git merge hotfix_branch\n## Summary Here we discussed the Gitflow Workflow. ","date":"16 February 2023","permalink":"/posts/gitflow-workflow-atlassian-git-tutorial/","section":"Posts","summary":"Gitflow-Workflow-Atlassian-Git-Tutorial # A simple way to do this is for one developer to create an empty develop branch locally and push it to the server:","title":"Gitflow-Workflow-Atlassian-Git-Tutorial"},{"content":"GitHub-How-to-make-a-fork-of-public-repository-pri # GitHub: How to make a fork of public repository private? # Created: January 30, 2020 8:55 PM URL: https://medium.com/@bilalbayasut/github-how-to-make-a-fork-of-public-repository-private-6ee8cacaf9d3 The answers are correct but don’t mention how to sync code between the public repo and the fork. Here is the full workflow First, duplicate the repo as others said (details here):\n","date":"16 February 2023","permalink":"/posts/github-how-to-make-a-fork-of-public-repository-pri/","section":"Posts","summary":"GitHub-How-to-make-a-fork-of-public-repository-pri # GitHub: How to make a fork of public repository private?","title":"GitHub-How-to-make-a-fork-of-public-repository-pri"},{"content":"GitHub-jarun-buku-Browser-independent-bookmark-m # GitHub - jarun/buku: Browser-independent bookmark manager # Created: March 29, 2020 1:00 PM URL: https://github.com/jarun/buku\nbuku # GitHub%20-%20jarun%20buku%20Browser-independent%20bookmark%20m%20a141975bbaca41e38da331e040915b9a/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f72656c656173652f6a6172756e2f62756b752e7376673f6d61784167653d363030 GitHub%20-%20jarun%20buku%20Browser-independent%20bookmark%20m%20a141975bbaca41e38da331e040915b9a/68747470733a2f2f7265706f6c6f67792e6f72672f62616467652f74696e792d7265706f732f62756b752e737667 GitHub%20-%20jarun%20buku%20Browser-independent%20bookmark%20m%20a141975bbaca41e38da331e040915b9a/68747470733a2f2f696d672e736869656c64732e696f2f707970692f762f62756b752e7376673f6d61784167653d363030 GitHub%20-%20jarun%20buku%20Browser-independent%20bookmark%20m%20a141975bbaca41e38da331e040915b9a/68747470733a2f2f696d672e736869656c64732e696f2f636972636c6563692f70726f6a6563742f6769746875622f6a6172756e2f62756b752e737667 GitHub%20-%20jarun%20buku%20Browser-independent%20bookmark%20m%20a141975bbaca41e38da331e040915b9a/68747470733a2f2f72656164746865646f63732e6f72672f70726f6a656374732f62756b752f62616467652f3f76657273696f6e3d6c6174657374 GitHub%20-%20jarun%20buku%20Browser-independent%20bookmark%20m%20a141975bbaca41e38da331e040915b9a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f707269766163792de29c932d6372696d736f6e GitHub%20-%20jarun%20buku%20Browser-independent%20bookmark%20m%20a141975bbaca41e38da331e040915b9a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c6963656e73652d47504c76332d79656c6c6f772e7376673f6d61784167653d32353932303030 GitHub%20-%20jarun%20buku%20Browser-independent%20bookmark%20m%20a141975bbaca41e38da331e040915b9a/68747470733a2f2f61736369696e656d612e6f72672f612f3133373036352e706e67 buku in action! * GitHub%20-%20jarun%20buku%20Browser-independent%20bookmark%20m%20a141975bbaca41e38da331e040915b9a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f50617950616c2d646f6e6174652d3165623066632e737667\nTable of Contents # Features Installation Dependencies From a package manager Release packages From source Running standalone Shell completion Usage Command-line options Colors Quickstart Examples Automation Troubleshooting Editor integration Collaborators Contributions Related projects In the Press Features # Store bookmarks with auto-fetched title, tags and description Auto-import from Firefox, Google Chrome and Chromium Open bookmarks and search results in browser Shorten, expand URLs, browse cached page from Wayback Machine Text editor integration Lightweight, clean interface, custom colors Powerful search options (regex, substring\u0026hellip;) Continuous search with on the fly mode switch Portable, merge-able database to sync between systems Import/export bookmarks from/to HTML, Markdown or Orgfile Smart tag management using redirection (\u0026raquo;, \u0026gt;, \u0026lt; export Orgfile, if file ends with \u0026lsquo;.org\u0026rsquo; format: *[[url][title]] :tags: export buku DB, if file ends with \u0026lsquo;.db\u0026rsquo; combines with search results, if opted -i, \u0026ndash;import file import bookmarks based on file extension supports \u0026lsquo;html\u0026rsquo;, \u0026lsquo;json\u0026rsquo;, \u0026lsquo;md\u0026rsquo;, \u0026lsquo;org\u0026rsquo;, \u0026lsquo;db\u0026rsquo; -p, \u0026ndash;print [\u0026hellip;] show record details by indices, ranges print all bookmarks, if no arguments -n shows the last n results (like tail) -f, \u0026ndash;format N limit fields in -p or JSON search output N=1: URL; N=2: URL, tag; N=3: title; N=4: URL, title, tag; N=5: title, tag; N0 (10, 20, 30, 40, 50) omits DB index -j, \u0026ndash;json JSON formatted output for -p and search \u0026ndash;colors COLORS set output colors in five-letter string \u0026ndash;nc disable color output -n, \u0026ndash;count N show N results per page (default 10) \u0026ndash;np do not show the prompt, run and exit -o, \u0026ndash;open [\u0026hellip;] browse bookmarks by indices and ranges open a random bookmark, if no arguments \u0026ndash;oa browse all search results immediately \u0026ndash;replace old new replace old tag with new tag everywhere delete old tag, if new tag not specified \u0026ndash;shorten index|URL fetch shortened url from tny.im service \u0026ndash;expand index|URL expand a tny.im shortened url \u0026ndash;cached index|URL browse a cached page from Wayback Machine \u0026ndash;suggest show similar tags when adding bookmarks \u0026ndash;tacit reduce verbosity \u0026ndash;threads N max network connections in full refresh default N=4, min N=1, max N=10 -V check latest upstream version available -z, \u0026ndash;debug show debug information and verbose logs SYMBOLS: url\ncomment tags # PROMPT KEYS: 1-N browse search result indices and/or ranges O [id|range [\u0026hellip;]] open search results/indices in GUI browser toggle try GUI browser if no arguments a open all results in browser s keyword [\u0026hellip;] search for records with ANY keyword S keyword [\u0026hellip;] search for records with ALL keywords d match substrings (\u0026lsquo;pen\u0026rsquo; matches \u0026lsquo;opened\u0026rsquo;) r expression run a regex search t [tag, \u0026hellip;] search by tags; show taglist, if no args g taglist id|range [\u0026hellip;] [\u0026raquo;|\u0026gt;|\u0026lt; https://ddg.gg\nAlternative search engine with perks privacy,search engine # where, \u0026gt;: url, +: comment, #: tags 3. **Export** bookmarks tagged `tag 1` or `tag 2` to HTML, Markdown, Orgfile or a new database: $ buku -e bookmarks.html \u0026ndash;stag tag 1, tag 2 $ buku -e bookmarks.md \u0026ndash;stag tag 1, tag 2 $ buku -e bookmarks.org \u0026ndash;stag tag 1, tag 2 $ buku -e bookmarks.db \u0026ndash;stag tag 1, tag 2\nAll bookmarks are exported if search is not opted. **Search** bookmarks for **ANY** of the keywords `kernel` and `debugging` in URL, title or tags: $ buku kernel debugging $ buku -s kernel debugging\n17. **Replace tag** \u0026#39;old tag\u0026#39; with \u0026#39;new tag\u0026#39;: $ buku \u0026ndash;replace \u0026lsquo;old tag\u0026rsquo; \u0026rsquo;new tag\u0026rsquo;\n30. **Append (or delete) tags** \u0026#39;tag 1\u0026#39;, \u0026#39;tag 2\u0026#39; to (or from) existing tags of bookmark at index 15012014: $ buku -u 15012014 \u0026ndash;tag + tag 1, tag 2 $ buku -u 15012014 \u0026ndash;tag - tag 1, tag 2\n32. ### Related projects - [bukubrow](https://github.com/SamHH/bukubrow), WebExtension for browser integration - [oil](https://github.com/AndreiUlmeyda/oil), search-as-you-type cli front-end - [buku_run](https://github.com/carnager/buku_run), rofi front-end - [pinku](https://github.com/mosegontar/pinku), a Pinboard-to-buku import utility - [buku-dmenu](https://gitlab.com/benoliver999/buku-dmenu), a simple bash dmenu wrapper - [poku](https://github.com/shanedabes/poku), sync between Pocket and buku - [Ebuku](https://github.com/flexibeast/ebuku), Emacs interface to buku [GitHub%20-%20jarun%20buku%20Browser-independent%20bookmark%20m%20a141975bbaca41e38da331e040915b9a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d627269676874677265656e2e7376673f6d61784167653d32353932303030](GitHub%20-%20jarun%20buku%20Browser-independent%20bookmark%20m%20a141975bbaca41e38da331e040915b9a/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d737461626c652d627269676874677265656e2e7376673f6d61784167653d32353932303030) ### In the Press - [2daygeek](http://www.2daygeek.com/buku-command-line-bookmark-manager-linux/) - [Hacker Milk](https://hackermilk.blogspot.com/2020/01/how-to-manage-your-browsers-bookmarks.html) - [It\u0026#39;s F.O.S.S. ","date":"16 February 2023","permalink":"/posts/github-jarun-buku-browser-independent-bookmark-m/","section":"Posts","summary":"GitHub-jarun-buku-Browser-independent-bookmark-m # GitHub - jarun/buku: Browser-independent bookmark manager # Created: March 29, 2020 1:00 PM URL: https://github.","title":"GitHub-jarun-buku-Browser-independent-bookmark-m"},{"content":"Glitch-Create # Glitch - Create # Created: May 14, 2021 3:12 AM URL: https://glitch.com/create Whether you’re new to code or an experienced developer, simply pick a starter app to remix. Glitch%20-%20Create%2029546e59947b4cd4a136c4ffa63abce4/50f784d9-9995-4fa4-a185-b4b1ea6e77c02Fillustration.svg Glitch is a collaborative programming environment that lives in your browser and deploys code as you type. Use Glitch to build anything from a good ol’ static webpage to full-stack Node apps. Glitch%20-%20Create%2029546e59947b4cd4a136c4ffa63abce4/605e2a51-d45f-4d87-a285-9410ad3505152Fhello-website-1280.png You never have to start from scratch: Just start remixing an existing starter app, or clone a project from services like GitHub and GitLab to experiment and deploy on Glitch. Your favorite companies use Glitch to share apps that get you up and running with their APIs. Anyone with a browser can jump in and pick up where you left off, and private .env files keep secrets like API keys, well, secret. 605e2a51-d45f-4d87-a285-9410ad350515%2FMultiplayer%20Demo%20Revised.mp4\nYour app is live, instantly # There\u0026rsquo;s no deployment setup—as soon as you create a new project, your Glitch app is live with its own URL (or your custom domain!).\n","date":"16 February 2023","permalink":"/posts/glitch-create/","section":"Posts","summary":"Glitch-Create # Glitch - Create # Created: May 14, 2021 3:12 AM URL: https://glitch.","title":"Glitch-Create"},{"content":"Glossary-of-tags-I-m-using-and-why-they-matter-to # Glossary of tags I\u0026rsquo;m using and why they matter to me | Mildly entertainingᵝ # Created: January 28, 2020 12:33 AM Tags: Productivity, Self URL: https://beepb00p.xyz/tags.html\nextendedmind immortality ¶quantified-self # \u0026ldquo;Quantified self\u0026rdquo; refers to self tracking and coming up with metrics in order to gain more insight and potentially improve various aspects of your life such as health (physical or mental) or performance. I can\u0026rsquo;t imagine going on a run without my HR monitor, because whatever the benefit exercise has and however exhausting run would be, at least I\u0026rsquo;ll have a data point.\nlearning about routine and trying to optimize routine is also way more fun than doing the actual routine Human body is fragile and needs constant care, but it\u0026rsquo;s still a fascinating mechanism. If I can\u0026rsquo;t feel changes subjectively and I can\u0026rsquo;t find them using linear regression, then surely it just doesn\u0026rsquo;t matter to my body. I do know people who claim minor things changed the way they e.g. sleep, so I don\u0026rsquo;t want to devalue others\u0026rsquo; experiences. finding my baseline while I\u0026rsquo;m healthy for potential future health issues Some links: ¶lifelogging # Lifelogging is not a new concept as people have kept diaries for centuries. ](https://matiroy.com/writings/Should-I-record-my-life.html) is a good summary highlighting pros and cons\nLifelogging, An Inevitability: describes early attempts at digital lifelogging and raises some ethical/legal questions ","date":"16 February 2023","permalink":"/posts/glossary-of-tags-i-m-using-and-why-they-matter-to/","section":"Posts","summary":"Glossary-of-tags-I-m-using-and-why-they-matter-to # Glossary of tags I\u0026rsquo;m using and why they matter to me | Mildly entertainingᵝ # Created: January 28, 2020 12:33 AM Tags: Productivity, Self URL: https://beepb00p.","title":"Glossary-of-tags-I-m-using-and-why-they-matter-to"},{"content":"Google-Site-Reliability-Engineering # Data pipelines go as far back as co-routines [Con63], the DTSS communication files [Bul80], the UNIX pipe [McI86], and later, ETL pipelines,116 but such pipelines have gained increased attention with the rise of \u0026ldquo;Big Data,\u0026rdquo; or \u0026ldquo;datasets that are so large and so complex that traditional data processing applications are inadequate. \u0026ldquo;117\nInitial Effect of Big Data on the Simple Pipeline Pattern # Programs that perform periodic or continuous transformations on Big Data are usually referred to as \u0026ldquo;simple, one-phase pipelines.\u0026rdquo;\nDrawbacks of Periodic Pipelines in Distributed Environments # Big Data periodic pipelines are widely used at Google, and so Google’s cluster management solution includes an alternative scheduling mechanism for such pipelines. Moiré load pattern in shared infrastructure When an inherently one-shot batch pipeline is overwhelmed by business demands for continuously updated results, the pipeline development team usually considers either refactoring the original design to satisfy current demands, or moving to a continuous pipeline model. Although all pipeline data may be stored in the Task Master, the best performance is usually achieved when only pointers to work are stored in the Task Master, and the actual input and output data is stored in a common filesystem or other storage. The model-view-controller design pattern as adapted for Google Workflow We can increase pipeline depth to any level inside Workflow by subdividing processing into task groups held in the Task Master. Because all pipeline configuration in Workflow is stored inside the Task Master in the same form as the work units themselves, in order to commit work, a worker must own an active lease and reference the task ID number of the configuration it used to produce its result.\n","date":"16 February 2023","permalink":"/posts/google-site-reliability-engineering/","section":"Posts","summary":"Google-Site-Reliability-Engineering # Data pipelines go as far back as co-routines [Con63], the DTSS communication files [Bul80], the UNIX pipe [McI86], and later, ETL pipelines,116 but such pipelines have gained increased attention with the rise of \u0026ldquo;Big Data,\u0026rdquo; or \u0026ldquo;datasets that are so large and so complex that traditional data processing applications are inadequate.","title":"Google-Site-Reliability-Engineering"},{"content":"Hacking-up-your-own-shell-completion-Computing-wit # I’ve often used fzf as a file finder, you can simply run something like git ls-files | fzf, and pass the output to your editor, and you’ve already made a git aware fuzzy file finder, running this inside your editor like fzf-vim makes this even more powerful. One quick solution to our doer problem is to write a bash script like this:\n#!/bin/bash doer $(doer -list | fzf) Here we just use command substitution to run doer’s list command, pass it to fzf, then run doer with the result. Okay fine, I guess I can add proper shell completion to the command using fish’s built in complete command, add it to the repo, and set up an install script. What if there was a way to strap an fzf completion thing into my shell, that I fully control through just a couple lines of fish script, that doesn’t interfere with built-in completion, and allows me to quickly add bindings to any command I frequently run?\nfunction fzf-smart-completion -d \u0026#34;List files and folders\u0026#34; set -l commandline (__fzf_parse_commandline) set -l dir $commandline[1] set -l fzf_query $commandline[2] # use our cool new completion checker set -l FZF_CMD (get-completion-command); or return # fzf edge case and formatting (prevents fzf from taking up the whole screen) set FZF_HEIGHT 40% begin set -lx FZF_DEFAULT_OPTS \u0026#34;--height $FZF_HEIGHT --reverse $FZF_DEFAULT_OPTS $FZF_CMD[2]\u0026#34; eval \u0026#34;$FZF_CMD[1] | \u0026#34;(__fzfcmd)\u0026#39; -m --query \u0026#34;\u0026#39;$fzf_query\u0026#39;\u0026#34;\u0026#39; | while read -l r; set result $result $r; end end if [ -z \u0026#34;$result\u0026#34; ] commandline -f repaint return else # Remove last token from commandline. commandline -t \u0026#34;\u0026#34; end for i in $result commandline -it -- (string escape $i) commandline -it -- \u0026#39; \u0026#39; end commandline -f repaint end bind -M insert \\et fzf-smart-completion bind \\et fzf-smart-completion With this little bit of fish scripting, we now have a pretty cool ALT-T command that runs our own fuzzy autocomplete like so.\nfunction get-completion-command set -l cmd (commandline) switch $cmd case \u0026#39;doer *\u0026#39; echo \u0026#39;doer -list\u0026#39; case \u0026#39;go test *\u0026#39; echo \u0026#39;git ls-files | grep _test.go\u0026#39; case \u0026#39;yarn test *\u0026#39; echo \u0026#39;git ls-files | grep .test.ts\u0026#39; case \u0026#39;*\u0026#39; return 1 end end You can see the go tester in action here: Another useful thing I’ve found is automating some routine git commands.\n","date":"16 February 2023","permalink":"/posts/hacking-up-your-own-shell-completion-computing-wit/","section":"Posts","summary":"Hacking-up-your-own-shell-completion-Computing-wit # I’ve often used fzf as a file finder, you can simply run something like git ls-files | fzf, and pass the output to your editor, and you’ve already made a git aware fuzzy file finder, running this inside your editor like fzf-vim makes this even more powerful.","title":"Hacking-up-your-own-shell-completion-Computing-wit"},{"content":"Hasura-by-far-lets-you-point-and-click-build-your # Hasura by far, lets you point-and-click build your database and table relationsh\u0026hellip; | Hacker News # Created: April 5, 2020 5:02 PM URL: https://news.ycombinator.com/item?id=22787313 Hasura by far, lets you point-and-click build your database and table relationships with a web dashboard and autogenerates a full GraphQL CRUD API with permissions you can configure and JWT/webhook auth baked-in. I\u0026rsquo;ve been able to build in a weekend no-code what would\u0026rsquo;ve taken my team weeks or months to build by hand, even with something as productive as Rails. It automates the boring stuff and you just have to write single endpoints for custom business logic, like \u0026ldquo;send a welcome email on sign-up\u0026rdquo; or \u0026ldquo;process a payment\u0026rdquo;. It has a database viewer, but it\u0026rsquo;s not the core of the product, so I use Forest Admin to autogenerate an Admin Dashboard that non-technical team members can use: With these two, you can point-and-click make 80% of a SaaS product in almost no time. I wrote a tutorial on how to integrate Hasura + Forest Admin, for anyone interested: For interacting with Hasura from a client, you can autogenerate fully-typed \u0026amp; documented query components in your framework of choice using GraphQL Code Generator: Then I usually throw Metabase in there as a self-hosted Business Intelligence platform for non-technical people to use as well, and PostHog for analytics: All of these all Docker Containers, so you can have them running locally or deployed in minutes. This stack is absurdly powerful and productive.\n","date":"16 February 2023","permalink":"/posts/hasura-by-far-lets-you-point-and-click-build-your/","section":"Posts","summary":"Hasura-by-far-lets-you-point-and-click-build-your # Hasura by far, lets you point-and-click build your database and table relationsh\u0026hellip; | Hacker News # Created: April 5, 2020 5:02 PM URL: https://news.","title":"Hasura-by-far-lets-you-point-and-click-build-your"},{"content":"Hidden-Networks-Network-Effects-That-Don-t-Look-Li # Hidden Networks: Network Effects That Don\u0026rsquo;t Look Like Network Effects - Andreessen Horowitz # Created: February 21, 2020 12:06 PM URL: https://a16z.com/2019/07/29/hidden-networks-effects/ Many of the most consequential projects of the internet era — from Wikipedia to Facebook and bitcoin — have all been predicated on network effects, where the network becomes more valuable to users as more people use it.\n1) Slow networks # Slow networks are defined by a time delay between when the network is created and when it starts to show value. Hidden%20Networks%20Network%20Effects%20That%20Don\u0026rsquo;t%20Look%20Li%2031a5f96f77484e4bb0378718e5a422cf/1f642.svg Like a slow network, an unfinished network will have bottled up network effects, but they won’t show up in any analysis or metrics.\n2B) Throttled networks # A throttled network is one where a product feature or strategic decision limits the size or engagement of the network in a material way, thereby masking the strength of the network effects. Like an unfinished network, a throttled network appears to have limited network effects — until all of a sudden, it doesn’t. The test for telling whether it’s a throttled network with strong network effects is fairly straight-forward: understand what would happen if one of more of these constraints (e.g., price, network growth, engagement, etc.)\n3) Latent networks: (aka “Come for the network, stay for the tool” networks) # There are a host of companies that build the tool or product before they build the network.\n","date":"16 February 2023","permalink":"/posts/hidden-networks-network-effects-that-don-t-look-li/","section":"Posts","summary":"Hidden-Networks-Network-Effects-That-Don-t-Look-Li # Hidden Networks: Network Effects That Don\u0026rsquo;t Look Like Network Effects - Andreessen Horowitz # Created: February 21, 2020 12:06 PM URL: https://a16z.","title":"Hidden-Networks-Network-Effects-That-Don-t-Look-Li"},{"content":"High-Resolution-Elevation-Maps # High%20Resolution%20Elevation%20Maps%2025c006ac060846ceb14ffc356e5ba4a1/mount-rainier-map-1613.jpg ! High%20Resolution%20Elevation%20Maps%2025c006ac060846ceb14ffc356e5ba4a1/mount-hood-map-1613.jpg ! High%20Resolution%20Elevation%20Maps%2025c006ac060846ceb14ffc356e5ba4a1/mount-shasta-map-1613.jpg ! High%20Resolution%20Elevation%20Maps%2025c006ac060846ceb14ffc356e5ba4a1/mount-st-helens-map-1613.jpg ! High%20Resolution%20Elevation%20Maps%2025c006ac060846ceb14ffc356e5ba4a1/mount-baker-map-1613.jpg ! High%20Resolution%20Elevation%20Maps%2025c006ac060846ceb14ffc356e5ba4a1/mount-bachelor-map-1613.jpg ! High%20Resolution%20Elevation%20Maps%2025c006ac060846ceb14ffc356e5ba4a1/mount-adams-map-1613.jpg !\n","date":"16 February 2023","permalink":"/posts/high-resolution-elevation-maps/","section":"Posts","summary":"High-Resolution-Elevation-Maps # High%20Resolution%20Elevation%20Maps%2025c006ac060846ceb14ffc356e5ba4a1/mount-rainier-map-1613.","title":"High-Resolution-Elevation-Maps"},{"content":"Home-Linux-Journey # Grasshopper # ! Home%20Linux%20Journey%20b7995621f90e47ce807af99add32e087/getting-started-cd39013b87fac4b4b5d668752511f154a673adc94fad86c9be53f0016bf3bc35.png ! Home%20Linux%20Journey%20b7995621f90e47ce807af99add32e087/command-line-dd6a59e10b5ec94f43e805d35d8556dfd070c25871788dd8b5a09a536a477b0a.png ! Home%20Linux%20Journey%20b7995621f90e47ce807af99add32e087/text-fu-622e3761a4638fdc72b7c21d2e6d41ae71861da119bdafe677a9bafa0627f1ca.png ! Home%20Linux%20Journey%20b7995621f90e47ce807af99add32e087/user-management-11b3136e7e9f551e6efa2929fbef7b3f4693dd8e9d33f8db709c3415b6815dca.png ! Home%20Linux%20Journey%20b7995621f90e47ce807af99add32e087/network-sharing-160c16689bb42ee68a9047bb8e3741a00934d9e70cf5c9feb8caef9b62a0b8e8.png ! Home%20Linux%20Journey%20b7995621f90e47ce807af99add32e087/network-fundamentals-23b42f49c6f57d45884285ffbfdf44127863ae96243eb51ce39fb139f585cab1.png !\n","date":"16 February 2023","permalink":"/posts/home-linux-journey/","section":"Posts","summary":"Home-Linux-Journey # Grasshopper # !","title":"Home-Linux-Journey"},{"content":"Home-MeetingBar # Home | MeetingBar # Created: July 29, 2020 4:20 PM URL: https://meetingbar.onrender.com/\nIntegrated with Google Meet and Zoom so you can quickly join meetings from event or create ad hoc meeting # data:image/svg+xml,%3csvg%20xmlns=\u0026lsquo;http://www.w3.org/2000/svg\u0026rsquo;%20width=\u0026lsquo;400\u0026rsquo;%20height=\u0026lsquo;225\u0026rsquo;%20viewBox=\u0026lsquo;0%200%20400%20225\u0026rsquo;%20preserveAspectRatio=\u0026lsquo;none\u0026rsquo;%3e%3cpath%20d=\u0026lsquo;M0%208v7h13c9%200%2011%200%208%201-5%201-5%201%201%201%202%200%203%201%201%203v2c2%202%203%207%201%2010l-1%203-3%201-2-2h-4c-4-2-8%200-5%202%201%202%202%202%205%202l4%202c2%203%202%203%200%204l-1%201%2014%202c5%200%2012-2%208-3l-6-2-2-1c-3-1-2-3%202-5%201-1%202-1%201-2l1-1h1l-2-1-2-2-2-4c-2-2-3-3-2-4v-3c1-3%201-3-1-3-1-1%205-1%2013-1l14%201%202%201%203-1%208-1c5%200%208%200%206%201l-2%201h2c6%202%206%205-1%204-2%200-3%200-2%201l-1%203-1%203-1%203c-2%202-2%202%201%202%202%200%202%200%201%202v2l-2%204c-4%204-3%205%202%205%204%200%208%203%206%206-1%201%201%206%202%206l1%203-1%202v1l1%2042%201%2042c1%202-5%200-8-2-2-2-3-3-3-6-1-7-4-11-9-11l-8-2c-2%200-4-2-6-4-5-6-7-7-14-10-6-2-6-2-11%200l-6%201-6-4-7-4c-1-1-1%2011-1%2059v60h13l14-1h1l24%201%2023-1v-1l1%201%20162%201h163v-61l-1-50c0%2011%200%2011-3%2011l-2%201c0%201-13%203-20%203l-6%201a262%20262%200%200123%202h8v20l-1%2020a96%2096%200%2000-21%202l-15%203-15%203h-6l-7%202-9%201-8%201-6%201-13%203a219%20219%200%2000-21%202l-19%203a455%20455%200%2001-32%201c-10%200-25-4-44-14a234%20234%200%2000-16-7c0-2-5-4-13-5l-14-2-16-1%2071-1h80v-29h-98a1780%201780%200%20010-2h98l1-3v-15l-1-12h-98a3234%203234%200%20010-1h99v-9c0-7%201-10%202-11%204-5%204-8%201-10-4-3-4-13-1-14l2-3c-1-2%203-5%206-5a160%20160%200%200022-2c1%202%203%201%2013%200%208-2%2010-2%2012-1%205%202%2011%203%2014%202s13-3%2020-3c4-1%205-1%205%201l4%203%203%201c1%202%2016%201%2017%200h3l1-15%201-25V0H274v14h-65l-1-7V0H0v8m133%2022c0%202-4%202-6%201l-3-1c-2%200-2%201-2%204s3%205%205%202h10c1-1%201-1%200%200%200%203%203%202%204-1v-4l-1%202h-1c-1-3-6-5-6-3m-9%2012v7l1-1c0-2%202-2%204-1%201%201%204%202%2010%202%208%200%209-1%209-2%200-2%200-2%201%200%201%203%205%202%205-1%200-4-10-4-10-1-1%202-1%202-1%200s-4-4-5-2l-7%201c-4%200-6%200-5-1l2-1h-4m57%203c0%203%200%204%203%204l3-1h16c1%201%201%201%201-1v-2l1%202c1%202%201%202%202-1%201-4%201-6-1-4h-3l-3%201c-2%200-2%201-3%202l-1%201v-3c-1%201-2%200-2-1h-1l-3%201-2%201c-1%201-1%201-1-1s0-2-2%200h-2c0-3-2-2-2%202m60-3l-5%201c-5%200-6%200-6%202l-1%203v-3c0-3-10-3-11%200%200%203%202%205%203%203h27c1%201%201%200%202-1l1-1%201%202%201-2c0-2-1-2-5-2-3%200-5-1-5-2l-1-1-1%201M93%2054l-1%202v1c-2%200%200%202%202%202l3%201h4c1-2%202-2%204%200h42c2%200%201-3-1-5s-3-2-3-1l-6%201-6%201h-3c0%202-2%201-3-1l-1-2-1%202c-1%203-7%203-9%200-1-2-2-2-3-1h-8c-2%200-1%202%201%202%201%201%201%201-1%201l-2-2c-1-2-4-3-5%200h-1c0-1-1-2-2-1m31%2012c-2%203%201%208%204%206h3l1-2c1-2%201-2%201%200s3%204%204%202h6l2-2h1l1%202%201-2h1c0%202%205%203%205%201l1-2%201%202%201%201c1-1%201%200%201%201%200%203%204%201%205-3v-3h-5l-6-1h-1c0%202-21%202-22%200h-5M93%2078c-1%201%200%206%201%206l1-2c0-2%200-2%201-1l1%202%206%201%206-1h1l2%201c2%200%203-3%202-5%200-2-4-3-4-1h-7c-2%200-1%202%202%202l2%202h-5l-1-2c0-3-2-4-3-1h-2c-1-2-3-3-3-1m52%203c0%204%204%205%204%201%201-2%201-2%201%200s1%202%206%202%206%200%206-2%200-2%201%200c2%202%203%203%204%200h1c0%202%206%203%206%201h1c1%202%205%201%206-2%200-1-1-2-3-2l-8-1c-1-1-2-1-3%201-1%201-2%202-3%200h-8l-2-1h-7l-1-1-1%204m-52%209v3c0%204%205%204%206%200%201-1%202-2%203-1v1l-1%201%202%201h1l2%201%202-1h1c1%202%205%201%205-2%200-4-4-5-5-3h-1l-2-1-2%201H93m30%203l1%203%201-1h1l10%201c8%200%209%200%209-2l1-2v2l1%202%201-2%201-2v2c0%202%201%202%203%202%203%200%203%200%203-3%200-2-1-2-6-2l-6-1h-3l-5%201h-7l-2-2c-3%200-3%200-3%204m27%2017h-6l-6%201h-7c-1%201%200%203%201%204h2c1-2%202-2%203-1h4c1%202%206%201%206-1h1l1%202%202%201c1%200%202-1%202-3-1-3-2-5-3-3m-57%2011l-1%203c0%202%203%205%204%203h3l1-2v-2l1%202c1%202%202%202%209%202%206%200%208%200%209-2%200-2-2-4-4-3l-2-1H93m45%200h-1l-2%201h-13v5l1-2%201-2v2l1%202%201-2%201-2v2c0%202%200%202%205%202l6-1h1c0%202%206%201%206-1h1c0%202%201%202%202%202l1%201c0%203%204%201%205-3%200-3-3-5-5-3h-4l-2-1c1-1%200-1-2-1l-3%201m-46%2022l1%204%201-1%202-2%201%202%201%201%201-2v-2l2%202c2%203%203%203%204%200l1-2v2c0%202%202%203%202%201h1c1%202%206%201%206-1v-2l1%202c1%203%207%203%207%200l1-2v2c0%202%201%202%208%202s8-1%208-3v-3h-15l-16-1h-3c-1%202-8%201-9%200-3-2-5-1-5%203m1%209c-2%203%200%207%203%207l2-1h1c2%202%206%201%206-1s0-2%201%200l2%202-1-7h-4c1%202-2%202-4%200h-6\u0026rsquo;%20fill=\u0026rsquo;%23d3d3d3\u0026rsquo;%20fill-rule=\u0026lsquo;evenodd\u0026rsquo;/%3e%3c/svg%3e\nInstallation # ","date":"16 February 2023","permalink":"/posts/home-meetingbar/","section":"Posts","summary":"Home-MeetingBar # Home | MeetingBar # Created: July 29, 2020 4:20 PM URL: https://meetingbar.","title":"Home-MeetingBar"},{"content":"Hosting-Flask-servers-on-Firebase-from-scratch-F # Any server backend with Cloud Run # With Firebase Hosting integrating with Cloud Run, you can generate content from anything that fits within a stateless Docker container.\nCloud Run has a free tier # To use Cloud Run with Firebase Hosting you currently need billing enabled, which requires putting a credit card on file. Now it’s time to set up Firebase Hosting to serve your static files and the content generated with Cloud Run. From the root of the project run the following command:\n./node_modules/.bin/firebase init hosting This calls the locally installed Firebase CLI to set up Firebase Hosting within your project. To delete then file, run the following command from the root of the project:\nrm static/index.html # Mac/Linux del static/index.html # Windows Add a style.css in the static directory # To add some style to your web app, create astatic/style.css file with the following contents: With the project set up locally, we can hook up Cloud Run to Firebase Hosting. During this 10 minute period Firebase Hosting will skip running your server code in Cloud Run and serve the cache content directly. Run the following commands:\ngcloud builds submit --tag gcr.io//flask-fire gcloud beta run deploy --image gcr.io//flask-fire This build the container in Cloud Build and deploy to Cloud Run.\n","date":"16 February 2023","permalink":"/posts/hosting-flask-servers-on-firebase-from-scratch-f/","section":"Posts","summary":"Hosting-Flask-servers-on-Firebase-from-scratch-F # Any server backend with Cloud Run # With Firebase Hosting integrating with Cloud Run, you can generate content from anything that fits within a stateless Docker container.","title":"Hosting-Flask-servers-on-Firebase-from-scratch-F"},{"content":"How-some-good-corporate-engineering-blogs-are-wr # How (some) good corporate engineering blogs are written # Created: March 29, 2020 8:56 AM URL: https://danluu.com/corp-eng-blogs/ I\u0026rsquo;ve been comparing notes with people who run corporate engineering blogs and one thing that I think is curious is that it\u0026rsquo;s pretty common for my personal blog to get more traffic than the entire corp eng blog for a company with a nine to ten figure valuation and it\u0026rsquo;s not uncommon for my blog to get an order of magnitude more traffic. To try to understand what companies with good corporate engineering blog have in common, I interviewed folks at three different companies that have compelling corporate engineering blogs (Cloudflare, Heap, and Segment) as well as folks at three different companies that have lame corporate engineering blogs (which I\u0026rsquo;m not going to name). At a high-level, the compelling engineering blogs had processes that shared the following properties:\nEasy approval process, not many approvals necessary Few or no non-engineering approvals required Implicit or explicit fast SLO on approvals Approval/editing process mainly makes posts more compelling to engineers Direct, high-level (co-founder, C-level, or VP-level) support for keeping blog process lightweight The less compelling engineering blogs had processes that shared the following properties: Slow approval process Many approvals necessary Significant non-engineering approvals necessary Non-engineering approvals suggest changes authors find frustrating Back-and-forth can go on for months Approval/editing process mainly de-risks posts, removes references to specifics, makes posts vaguer and less interesting to engineers Effectively no high-level support for blogging Leadership may agree that blogging is good in the abstract, but it\u0026rsquo;s not a high enough priority to take concrete action Reforming process to make blogging easier very difficult; previous efforts have failed Changing process to reduce overhead requires all \u0026ldquo;stakeholders\u0026rdquo; to sign off (14 in one case) Any single stakeholder can block No single stakeholder can approve Stakeholders wary of approving anything that reduces overhead Approving involves taking on perceived risk (what if something bad happens) with no perceived benefit to them One person at a company with a compelling blog noted that a downside of having only one approver and/or one primary approver is that if that person is busy, it can takes weeks to get posts approved. Here are the processes, as described to me, for the three companies I interviewed (presented in sha512sum order, which is coincidentally ordered by increasing size of company, from a couple hundred employees to nearly one thousand employees): Heap # Someone has an idea to write a post Writer (who is an engineer) is paired with a \u0026ldquo;buddy\u0026rdquo;, who edits and then approves the post Buddy is an engineer who has a track record of producing reasonable writing This may take a few rounds, may change thrust of the post CTO reads and approves Usually only minor feedback May make suggestions like \u0026ldquo;a designer could make this graph look better\u0026rdquo; Publish post The first editing phase used to involve posting a draft to a slack channel where \u0026ldquo;everyone\u0026rdquo; would comment on the post. Segment # Someone has an idea to write a post Often comes from: internal docs, external talk, shipped project, open source tooling (built by Segment) Writer (who is an engineer) writes a draft May have a senior eng work with them to write the draft Until recently, no one really owned the feedback process Calvin French-Owen (co-founder) and Rick (engineering manager) would usually give most feedback Maybe also get feedback from manager and eng leadership Typically, 3rd draft is considered finished Now, have a full-time editor who owns editing posts Also socialize among eng team, get get feedback from 15-20 people PR and legal will take a look, lightweight approval Some changes that have been made include At one point, when trying to establish an \u0026ldquo;engineering brand\u0026rdquo;, making in-depth technical posts a top-level priority had a \u0026ldquo;blogging retreat\u0026rdquo;, one week spent on writing a post added writing and speaking as explicit criteria to be rewarded in performance reviews and career ladders Although there\u0026rsquo;s legal and PR approval, Calvin noted \u0026ldquo;In general we try to keep it fairly lightweight. Cloudflare # Someone has an idea to write a post Internal blogging is part of the culture, some posts come from the internal blog John Graham-Cumming (CTO) reads every post, other folks will read and comment John is approver for posts Matthew Prince (CEO) also generally supportive of blogging \u0026ldquo;Very quick\u0026rdquo; legal approval process, SLO of 1 hour This process is so lightweight that one person didn\u0026rsquo;t really think of it as an approval, another person didn\u0026rsquo;t mention it at all (a third person did mention this step) Comms generally not involved One thing to note is that this only applies to technical blog posts. Cloudflare # https://blog.cloudflare.com/how-verizon-and-a-bgp-optimizer-knocked-large-parts-of-the-internet-offline-today/ Talks about a real technical problem that impacted a lot of people, reasonably in depth Timely, released only eight hours after the outage, when people were still really interested in hearing about what happened; most companies can\u0026rsquo;t turn around a compelling blog post this quickly or can only do it on a special-case basis, Cloudflare is able to crank out timely posts semi-regularly https://blog.cloudflare.com/the-relative-cost-of-bandwidth-around-the-world/ Exploration of some data https://blog.cloudflare.com/the-story-of-one-latency-spike/ A debugging story https://blog.cloudflare.com/when-bloom-filters-dont-bloom/ A debugging story, this time in the context of developing a data structure Segment # https://segment.com/blog/when-aws-autoscale-doesn-t/ Concrete explanation of a gotcha in a widely used service https://segment.com/blog/gotchas-from-two-years-of-node/ Concrete example and explanation of a gotcha in a widely used tool https://segment.com/blog/automating-our-infrastructure/ Post with specific details about how a company operates; in theory, any company could write this, but few do Heap # https://heap.io/blog/engineering/basic-performance-analysis-saved-us-millions Talks about a real problem and solution https://heap.io/blog/engineering/clocksource-aws-ec2-vdso Talks about a real problem and solution In HN comments, engineers (malisper, kalmar) have technical responses with real reasons in them and not just the usual dissembling that you see in most cases https://heap.io/blog/analysis/migrating-to-typescript Real talk about how the first attempt at driving a company-wide technical change failed One thing to note is that these blogs all have different styles. ","date":"16 February 2023","permalink":"/posts/how-some-good-corporate-engineering-blogs-are-wr/","section":"Posts","summary":"How-some-good-corporate-engineering-blogs-are-wr # How (some) good corporate engineering blogs are written # Created: March 29, 2020 8:56 AM URL: https://danluu.","title":"How-some-good-corporate-engineering-blogs-are-wr"},{"content":"How-to-analyze-marketing-attribution # “Simple”, “out-of-the-box” solutions that are cheap, but only offer insight on a limited slice of data Marketers that have completely given up on the problem (“trust your gut”) and are exhausted by vendor promises Marketers have been told that attribution is a data problem \u0026ndash; “Just get the data and you can have full knowledge of what’s working!” \u0026ndash; when really it’s a data modeling problem. This playbook assumes you’re operating within a modern data stack , so you already have the infrastructure that you need in place: You’re collecting events data with a tool like Snowplow or Segment (though Segment might get a little pricey) You’re extracting data from ad platforms using Stitch or Fivetran You’re loading data into a modern, cloud data warehouse like Snowflake, BigQuery, or Redshift And you’re using dbt so your analysts can model data in SQL There are four main ways to divvy up this point:\nFirst touch: Attribute the entire conversion to the first touch Last touch: Attribute the entire conversion to the last touch Forty-twenty-forty: Attribute 40% (0.4 points) of the attribution to the first touch, 40% to the last touch, and divide the remaining 20% between all touches in between Linear: Divide the point equally among all touches There’s no hard and fast answer to which attribution method you should use — chat with your marketing team about what’s most appropriate for your company. Join your two data sources together to do this: select * from sessions left join conversion using (customer_id) where sessions.started_at \u0026lt;= customer_conversions.converted_at and sessions.started_at \u0026gt;= dateadd(days, -30, customer_conversions.converted_at) We often limit the sessions that count towards attribution to just the sessions within the 30 days that preceded conversion (often referred to as an “attribution window”). So, rather than answering “how many Facebook sessions led to conversions this week?”, we’re choosing to answer “how many conversions this week were a result of Facebook sessions?”.Now that your marketing team knows which channels are leading to the most conversions, they might then ask: “which channel is leading to the highest value conversions?”\n4. # That way, we can join across the two data sets to calculate:\nCost per acquisition: the number of advertising dollars spent to acquire a customer Return on ad spend: attributed revenue / ad spend with ad_spend as ( select * from {{ ref(\u0026#39;ad_spend\u0026#39;) }} ), attribution as ( select * from {{ ref(\u0026#39;attribution_touches\u0026#39;) }} ), -- aggregate first as this is easier to debug / often leads to fewer fanouts ad_spend_aggregated as ( select date_trunc(\u0026#39;month\u0026#39;, date_day) as date_month, utm_source, sum(spend) as total_spend from ad_spend group by 1, 2 ), attribution_aggregated as ( select date_trunc(\u0026#39;month\u0026#39;, converted_at) as date_month, utm_source, sum(linear_points) as attribution_points, sum(linear_revenue) as attribution_revenue from attribution group by 1, 2 ), joined as ( select *, 1.0 * nullif(total_spend, 0) / attribution_points as cost_per_acquisition, 1.0 * attribution_revenue / nullif(total_spend, 0) as return_on_advertising_spend from attribution_aggregated full outer join ad_spend_aggregated using (date_month, utm_source) ) select * from joined order by date_month, utm_source This gives us a view of the data like so: Untitled There’s a few things worth noting in this query:\nWe aggregated in a CTE and then joined the two CTEs together — I get nervous whenever a join and an aggregation are happening in the same query (too much risk of fanout! We have one here to ensure that Conversions that don’t have associated ad spend appear in our result set Ad spend that doesn’t result in conversions still appears in our result set Often, the utm attributes on your ad spend data won’t perfectly match the utm parameters on your sessions, so you’ll need to do some upstream data cleaning to get this join to work. ","date":"16 February 2023","permalink":"/posts/how-to-analyze-marketing-attribution/","section":"Posts","summary":"How-to-analyze-marketing-attribution # “Simple”, “out-of-the-box” solutions that are cheap, but only offer insight on a limited slice of data Marketers that have completely given up on the problem (“trust your gut”) and are exhausted by vendor promises Marketers have been told that attribution is a data problem \u0026ndash; “Just get the data and you can have full knowledge of what’s working!","title":"How-to-analyze-marketing-attribution"},{"content":"How-to-Beat-Procrastination-Wait-But-Why # Let’s begin by trying to unwrap the procrastinator’s psychology and see what’s really at the core of things: We know about the Instant Gratification Monkey (the part of your brain that makes you procrastinate) and his dominion over the Rational Decision Maker, but what’s really happening there? The Critical Entrance is where you go to officially start work on the task, the Dark Woods are the process of actually doing the work, and once you finish, you’re rewarded by ending up in The Happy Playground—a place where you feel satisfaction and where leisure time is pleasant and rewarding because you got something hard done. How%20to%20Beat%20Procrastination%20%E2%80%94%20Wait%20But%20Why%205e80a1d1ef8b4588a51618f3f4eafd4c/darkwoodspro2.png Here’s a procrastinator who couldn’t bring himself to get started, even though a work deadline was approaching, and he spent hours in The Dark Playground, knowing the looming deadline was drawing near and he was only making his life harder by not starting. To make things harder, the Dark Woods is surrounded by the Dark Playground, one of the monkey’s favorite places, and since he can see how close it is, he’ll try as hard as he can to leave the Dark Woods. How%20to%20Beat%20Procrastination%20%E2%80%94%20Wait%20But%20Why%205e80a1d1ef8b4588a51618f3f4eafd4c/flow.png Fighting through to the Tipping Point is hard, but what makes procrastination so hard to beat is that the Instant Gratification Monkey has a terribly short-term memory—even if you wildly succeed on Monday, when you begin a task on Tuesday, the monkey has forgotten everything and will again resist entering the Dark Woods or working through them. Laying each brick yields an inner struggle—and in the end, your ability to win this very specific struggle and lay brick after brick, day after day, is what lies at the core of a procrastinator’s struggle to gain control over his world. Part of the reason I assigned terms to so many of these feelings or phenomena—the Instant Gratification Monkey, the Rational Decision-Maker, the Panic Monster, the Dark Playground, Ickiness, Bricks, the Critical Entrance, the Dark Woods, the Tipping Point, the Happy Playground, Flow, your Storyline—is that terms help you clarify the reality of the choices you’re making.\n","date":"16 February 2023","permalink":"/posts/how-to-beat-procrastination-wait-but-why/","section":"Posts","summary":"How-to-Beat-Procrastination-Wait-But-Why # Let’s begin by trying to unwrap the procrastinator’s psychology and see what’s really at the core of things: We know about the Instant Gratification Monkey (the part of your brain that makes you procrastinate) and his dominion over the Rational Decision Maker, but what’s really happening there?","title":"How-to-Beat-Procrastination-Wait-But-Why"},{"content":"How-To-Calculate-Cohort-Retention-in-SQL-Sisense # Calculating basic user retention # The key to calculating retention is counting users who were active at time #1, then counting how many were active at time #2. An easy way to do this in SQL is to left join your user activity table to itself like so:\nselect * from activity left join activity as future_activity on activity.user_id = future_activity.user_id and activity.date = future_activity.date - interval \u0026#39;1 day\u0026#39; Now, for every row of user activity, we have — in that same row — their activity 1 day in the future. This gives us an ideal table for calculating retention with some simple counts:\nselect activity.date, count(distinct activity.user_id) as active_users, count(distinct future_activity.user_id) as retained_users, count(distinct future_activity.user_id) / count(distinct activity.user_id)::float as retention from activity left join activity as future_activity on activity.user_id = future_activity.user_id and activity.date = future_activity.date - interval \u0026#39;1 day\u0026#39; group by 1 We get this chart: For extra credit, change the 1-day retention to 7-day or 30-day to capture a sense of longer-term user engagement. To calculate new user retention, simply join in your users table and only look at activity rows that occurred on the user’s join date:\nselect users.date as date, count(distinct activity.user_id) as new_users, count(distinct future_activity.user_id) as retained_users, count(distinct future_activity.user_id) / count(distinct activity.user_id)::float as retention from activity -- Limits activity to activity from new users join users on activity.user_id = users.id and users.date = activity.date left join activity as future_activity on activity.user_id = future_activity.user_id and activity.date = future_activity.date - interval \u0026#39;1 day\u0026#39; group by 1 We see that while overall retention is 46%, new user retention is only 5.8%! Our query now looks like:\nselect activity.date as date, count(distinct activity.user_id) as new_users, count(distinct future_activity.user_id) as retained_users, count(distinct future_activity.user_id) / count(distinct activity.user_id)::float as retention from activity -- Limits activity to activity from existing users join users on activity.user_id = users.id and users.date != activity.date left join activity as future_activity on activity.user_id = future_activity.user_id and activity.date = future_activity.date - interval \u0026#39;1 day\u0026#39; group by 1 As expected, existing user retention is higher than the overall average: 66% vs. 46%.\nfrom activity join users on users.id = activity.user_id and users.date = activity.date ) Cohort_active_user_count calculates the total number of active users — the denominator in our retention calculation — in each daily cohort: , cohort_active_user_count as ( select date, count(distinct user_id) as count from new_user_activity group by 1 )\nOn top of that, we’ll make a few smaller changes to the main query: Calculate the retention period — the number days retained — as future_activity.date – new_user_activity.date and group by it. Without further ado: select date, \u0026lsquo;Day \u0026lsquo;|| to_char(period, \u0026lsquo;DD\u0026rsquo;) as period, new_users, retained_users, retention from ( select new_user_activity.date as date, (future_activity.date\nnew_user_activity.date) as period, max(cohort_size.count) as new_users, \u0026ndash; all equal in group count(distinct future_activity.user_id) as retained_users, count(distinct future_activity.user_id) / max(cohort_size.count)::float as retention from new_user_activity left join activity as future_activity on new_user_activity.user_id = future_activity.user_id and new_user_activity.date \u0026lt; future_activity.date and (new_user_activity.date + interval \u0026lsquo;10 days\u0026rsquo;) = future_activity.date left join cohort_active_user_count as cohort_size on new_user_activity.date = cohort_size.date group by 1, 2) t where period is not null order by date, period\nNotice also the range join, [one of our favorite SQL tricks](https://www.sisense.com/blog/range-joins-give-you-accurate-histories/), to get multiple days of retention in one chart. ","date":"16 February 2023","permalink":"/posts/how-to-calculate-cohort-retention-in-sql-sisense/","section":"Posts","summary":"How-To-Calculate-Cohort-Retention-in-SQL-Sisense # Calculating basic user retention # The key to calculating retention is counting users who were active at time #1, then counting how many were active at time #2.","title":"How-To-Calculate-Cohort-Retention-in-SQL-Sisense"},{"content":"How-to-Create-Scalable-Data-Pipelines-with-Python # The definition of the message structure is available online, but here’s a sample message:\nevent: message id: [{\u0026#34;topic\u0026#34;:\u0026#34;eqiad.mediawiki.recentchange\u0026#34;,\u0026#34;partition\u0026#34;:0,\u0026#34;timestamp\u0026#34;:1532031066001},{\u0026#34;topic\u0026#34;:\u0026#34;codfw.mediawiki.recentchange\u0026#34;,\u0026#34;partition\u0026#34;:0,\u0026#34;offset\u0026#34;:-1}]data: {\u0026#34;event\u0026#34;: \u0026#34;data\u0026#34;, \u0026#34;is\u0026#34;: \u0026#34;here\u0026#34;} Server Side Events (SSE) are defined by the World Wide Web Consortium (W3C) as part of the HTML5 definition. You have two choices:\nDownload the pre-built Data Pipeline runtime environment (including Python 3.6) for Linux or macOS and install it using the State Tool into a virtual environment, or Follow the instructions provided in my Python Data Pipeline Github repository to run the code in a containerized instance of JupyterLab. Once you’ve installed the Moto server library and the AWS CLI client, you have to create a credentials file at ~/.aws/credentials with the following content in order to authenticate to the AWS services: [default] AWS_ACCESS_KEY_ID = foo AWS_SECRET_ACCESS_KEY = bar You can then launch the SQS mock server from your terminal with the following command:\nmoto_server sqs -p 4576 -H localhost If everything is OK, you can create a queue in another terminal using the following command:\naws --endpoint-url=http://localhost:4576 sqs create-queue --queue-name sse_queue --region us-east-1 This will return the URL of the queue that we’ll use in our SSE Consumer component. To extract just the JSON, we’ll use the SSEClient Python library and code a simple function to iterate over the message stream to pull out the JSON payload, and then place it into the recently created Message Queue using the AWS Boto3 Python library:\nimport boto3 import json from sseclient import SSEClient as EventSource #SQS client library sqs = boto3.client(\u0026#39;sqs\u0026#39; , endpoint_url=\u0026#34;http://localhost:4576\u0026#34; #only for test purposes , use_ssl=False #only for test purposes , region_name=\u0026#39;us-east-1\u0026#39;) queue_url = \u0026#39;http://localhost:4576/queue/sse_queue\u0026#39; def catch_events(): url = \u0026#39;https://stream.wikimedia.org/v2/stream/recentchange\u0026#39; for event in EventSource(url): if event.event == \u0026#39;message\u0026#39;: try: message = json.loads(event.data) except ValueError: pass else: enqueue_message( json.dumps(message) ) def enqueue_message( message ): response = sqs.send_message( QueueUrl = queue_url, DelaySeconds=1, MessageBody = message ) print(\u0026#39;\\rMessage %s enqueued\u0026#39; % response[\u0026#39;MessageId\u0026#39;], sep=\u0026#39; \u0026#39;, end=\u0026#39;\u0026#39;, flush=True ) if __name__== \u0026#34;__main__\u0026#34;: catch_events() This component will run indefinitely, consuming the SSE events and printing the id of each message queued.\nProcessing Data Streams with Python # In order to explore the data from the stream, we’ll consume it in batches of 100 messages. Next, the process_batch function will clean the message’s body and enrich each one with their respective ReceiptHandle*,* which is an attribute from the Message Queue that uniquely identifies the message:\ndef process_batch( messages ): global list_msgs for message in messages: d = json.loads(message[\u0026#39;Body\u0026#39;]) #This just cleans the message\u0026#39;s body from non-desired data clean_dict = { key:(d[key] if key in d else None) for key in map_keys } #We enrich our df with the message\u0026#39;s receipt handle in order to clean it from the queue clean_dict[\u0026#39;ReceiptHandle\u0026#39;] = message[\u0026#39;ReceiptHandle\u0026#39;] list_msgs.append(clean_dict) if len( list_msgs ) \u0026gt;= 100: print(\u0026#39;Batch ready to be exported to the Data Lake\u0026#39;) to_data_lake( list_msgs ) list_msgs = [] This function is an oversimplification. Finally, if the list contains the desired batch size (i.e., 100 messages), our processing function will persist the list into the data lake, and then restart the batch:\ndef to_data_lake( df ): batch_df = pd.DataFrame( list_msgs ) csv = batch_df.to_csv( index=False ) filename = \u0026#39;batch-%s.csv\u0026#39; % df[0][\u0026#39;id\u0026#39;] #csv to s3 bucket s3.Bucket(\u0026#39;sse-bucket\u0026#39;).put_object( Key=filename, Body=csv, ACL=\u0026#39;public-read\u0026#39; ) print(\u0026#39;\\r%s saved into the Data Lake\u0026#39; % filename, sep=\u0026#39; \u0026#39;, end=\u0026#39;\u0026#39;, flush=True ) remove_messages( batch_df ) The to_data_lake function transforms the list into a Pandas DataFrame in order to create a simple CSV file that will be put into the S3 service using the first message of the batch’s ReceiptHandle as a unique identifier.\n","date":"16 February 2023","permalink":"/posts/how-to-create-scalable-data-pipelines-with-python/","section":"Posts","summary":"How-to-Create-Scalable-Data-Pipelines-with-Python # The definition of the message structure is available online, but here’s a sample message:","title":"How-to-Create-Scalable-Data-Pipelines-with-Python"},{"content":"How-to-hire-an-analytics-engineer # Many companies still call these people data analysts, but we’ve started to call them “analytics engineers.” It seems to fit–these hybrids sit at the intersection between data analysts and data engineers.\nIncrease productivity of the data team # It’s still common for data engineers to own 100% of the ETL process in an organization, although this is often a legacy organizational structure from the time when data warehouses weren’t fast enough to allow for data transformation to be done in-warehouse. When data engineers own data transformation, quality erodes because they often don’t quite have the depth of understanding of the business needs that data analysts have. Does a tidy warehouse bring you joy?” An analytics engineer is steeped in practical problems so Thoren avoids people who are too far on either the data engineering side–“too much reliance on a large engineering infrastructure (e.g. Facebook, Google, Microsoft)–or too far on the business side–“too much experience in non-reproducible workflows (e.g. tools like excel, powerpoint, etc).” John Lynch, Data Analyst at CoverWallet, looks for a minimum of three years of experience as a data analyst and “prior experience working with software engineers or software engineering workflows.” But, like Thoren, he warns that being too engineering focused can be a red flag, “We want our analytics engineers to truly be a bridge between engineers and analysts, not just another data engineer.” Andrej Blaha, Head of BI at On also cautions against someone who “wants to build neural networks, but has no real business exposure.” Emily O’Connell, Talent Acquisition Partner at Zylo, says one helpful way to suss out natural affinity for engineering workflows is to ask candidates to, “Explain why their data is impactful, valuable, or special.” She says that different types of analysts are motivated by different things. James Darrell, Data Scientist at HotelEngine, looks for these great SQL skills combined with business knowledge “Very strong SQL knowledge and database knowledge, experience working with ‘dirty’ data, collaborative personality and bias for action, passion for the business and digging into the business needs with respect to data, an understanding that good analytical work starts with a good source of data.” For the GitLab team, great SQL skills includes window functions, CTEs, and in general just more complex SQL. Taylor Murphy avoids candidates whose “primary analyst experience is with ‘all in one’ tools, or those who have only worked on data teams where someone else was cleaning the raw data for them.” At Fishtown Analytics, we’ve hired quite a few analytics engineers and above average SQL is great, but even more than that we look for an openness to feedback on how to write better SQL. Analytics engineers are a great way to get data engineers and data analysts/scientists working together more closely.” We couldn’t agree more.\n","date":"16 February 2023","permalink":"/posts/how-to-hire-an-analytics-engineer/","section":"Posts","summary":"How-to-hire-an-analytics-engineer # Many companies still call these people data analysts, but we’ve started to call them “analytics engineers.","title":"How-to-hire-an-analytics-engineer"},{"content":"How-to-Implement-a-Secure-Central-Authentication-S # How%20to%20Implement%20a%20Secure%20Central%20Authentication%20S%2062529fc47abc4199930b2db48f5fcd28/002_Shop_isolation_of_users_2x_e46bda9c-6859-4248-86c3-a696f7281e63.jpg Shop isolation of users\nModelling User Accounts Within Identity # User accounts modelled within our Identity service are two important types: Identity accounts and Legacy accounts. can only access Shops We ensured that new accounts are created as Identity accounts and that existing users with legacy accounts can be safely and securely upgraded to Identity accounts. Prompt Users to Combine Their Accounts Together Users with an email address that can sign into more than one single Shopify service are prompted to combine their accounts together into a single Identity account. Inside a single database transaction create the complete new account, associate destinations from legacy accounts to it, and delete the old accounts We needed to do this inside a transaction after getting all of the information from a user to prevent the potential for reducing the security of their accounts. If a user was using 2FA before starting this process and we created the Identity account immediately after the new password was provided, there exists a small window of time when their new Identity account would be less secure than their old legacy accounts. Some of the more complex pieces of logic for this process included finding all of the related accounts for a given email address and the information about the destinations they had access to, replacing the legacy accounts when creating the Identity account, and ensuring that the Identity account was setup correctly with all of the required data defined correctly.\nBuild New Experiences for Shopify Users That Rely on SSO Identity Accounts # As of the time of this writing over 75% of active user accounts have been auto-upgraded or combined into a single Identity account.\n","date":"16 February 2023","permalink":"/posts/how-to-implement-a-secure-central-authentication-s/","section":"Posts","summary":"How-to-Implement-a-Secure-Central-Authentication-S # How%20to%20Implement%20a%20Secure%20Central%20Authentication%20S%2062529fc47abc4199930b2db48f5fcd28/002_Shop_isolation_of_users_2x_e46bda9c-6859-4248-86c3-a696f7281e63.","title":"How-to-Implement-a-Secure-Central-Authentication-S"},{"content":"How-to-install-docker-in-Amazon-Linux # How to install docker in Amazon Linux # Created: June 13, 2020 12:14 AM URL: https://www.lewuathe.com/how-to-install-docker-in-amazon-linux.html ! Our daily development tends to depend on the container platform highly. But I found AWS Linux I recently launched does not have Docker engine as default. Here is the process to install Docker engine in your AWS Linux. That article is written mainly for avoiding my memory lost :) FYI: The AMI I used in this experiment is ami-0f9ae750e8274075b.\nInstall Docker Engine # $ sudo yum update -y $ sudo yum install -y docker $ sudo service docker start Starting cgconfig service: [ OK ] Starting Docker: [ OK ] Add User Group # But you need to prepend sudo every time you run docker command.\n$ sudo usermod -a -G docker ec2-user After you log in the instance again, you should be able to run docker command without any difficulty.\n","date":"16 February 2023","permalink":"/posts/how-to-install-docker-in-amazon-linux/","section":"Posts","summary":"How-to-install-docker-in-Amazon-Linux # How to install docker in Amazon Linux # Created: June 13, 2020 12:14 AM URL: https://www.","title":"How-to-install-docker-in-Amazon-Linux"},{"content":"How-to-Pick-the-Best-Data-Visualization-Format-for # How to Pick the Best Data Visualization Format for Your Story # Created: February 21, 2020 12:17 PM Tags: Communication, Data viz, Technical writing URL: https://visage.co/pick-best-data-visualization-format-story/ If you want to tell a powerful story, data is the way to go. That’s why it’s so important to choose the right data visualization format for your data. When it comes to data storytelling, people tend to use certain terms interchangeably (or misuse them entirely), but there are very different types of data mediums and data visualization formats.\nCHOOSING YOUR DATA VISUALIZATION FORMAT # Each data story is unique, so there isn’t a single rule or formula for choosing the best presentation.\nDATA VISUALIZATION # A strong data visualization can be used alone or as part of a larger piece. How%20to%20Pick%20the%20Best%20Data%20Visualization%20Format%20for%2064a32a3c3f7147b9910d567fd38c1bbe/140516-Updated-VN-HackYourGrill-proof-5.png How to choose the right data visualization format *To make sure your data visualizations are effective, design them according to best practices. How%20to%20Pick%20the%20Best%20Data%20Visualization%20Format%20for%2064a32a3c3f7147b9910d567fd38c1bbe/PR012484_SocialPromo_Blog_R1b_jf.gif How to choose data visualization format\nINTERACTIVE INFOGRAPHICS # Interactive infographics are ideal for instances in which you have an enormous amount of data that needs to be easily navigable.\n","date":"16 February 2023","permalink":"/posts/how-to-pick-the-best-data-visualization-format-for/","section":"Posts","summary":"How-to-Pick-the-Best-Data-Visualization-Format-for # How to Pick the Best Data Visualization Format for Your Story # Created: February 21, 2020 12:17 PM Tags: Communication, Data viz, Technical writing URL: https://visage.","title":"How-to-Pick-the-Best-Data-Visualization-Format-for"},{"content":"How-To-Read-an-Unlabeled-Sales-Chart-Evan-Miller # How To Read an Unlabeled Sales Chart – Evan Miller # Created: January 14, 2020 4:02 PM Tags: Data, How To URL: https://www.evanmiller.org/how-to-read-an-unlabeled-sales-chart.html A genre of blog post that is perennially popular among independent software developers is “How My Sales Skyrocketed After…”, where for the ellipsis you my substitute any number of fortuitous events or marketing techniques. As I pondered the blurred-out Y-axis in the above picture, a much more interesting topic occurred to me, and so I’ve decided to write about that instead: how to deduce the scale of a sales chart in which the Y-axis is missing.\nTechnique #1: Counting Pixels and the Riemann Zeta Function # The first thing you might try is to measure the size of each bar in pixels and determine the greatest common divisor among all of the pixel counts. If there is no common divisor among the sale counts, then greatest common divisor among the pixel counts will correspond to the number of pixels that represent a single sale.\nTechnique #2: Properties of a Poisson Process # In the absence of calamity, fortuitous events, or brilliant new marketing strategies, sale counts are well-described by a Poisson process. Assuming sales are a Poisson process, we can equate the sales mean to the sales variance: S=i=1(SPi−S)2 Rearranging you get a nice equation for sales per pixel: S= Which could also be written: S= In plain English, the number of sales per pixel can be estimated as the average bar height in pixels divided by the variance of the bar heights. To see how well this works in practice, I ran the numbers on the first 13 bars of the above chart (that is, before the sales spike, which interrupted the usual Poisson process).\n","date":"16 February 2023","permalink":"/posts/how-to-read-an-unlabeled-sales-chart-evan-miller/","section":"Posts","summary":"How-To-Read-an-Unlabeled-Sales-Chart-Evan-Miller # How To Read an Unlabeled Sales Chart – Evan Miller # Created: January 14, 2020 4:02 PM Tags: Data, How To URL: https://www.","title":"How-To-Read-an-Unlabeled-Sales-Chart-Evan-Miller"},{"content":"How-to-set-up-a-custom-domain-for-your-homepage-in # How to set up a custom domain for your homepage in Notion # Created: January 29, 2020 9:01 AM Tags: How To URL: https://medium.com/@TarasPyoneer/how-to-set-up-a-custom-domain-for-your-homepage-in-notion-53fa3d54f848 ! Notion itself does not provide this function yet, according to their twitter we might see it someday: Luckily I have found this gist from mayneyao. And this is why I decided to create a step by step guide on how to set up custom domains for Notion. To follow this tutorial, you will need:\nCustom domain and access to the DNS settings * Page in Notion that should be your landing page * Account on CloudFlare Let’s start with the page itself. For our use case, we will make use of the Workers feature that will forward incoming requests from users to the public Notion page that we have created earlier. Modify them using your domain in your domain and public Notion page in start page url. If everything went smoothly and the * Notion page is set to public * the nameservers on your domain provider were changed * CloufFlare successfully verified your domain You should be able to see your public Notion page after entering your domain in the URL of your browser: Each use-case is different. ","date":"16 February 2023","permalink":"/posts/how-to-set-up-a-custom-domain-for-your-homepage-in/","section":"Posts","summary":"How-to-set-up-a-custom-domain-for-your-homepage-in # How to set up a custom domain for your homepage in Notion # Created: January 29, 2020 9:01 AM Tags: How To URL: https://medium.","title":"How-to-set-up-a-custom-domain-for-your-homepage-in"},{"content":"How-to-Speak-How-to-Speak-MIT-OpenCourseWare # How to Speak | How to Speak | MIT OpenCourseWare # Created: April 12, 2020 3:35 PM URL: https://ocw.mit.edu/resources/res-tll-005-how-to-speak-january-iap-2018/how-to-speak/index.htm ! RES-TLL-005IAP18.jpg By the end of the next 60 minutes you will have been exposed to a lot of ideas, some of which you will incorporate into your own repertoire, and they will ensure that you get the maximum opportunity to have your ideas valued and accepted by the people you speak with.\nBackground # Around 40 years ago, Professor Patrick Henry Winston ’65, SM ’67, PhD ’70 gave his first talk on How to Speak. We were sitting in my office, whining about somebody\u0026rsquo;s horrible lectures, when he said, “You should do an IAP class on how to speak.” “No,” I said, “I\u0026rsquo;ve never given a lecture I rate at better than a B+; I\u0026rsquo;d be depressed for a month afterward; it would take a week to prepare; and, besides, nobody would come.” “I\u0026rsquo;ll come,\u0026quot; he said. It\u0026rsquo;s a little hard to say exactly because [the lecture hall] officially seats 150 and perhaps another 100 sat on the stairs and floor or stood in the back or watched from the hall. As he put it, \u0026ldquo;There is much more now, of course, because I keep learning new things. I\u0026rsquo;ve added techniques for passing oral exams, delivering successful job-interview talks, and ensuring that ideas become as famous as they ought to be.\n","date":"16 February 2023","permalink":"/posts/how-to-speak-how-to-speak-mit-opencourseware/","section":"Posts","summary":"How-to-Speak-How-to-Speak-MIT-OpenCourseWare # How to Speak | How to Speak | MIT OpenCourseWare # Created: April 12, 2020 3:35 PM URL: https://ocw.","title":"How-to-Speak-How-to-Speak-MIT-OpenCourseWare"},{"content":"How-to-start-automating-your-data-pipelines-with-A # How%20to%20start%20automating%20your%20data%20pipelines%20with%20A%203357f45d3f42478594e6578a8c435e54/1xbaTpPkyt_Q5czvXTzZ0wQ.png The DAG that we are building using Airflow In Airflow, Directed Acyclic Graphs (DAGs) are used to create the workflows. Defining a DAG enables the scheduler to know which tasks can be run immediately, and which have to wait for other tasks to complete. To ensure that Airflow knows all the DAGs and tasks that need to be run, there can only be one scheduler. Tasks are defined as “what to run?” and operators are “how to run”.\nImporting various packages # # airflow relatedfrom airflow import DAGfrom airflow.operators.python_operator import PythonOperatorfrom airflow.operators.bash_operator import BashOperator# other packagesfrom datetime import datetimefrom datetime import timedelta We import three classes, DAG, BashOperator and PythonOperator that will define our basic setup.\nDefining our DAG, Tasks, and Operators # Let’s define all the tasks for our existing workflow.\ndef source1_to_s3(): # code that writes our data from source 1 to s3def source2_to_hdfs(config, ds, **kwargs): # code that writes our data from source 2 to hdfs # ds: the date of run of the given task. ","date":"16 February 2023","permalink":"/posts/how-to-start-automating-your-data-pipelines-with-a/","section":"Posts","summary":"How-to-start-automating-your-data-pipelines-with-A # How%20to%20start%20automating%20your%20data%20pipelines%20with%20A%203357f45d3f42478594e6578a8c435e54/1xbaTpPkyt_Q5czvXTzZ0wQ.","title":"How-to-start-automating-your-data-pipelines-with-A"},{"content":"How-to-take-smart-notes-Ahrens-2017-LessWrong # These are notes made only for a project, such as a note that collects all the notes that you\u0026rsquo;d want to assemble into a paper. You can experiment with other ways to make the notes and you will see immediately what works and what doesn\u0026rsquo;t. Prepare the literature notes so that when you make permanent notes, you can elaborate on the texts, that is, describe the context, find connections and contrasts and contradictions with other texts.\nThere are three kinds of links between notes:\nIndex -\u0026gt; Entry point note Note -\u0026gt; Note Note Note At the top level, there is one note called \u0026ldquo;Index\u0026rdquo;. Luhmann would make these notes to be an annotated list of notes that cover various aspects of the topic. Followed by: [link 5] [link 6] \u0026hellip; After reading this note, you can go along the sequence and read \u0026ldquo;Followed by\u0026rdquo; notes, or take a sideways stride and follow the horizontal link [link 3]. Ideally, you should make the network of slip-box notes to be like a small-world network, with a few notes having many connections, and some notes having \u0026ldquo;weak ties\u0026rdquo; to far-away notes (Granovetter, Mark S, 1977 The Strength of Weak Ties).\n","date":"16 February 2023","permalink":"/posts/how-to-take-smart-notes-ahrens-2017-lesswrong/","section":"Posts","summary":"How-to-take-smart-notes-Ahrens-2017-LessWrong # These are notes made only for a project, such as a note that collects all the notes that you\u0026rsquo;d want to assemble into a paper.","title":"How-to-take-smart-notes-Ahrens-2017-LessWrong"},{"content":"How-we-scrape-300k-prices-per-day-from-Google-Flig # How we scrape 300k prices per day from Google Flights # Created: June 13, 2020 2:23 PM URL: https://medium.com/brisk-voyage/how-we-scrape-300k-flight-prices-per-day-from-google-flights-79f5ddbdc4c0 0*ohKb6_EQ-Ck-aCuG Brisk Voyage finds cheap, last-minute weekend trips for our members. To find flight and hotel prices, we scrape Google Flights and Google Hotels. To do this, we fetch around 300,000 flight prices from 25,000 Google Flights pages every day. The crawler consists of two Lambda functions:\nThe primary Lambda function ingests messages from the SQS queue, crawls Google Flights, and stores the output. We define this as a pure Lambda function with Chalice, as this function will be separately triggered: The second Lambda function triggers multiple instances of the first function. These libraries are added to the vendor directory inside our Chalice project which tells Chalice to install them on each Lambda crawl instance: As the 50 crawl functions start up over ~5 seconds, Chrome instances are launched within each function. This allows the range key to both serve as a unique identifier and support queries such as “give me the cheapest trips to BED that we’ve crawled in the past 30 minutes”: response = crawled_flights_table.query( KeyConditionExpression=Key(\u0026#34;destination\u0026#34;).eq(iata_code) \u0026amp; Key(\u0026#34;id\u0026#34;).gt(earliest_id), FilterExpression=Attr(\u0026#34;cheapest_entry\u0026#34;).eq(1), ) Monitoring and testing # We use Dashbird for monitoring the crawler and everything else that’s run under Lambda.\n","date":"16 February 2023","permalink":"/posts/how-we-scrape-300k-prices-per-day-from-google-flig/","section":"Posts","summary":"How-we-scrape-300k-prices-per-day-from-Google-Flig # How we scrape 300k prices per day from Google Flights # Created: June 13, 2020 2:23 PM URL: https://medium.","title":"How-we-scrape-300k-prices-per-day-from-Google-Flig"},{"content":"Hubspot-Marketing-2-0-ERD-Google-Slides # Hubspot Marketing 2.0 ERD - Google Slides # Created: April 18, 2020 6:11 PM URL: https://docs.google.com/presentation/d/1hrPp310SNK2qyESCV_g_JFx_Knm1MwB467wN3dEgy0M/edit#slide=id.g244d368397_0_1 https://docs.google.com/presentation/d/1hrPp310SNK2qyESCV_g_JFx_Knm1MwB467wN3dEgy0M/edit#slide=id.g244d368397_0_1\n","date":"16 February 2023","permalink":"/posts/hubspot-marketing-2-0-erd-google-slides/","section":"Posts","summary":"Hubspot-Marketing-2-0-ERD-Google-Slides # Hubspot Marketing 2.","title":"Hubspot-Marketing-2-0-ERD-Google-Slides"},{"content":"I-almost-sold-Baremetrics-for-5m-Baremetrics # In the past six years of Baremetrics’ existence, I’ve received dozens upon dozens of emails from folks interested in acquiring Baremetrics. I’ve always sort of assumed that the sale of Baremetrics would one day mean I make a decent chunk of money, but now it was “Hey Josh, you could be a millionaire in a matter of weeks!”\nThere’s always a price # I was still very torn on the idea. Most people will tell you, especially when things are going well, that they’re not interested in selling their company. During this time I was also having weekly calls with the buyer, checking in as they did their own research and (theoretically) had their lawyers looking over things while we did our work with diligence documents. After that I went back to the drawing board to try and salvage all the work that’d been done with other interested buyers, but at the end of the day, none of the other offers (which were anywhere from $1m to $4.5m) made economic, logistical or cultural sense. Once I realized we didn’t have any offers that I was happy with, I pouted for a day, then immediately jumped back in to running the company and planning out the future. *If you’re interested, read about the 5 things I learned failing to sell the company!\n","date":"16 February 2023","permalink":"/posts/i-almost-sold-baremetrics-for-5m-baremetrics/","section":"Posts","summary":"I-almost-sold-Baremetrics-for-5m-Baremetrics # In the past six years of Baremetrics’ existence, I’ve received dozens upon dozens of emails from folks interested in acquiring Baremetrics.","title":"I-almost-sold-Baremetrics-for-5m-Baremetrics"},{"content":"Indoor-Environmental-Quality-Aging-In-Place-About # Indoor Environmental Quality :: Aging In Place :: About Healthy Heating # Created: May 23, 2020 11:46 AM URL: http://www.healthyheating.com/about.htm#.Wz4YIC2ZMk9 We support the purposes and missions of the healthy house instituteTM I consider Robert Bean to be one of the most knowledgeable professionals in this field. Not just in North America, but globally. Robert presents the “big picture” of how indoor environmental quality affects human comfort, productivity, and health. He is adept at explaining that it’s not just room air temperature, but rather a complex interaction of room air temperature, humidity, mean radiant temperature, ventilation rate, lighting, sound, and even odors that determine the true quality of an interior environment. Robert explains complex topics in ways that are understandable, applicable, and entertaining. He doesn’t just share his opinions, but builds his points and recommendations upon solid research, extensive references, and scientific reason. An opportunity to learn the details of integrated system design from Robert is indeed one to be savored.\u0026quot;\n","date":"16 February 2023","permalink":"/posts/indoor-environmental-quality-aging-in-place-about/","section":"Posts","summary":"Indoor-Environmental-Quality-Aging-In-Place-About # Indoor Environmental Quality :: Aging In Place :: About Healthy Heating # Created: May 23, 2020 11:46 AM URL: http://www.","title":"Indoor-Environmental-Quality-Aging-In-Place-About"},{"content":"Installing-Postgres-via-Brew-OSX # Installing Postgres via Brew (OSX) # Created: December 5, 2019 4:44 PM Tags: Data, How To URL: https://gist.github.com/ibraheem4/ce5ccd3e4d7a65589ce84f2a3b7c23a3 https://gist.github.com/ibraheem4/ce5ccd3e4d7a65589ce84f2a3b7c23a3\n","date":"16 February 2023","permalink":"/posts/installing-postgres-via-brew-osx/","section":"Posts","summary":"Installing-Postgres-via-Brew-OSX # Installing Postgres via Brew (OSX) # Created: December 5, 2019 4:44 PM Tags: Data, How To URL: https://gist.","title":"Installing-Postgres-via-Brew-OSX"},{"content":"Interesting-number-paradox-Wikipedia # Interesting number paradox - Wikipedia # Created: February 2, 2020 7:56 AM URL: https://en.wikipedia.org/wiki/Interesting_number_paradox The interesting number paradox is a semi-humorous paradox which arises from the attempt to classify every natural number as either \u0026ldquo;interesting\u0026rdquo; or \u0026ldquo;uninteresting\u0026rdquo;. The \u0026ldquo;proof\u0026rdquo; is by contradiction: if there exists a non-empty set of uninteresting natural numbers, there would be a smallest uninteresting number – but the smallest uninteresting number is itself interesting because it is the smallest uninteresting number, thus producing a contradiction. In a discussion between the mathematicians G. H. Hardy and Srinivasa Ramanujan about interesting and uninteresting numbers, Hardy remarked that the number 1729 of the taxicab he had ridden seemed \u0026ldquo;rather a dull one\u0026rdquo;, and Ramanujan immediately answered that it is interesting, being the smallest number that is the sum of two cubes in two different ways.\nParadoxical nature[edit] # Attempting to classify all numbers this way leads to a paradox or an antinomy of definition. [1] The number fitting this definition later became 12407 from November 2009 until at least November 2011, then 13794 as of April 2012, until it appeared in sequence OEIS: A218631 as of 3 November 2012. [citation needed] The mathematician and philosopher Alex Bellos suggested in 2014 that a candidate for the lowest uninteresting number would be 247 because it was, at the time, \u0026ldquo;the lowest number not to have its own page on Wikipedia\u0026rdquo;. [3] Untitled\nSee also[edit] # Church–Turing thesis Gödel\u0026rsquo;s incompleteness theorems Grelling–Nelson paradox Kleene–Rosser paradox List of paradoxes Richard\u0026rsquo;s paradox The Penguin Dictionary of Curious and Interesting Numbers Unexpected hanging paradox Further reading[edit] # Gardner, Martin (1959). ","date":"16 February 2023","permalink":"/posts/interesting-number-paradox-wikipedia/","section":"Posts","summary":"Interesting-number-paradox-Wikipedia # Interesting number paradox - Wikipedia # Created: February 2, 2020 7:56 AM URL: https://en.","title":"Interesting-number-paradox-Wikipedia"},{"content":"isEven-API-Tell-if-a-number-is-even # isEven API - Tell if a number is even # Created: March 7, 2020 2:59 PM URL: https://isevenapi.xyz/ ! isEven%20API%20-%20Tell%20if%20a%20number%20is%20even%2075600afd233a4f2390275c869b1e7f46/header-software-app.png ! isEven%20API%20-%20Tell%20if%20a%20number%20is%20even%2075600afd233a4f2390275c869b1e7f46/description-1.png ! isEven%20API%20-%20Tell%20if%20a%20number%20is%20even%2075600afd233a4f2390275c869b1e7f46/description-2.png\nisEven Public API Docs # isEven API is a RESTful API that returns json. API URL: https://api.isevenapi.xyz/api/\nGET /iseven// # Returns whether a given number is even. See Pricing below. URL Parameters number: the number you want to check Example\n$ curl https://api.isevenapi.xyz/api/iseven/6/ { \u0026#34;iseven\u0026#34;: true, \u0026#34;ad\u0026#34;: \u0026#34;Buy isEvenCoin, the hottest new cryptocurrency!\u0026#34; ","date":"16 February 2023","permalink":"/posts/iseven-api-tell-if-a-number-is-even/","section":"Posts","summary":"isEven-API-Tell-if-a-number-is-even # isEven API - Tell if a number is even # Created: March 7, 2020 2:59 PM URL: https://isevenapi.","title":"isEven-API-Tell-if-a-number-is-even"},{"content":"Jane-Eyre-and-the-Invention-of-Self # The actress Joan Fontaine as Jane Eyre in the 1943 film 20TH CENTURY FOX The actress Joan Fontaine as Jane Eyre in the 1943 film 20TH CENTURY FOX Consider the selfie. Perhaps the first novel to best express the modern idea of the self was Jane Eyre, written in 1847 by Charlotte Brontë, born 200 years ago this year. The broader cultural implications of the story—its insistence on the value of conscience and will—were such that one critic fretted some years after its publication that the “most alarming revolution of modern times has followed the invasion of Jane Eyre.” Before the Reformation and the Enlightenment that followed, before Rene Descartes’s cogito ergo sum (“I think, therefore I am”), when the sources of authority were external and objective, the aspects of the self so central to today’s understanding mattered little because they didn’t really affect the course of an individual’s life. “As we open Jane Eyre once more,” a doubting Woolf wrote in The Common Reader, “we cannot stifle the suspicion that we shall find her world of imagination as antiquated, mid-Victorian, and out of date as the parsonage on the moor, a place only to be visited by the curious, only preserved by the pious.” Woolf continues, “So we open Jane Eyre; and in two pages every doubt is swept clean from our minds.” There is nothing of the book, Woolf declares, “except Jane Eyre.” Jane’s voice is the source of the power the book has to absorb the reader completely into her world. More disturbing to Brontë’s Victorian readers than the sheer sensuality of the story and Jane’s deep passion was “the heroine’s refusal to submit to her social destiny,” as the literary critic Sandra M. Gilbert explains. Indeed, one contemporary review complained, “It is true Jane does right, and exerts great moral strength,” but the critic continues that “it is the strength of a mere heathen mind which is a law unto itself.” In presenting such a character, the reviewer worries, Brontë has “overthrown authority” and cultivated “rebellion.” And in a way they were right: “I resisted all the way,” Jane says as she is dragged by her cruel aunt toward banishment in the bedroom where her late uncle died.\nIn a letter to a friend, Bronte responded to her critics’ objections by declaring, “Unless I have the courage to use the language of Truth in preference to the jargon of Conventionality, I ought to be silent \u0026hellip;” The refusal of such a woman, who lived in such a time, to be silent created a new mold for the self—one apparent not only in today’s Instagram photos, but also more importantly in the collective modern sense that a person’s inner life can allow her to effect change from the inside out.\n","date":"16 February 2023","permalink":"/posts/jane-eyre-and-the-invention-of-self/","section":"Posts","summary":"Jane-Eyre-and-the-Invention-of-Self # The actress Joan Fontaine as Jane Eyre in the 1943 film 20TH CENTURY FOX The actress Joan Fontaine as Jane Eyre in the 1943 film 20TH CENTURY FOX Consider the selfie.","title":"Jane-Eyre-and-the-Invention-of-Self"},{"content":"JavaScript-Jinja-Flask-Crawford-Collins-Med # JavaScript, Jinja \u0026amp; Flask - Crawford Collins - Medium # Created: February 17, 2020 10:14 PM URL: https://medium.com/@crawftv/javascript-jinja-flask-b0ebfdb406b3 ! 1*2QTOeryqlhZSGCU5SyvE0g.jpeg This article is guide for getting data from your python program to JavaScript inside of a Jinja HTML template .\nStep 1: Make sure to use the right Flask JSON module. # The flask library has two libraries for making JSON objects. Technically this code uses the standard library JSON module, but it still needs to be imported through flask. Get the template ready for the JavaScript code by adding a special JavaScript block content and adding script tags. Untitled\nStep 3: Admire your work # Being able to incorporate a little JavaScript into your Flask app makes it look 10 times better.\n","date":"16 February 2023","permalink":"/posts/javascript-jinja-flask-crawford-collins-med/","section":"Posts","summary":"JavaScript-Jinja-Flask-Crawford-Collins-Med # JavaScript, Jinja \u0026amp; Flask - Crawford Collins - Medium # Created: February 17, 2020 10:14 PM URL: https://medium.","title":"JavaScript-Jinja-Flask-Crawford-Collins-Med"},{"content":"Javascript-tracker-setup-snowplow-snowplow-Wiki # Javascript tracker setup · snowplow/snowplow Wiki # Created: April 25, 2020 9:36 AM URL: https://github.com/snowplow/snowplow/wiki/Javascript-tracker-setup 1476001 HOME » SNOWPLOW SETUP GUIDE » Step 2: setup a Tracker » JavaScript Tracker Note: Before beginning this tutorial, you will first need to host the Snowplow JavaScript tracker file sp.js. Before you integrate Snowplow\u0026rsquo;s JavaScript Tracker, you need to decide whether you\u0026rsquo;ll integrate it with a tag management system, or implement the Snowplow tags directly onto your site. We strongly advise new Snowplow users who are not using a Tag Management solution to implement one before implementing Snowplow, and integrate Snowplow using it. We have documented how to setup Google Tag Manager and how to setup QuBit\u0026rsquo;s OpenTag, as well as how to integrate Snowplow using both these solutions, as part of this setup guide. Select a setup option below based on your choice of Tag Management solution: Once you have integrated tags on our site (either directly or via a tag manager) you should test that the tags are firing correctly. Setting up campaign tracking (optional but recommended). Back to Tracker Setup.\n","date":"16 February 2023","permalink":"/posts/javascript-tracker-setup-snowplow-snowplow-wiki/","section":"Posts","summary":"Javascript-tracker-setup-snowplow-snowplow-Wiki # Javascript tracker setup · snowplow/snowplow Wiki # Created: April 25, 2020 9:36 AM URL: https://github.","title":"Javascript-tracker-setup-snowplow-snowplow-Wiki"},{"content":"Josh-Wills-on-Building-Resilient-Data-Engineering # Josh Wills on Building Resilient Data Engineering and Machine Learning Products at Slack # Created: May 27, 2020 12:45 PM Tags: Data engineering, Etl, Programming URL: https://www.infoq.com/podcasts/slack-building-resilient-data-engineering/?useSponsorshipSuggestions=true\u0026amp;itm_source=podcasts_about_ai-ml-data-eng\u0026amp;itm_medium=link\u0026amp;itm_campaign=ai-ml-data-eng ! Slack-Josh-Wills-1575853123115.jpg\n","date":"16 February 2023","permalink":"/posts/josh-wills-on-building-resilient-data-engineering/","section":"Posts","summary":"Josh-Wills-on-Building-Resilient-Data-Engineering # Josh Wills on Building Resilient Data Engineering and Machine Learning Products at Slack # Created: May 27, 2020 12:45 PM Tags: Data engineering, Etl, Programming URL: https://www.","title":"Josh-Wills-on-Building-Resilient-Data-Engineering"},{"content":"JSON-Web-Tokens-JWT # 1*9pWUdYPAwCQoWdEnvZli4A.jpeg\nJSON Web Token (JWT) Is a JSON object and it is considered one of the safest ways to transfer information between two participants. To create it, you need to define a header with general information on the token, payload data, such as the user id, his role, etc. First, the user logs on to the authentication server using an authentication key (it can be a username / password pair , or a Facebook key, or a Google key, or a key from another account). The JWT header contains information on how the JWT signature should be calculated . A header is also a JSON object that looks like this: header = { “alg”: “HS256”, “typ”: “JWT”}\nIt will be used when creating the signature. In the example the authentication server creates a JWT with information about the userId\npayload = { “userId”: “b08f86af-35da-48f2–8fab-cef3904660bd” }\nThere is a list of standard applications for JWT payload — here are some of them:\niss (issuer) — defines the application from which the token is sent. // header eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9// payload eyJ1c2VySWQiOiJiMDhmODZhZi0zNWRhLTQ4ZjItOGZhYi1jZWYzOTA0NjYwYmQifQ// signature -xN_h82PHVTCMA9vdoHrcZxH-x5mb11y1537t3rGzcM\nNow that we have all three components, we can create our JWT const token = encodeBase64Url(header) + ‘.’ + encodeBase64Url(payload) + ‘.’ + encodeBase64Url(signature) // JWT Token //eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJ1c2VySWQiOiJiMDhmODZhZi0zNWRhLTQ4ZjItOGZhYi1jZWYzOTA0NjYwYmQifQ.-xN_h82PHVTCMA9vdoHrcZxH-x5mb11y1537t3rGzcM **You can try to create your own JWT Online jwt.io .\n","date":"16 February 2023","permalink":"/posts/json-web-tokens-jwt/","section":"Posts","summary":"JSON-Web-Tokens-JWT # 1*9pWUdYPAwCQoWdEnvZli4A.","title":"JSON-Web-Tokens-JWT"},{"content":"Kurt-Vonnegut-on-the-8-shapes-of-stories-Big-Thi # Kurt Vonnegut on the 8 shapes of stories - Big Think # Created: December 19, 2020 11:30 AM URL: https://bigthink.com/culture-religion/vonnegut-shapes\nThe American author said he attempted to bring scientific thinking to literary criticism, but received \u0026ldquo;very little gratitude for this.\u0026rdquo; # Kurt%20Vonnegut%20on%20the%208%20shapes%20of%20stories%20-%20Big%20Thi%2060923728876e40ef89675fb2f36aa7ab/img.jpg\nKurt Vonnegut wrote a master\u0026rsquo;s thesis on the shapes of stories that he submitted to the anthropology department at the University of Chicago, which rejected it. Vonnegut half-jokingly defended his \u0026ldquo;scientific\u0026rdquo; approach to literary criticism over his career, but noted that great stories can\u0026rsquo;t be easily plotted on a diagram. But in his autobiography Palm Sunday, Vonnegut claimed that his \u0026ldquo;prettiest contribution\u0026rdquo; to the culture is his theory on the shapes of stories. Vonnegut describes how Hamlet\u0026rsquo;s experiences throughout the play can\u0026rsquo;t easily be classified as good or bad. Ambiguities like this make the story of \u0026ldquo;Hamlet\u0026rdquo; difficult to plot on Vonnegut\u0026rsquo;s diagram. But Vonnegut suggested that learning to see life\u0026rsquo;s ebbs and flows in stories might help you appreciate when things are good in your life. ","date":"16 February 2023","permalink":"/posts/kurt-vonnegut-on-the-8-shapes-of-stories-big-thi/","section":"Posts","summary":"Kurt-Vonnegut-on-the-8-shapes-of-stories-Big-Thi # Kurt Vonnegut on the 8 shapes of stories - Big Think # Created: December 19, 2020 11:30 AM URL: https://bigthink.","title":"Kurt-Vonnegut-on-the-8-shapes-of-stories-Big-Thi"},{"content":"Leaflet-How-to-save-various-marker-position-in-db # Leaflet: How to save various marker position in db and load later - Stack Overflow # Created: February 27, 2020 2:03 PM URL: https://stackoverflow.com/questions/36185565/leaflet-how-to-save-various-marker-position-in-db-and-load-later ! apple-touch-icon@2.png I would do something like this:\nvar map = L.map(\u0026#39;mapcanvas\u0026#39;).setView([51.505, -0.09], 13); map.on(\u0026#39;click\u0026#39;, function(e){ // Place marker var marker = new L.marker(e.latlng).addTo(map); // Ajax query to save the values: var data = { lat: e.latlng.lat, lng: e.latlng.lng } var request = new XMLHttpRequest(); request.open(\u0026#39;POST\u0026#39;, \u0026#39;/my/url\u0026#39;, true); request.setRequestHeader(\u0026#39;Content-Type\u0026#39;, \u0026#39;application/x-www-form-urlencoded; charset=UTF-8\u0026#39;); request.send(data); }); Although it is probably better to use jQuery. You would then obtain your saved values from the database, store them in a js object, like\nvar positions = [ { lat: 123.45 lng: 123.45 }, { lat: 123.45 lng: 123.45 } ] Iterate over them and add markers:\nfor (var i in positions) { new L.marker([positions[i].lat, positions[i].lng]).addTo(map); } ","date":"16 February 2023","permalink":"/posts/leaflet-how-to-save-various-marker-position-in-db/","section":"Posts","summary":"Leaflet-How-to-save-various-marker-position-in-db # Leaflet: How to save various marker position in db and load later - Stack Overflow # Created: February 27, 2020 2:03 PM URL: https://stackoverflow.","title":"Leaflet-How-to-save-various-marker-position-in-db"},{"content":"Machine-Learning-1-Lesson-1-Hiromi-Suenaga-Med # 1*ZzaVXSfj0SCERQUCnQljpw.png\nSyllabus in brief # Depending on time and class interests, we’ll cover something like (not necessarily in this order): Train vs. test\nEffective validation set construction Trees and ensembles Creating random forests Interpreting random forests What is ML? Ethical considerations Skip: Dimensionality reduction Interactions Monitoring training Collaborative filtering Momentum and LR annealing Random Forest: Blue Book for Bulldozers # %load_ext autoreload%autoreload 2%matplotlib inlinefrom fastai.imports import *from fastai.structured import *from pandas_summary import DataFrameSummaryfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifierfrom IPython.display import display Data science ≠Software engineering [08:43]. Unstructured data: Images pandas is the most important library when you are working with structured data which is usually imported as pd. The variable we want to predict is called Dependent Variable in this case our dependent variable is SalePrice Question: Should you never look at the data because of the risk of overfit?\nCreate an instance of an object for the machine learning model Call fit by passing in the independent variables (the things you are going to use to predict) and dependent variable (the thing you want to predict). It is important that validation and test sets will use the same category mappings (in other words, if you used 1 for “high” for a training dataset, then 1 should also be for “high” in validation and test datasets). df, y, nas = proc_df(df_raw, \u0026#39;SalePrice\u0026#39;) proc_df in structured.py\ndf — data frame y_fld — name of the dependent variable It makes a copy of the data frame, grab the dependent variable values (y_fld), and drop the dependent variable from the data frame. ","date":"16 February 2023","permalink":"/posts/machine-learning-1-lesson-1-hiromi-suenaga-med/","section":"Posts","summary":"Machine-Learning-1-Lesson-1-Hiromi-Suenaga-Med # 1*ZzaVXSfj0SCERQUCnQljpw.","title":"Machine-Learning-1-Lesson-1-Hiromi-Suenaga-Med"},{"content":"Making-Wrong-Code-Look-Wrong-Joel-on-Software # Making Wrong Code Look Wrong – Joel on Software # Created: May 20, 2021 3:30 AM URL: https://www.joelonsoftware.com/2005/05/11/making-wrong-code-look-wrong/ ! 11969842.jpg\n","date":"16 February 2023","permalink":"/posts/making-wrong-code-look-wrong-joel-on-software/","section":"Posts","summary":"Making-Wrong-Code-Look-Wrong-Joel-on-Software # Making Wrong Code Look Wrong – Joel on Software # Created: May 20, 2021 3:30 AM URL: https://www.","title":"Making-Wrong-Code-Look-Wrong-Joel-on-Software"},{"content":"Managing-my-personal-knowledge-base-tkainrad # Some examples of well-suited Chrome bookmarks:\nAWS EC2 dashboard Google cloud platform console Slack workspaces GitLab boards Examples of things that should not be Chrome bookmarks: Blog posts to be read later Links to Open Source Projects Stack Overflow questions 2. # Managing%20my%20personal%20knowledge%20base%20%C2%B7%20tkainrad%20ab3a60ace0994129b546bac8538158da/notion-bookmark-database.png My bookmarks database filtered to show only entries related to this post. To keep track of third-party resources that were helpful in creating a post, I add them to my bookmarks database and link them to my posts database. Managing%20my%20personal%20knowledge%20base%20%C2%B7%20tkainrad%20ab3a60ace0994129b546bac8538158da/notion-blog-sources.png My bookmarks database filtered to show only entries related to this post. On the other hand, it is important to make clear also what should not be a note in this system:\nProject-specific knowledge Everything that is directly related to a specific software project, should not be a note in your personal knowledge base. Managing%20my%20personal%20knowledge%20base%20%C2%B7%20tkainrad%20ab3a60ace0994129b546bac8538158da/vscode-insert-snippet.gif Notion Snippet database # The second use case concerns small pieces of code that I have written myself and that I would like to remember. Managing%20my%20personal%20knowledge%20base%20%C2%B7%20tkainrad%20ab3a60ace0994129b546bac8538158da/notion-snippet.database.png An exemplary entry of my code snippets database in Notion.\n","date":"16 February 2023","permalink":"/posts/managing-my-personal-knowledge-base-tkainrad/","section":"Posts","summary":"Managing-my-personal-knowledge-base-tkainrad # Some examples of well-suited Chrome bookmarks:","title":"Managing-my-personal-knowledge-base-tkainrad"},{"content":"Mapbox-Tutorial-Heatmap-using-GL-JS-Madison-Drap # Mapbox Tutorial: Heatmap using GL JS - Madison Draper - Medium # Created: February 20, 2020 9:02 AM URL: https://medium.com/@mzdraper/mapbox-tutorial-heatmap-using-gl-js-bc2e5199d630 ! 1*Un83np6gXvUEialrlXo2pA.png Heatmaps, or isorhythmic maps, display dense amounts of points as a painted cluster. One of the most common heat maps is a meteorological map. Here is Mapbox’s original tutorial for another point of reference. Add Mapbox GL JS between the tags. Between the tags, add CSS styling for your map. Add your map container between the `` tags.\n","date":"16 February 2023","permalink":"/posts/mapbox-tutorial-heatmap-using-gl-js-madison-drap/","section":"Posts","summary":"Mapbox-Tutorial-Heatmap-using-GL-JS-Madison-Drap # Mapbox Tutorial: Heatmap using GL JS - Madison Draper - Medium # Created: February 20, 2020 9:02 AM URL: https://medium.","title":"Mapbox-Tutorial-Heatmap-using-GL-JS-Madison-Drap"},{"content":"Mapping-All-of-the-Trees-with-Machine-Learning-d # 1*MZcIQRCAJX5fbn0Zw6cPlA.jpeg Descartes Labs built a machine learning model to identify tree canopy globally using a combination of lidar, aerial imagery and satellite imagery. San Francisco’s wonderful Open Forest Map tree inventory (point data) alternating with the Descartes Labs tree canopy layer (image data) This data gap is neither accidental nor purposeful. Note the city census does not include park trees or trees in private gardens. Boston vegetation (NDVI from an August 28, 2018 Sentinel 2 scene) alternating with the Descartes Labs tree canopy layer — looks like quite a lot of that vegetation might not be trees! Washington, D.C. tree canopy created with NAIP source imagery shown at different scales—all the way down to individual “TREES!” on The Ellipse. The ability to map tree canopy at a such a high resolution in areas that can’t be easily reached on foot would be helpful for utility companies to pinpoint encroachment issues—or for municipalities to find possible trouble spots beyond their official tree census (if they even have one). Scroll through this New York City tree image and notice how the landscape of trees ebbs and flows throughout.\n","date":"16 February 2023","permalink":"/posts/mapping-all-of-the-trees-with-machine-learning-d/","section":"Posts","summary":"Mapping-All-of-the-Trees-with-Machine-Learning-d # 1*MZcIQRCAJX5fbn0Zw6cPlA.","title":"Mapping-All-of-the-Trees-with-Machine-Learning-d"},{"content":"Materialize-The-Streaming-Data-Warehouse # Materialize – The Streaming Data Warehouse # Created: February 18, 2020 5:46 PM URL: https://materialize.io/\nBuilding streaming data architecture can be as simple as writing SQL. # Materialize easily processes complex analytics over streaming datasets – offering the power and flexibility of a SQL data warehouse for the world of real-time data. Materialize%20%E2%80%93%20The%20Streaming%20Data%20Warehouse%2057dbeb0ce07c4e858c411328641e8b50/home-large.png Materialize is the only technology that can enable engineers to completely develop for streaming data in a powerful declarative language – PostgreSQL – instead of building microservices. Materialize%20%E2%80%93%20The%20Streaming%20Data%20Warehouse%2057dbeb0ce07c4e858c411328641e8b50/SQL-01-1250.png Materialize is the first and only streaming system capable of running the TPC-H decision support benchmark – a set of criteria originally developed for the world of batch data warehouses. With Materialize, data engineering teams can move away from the endless development of microservices – and replace thousands of lines of Java with a few lines of SQL. Materialize%20%E2%80%93%20The%20Streaming%20Data%20Warehouse%2057dbeb0ce07c4e858c411328641e8b50/microservices-01-1250.png ! Materialize%20%E2%80%93%20The%20Streaming%20Data%20Warehouse%2057dbeb0ce07c4e858c411328641e8b50/timely-dataflow-01-1250.png\nPowered by Timely Dataflow # Materialize offers all of the benefits of Timely Dataflow and Differential Dataflow, accessible via the familiar and accessible PostgreSQL.\n","date":"16 February 2023","permalink":"/posts/materialize-the-streaming-data-warehouse/","section":"Posts","summary":"Materialize-The-Streaming-Data-Warehouse # Materialize – The Streaming Data Warehouse # Created: February 18, 2020 5:46 PM URL: https://materialize.","title":"Materialize-The-Streaming-Data-Warehouse"},{"content":"MCF-Data-Driven-Attribution-methodology-Analytic # MCF Data-Driven Attribution methodology - Analytics Help # Created: February 21, 2020 12:08 PM URL: https://support.google.com/analytics/answer/3191594?hl=en\u0026amp;ref_topic=3180362 There are two main parts to the Multi-Channel Funnels (MCF) Data-Driven Attribution methodology: (1) analyzing all of your available path data to develop custom conversion probability models, and (2) applying to that probabilistic data set a sophisticated algorithm that assigns partial conversion credit to your marketing touchpoints.\nDevelop conversion probability models from all available path data # MCF Data-Driven Attribution uses all available path data—including data from both converting and non-converting users—to understand how the presence of particular marketing touchpoints impacts your users’ probability of conversion.\nAlgorithmically assign conversion credit to marketing touchpoints # MCF Data-Driven Attribution then applies to this probabilistic data set an algorithm based on a concept from cooperative game theory called the Shapley Value. In the case of MCF Data-Driven Attribution, the “team” being analyzed has marketing touchpoints (e.g., Organic Search, Display, and Email) as “team members,” and the “output” of the team is conversions. The MCF Data-Driven Attribution algorithm computes the counterfactual gains of each marketing touchpoint—that is, it compares the conversion probability of similar users who were exposed to these touchpoints, to the probability when one of the touchpoints does not occur in the path. This means that the MCF Data-Driven Attribution algorithm takes into account the order in which each touchpoint occurs and assigns different credit for different path positions. MCF%20Data-Driven%20Attribution%20methodology%20-%20Analytic%202981068ac24846aa90bd6729b788f890/tMncmkROzRFPKDix5qqyoZ2VPUrOdSpS_Z1C5GOiMAFPorZQuGjt5rPv4zrN6-nOoRaFTxNtw520 illustration of display increasing likelihood of purchase\nExplore your MCF Data-Driven model and analyze its ROI implications # You can use the MCF Model Explorer report to explore the specific weights your Data-Driven model sets based on channel and position.\n","date":"16 February 2023","permalink":"/posts/mcf-data-driven-attribution-methodology-analytic/","section":"Posts","summary":"MCF-Data-Driven-Attribution-methodology-Analytic # MCF Data-Driven Attribution methodology - Analytics Help # Created: February 21, 2020 12:08 PM URL: https://support.","title":"MCF-Data-Driven-Attribution-methodology-Analytic"},{"content":"Medical-Care-of-Neonatal-Kittens-Ontario-Shelter # Nursing: Watch out for too much of a good thing # It may seem really obvious, but kitten carers often think that if a kitten is constantly nursing from the queen, all must be well. Medical%20Care%20of%20Neonatal%20Kittens%20-%20Ontario%20Shelter%20571e024c67844e75b5d4773072420d3d/Kitten-incubator-1-150x150.jpg DIY kitten incubator (Courtesy: Linda Jacobson, Toronto Humane Society)\nKitten reflexes # Absence or weakness of these basic reflexes, necessary for survival, are indicators of serious trouble. The kitten should suckle automatically\nDrug dosing in neonates # Liver and kidney function are immature up to 6 weeks of age, so drug metabolism is impaired.\nCrashing kittens # Critically ill kittens typically have hypothermia, hypoglycemia and/or dehydration. Only give oral fluids or dextrose after the kitten has been warmed.\nThe colours of kittens (mucous membranes and stool, that is) # Mucous membranes:\nHyperemia is normal for kittens \u0026lt; 7 days – thereafter their mucous membranes should be pink and hyperemia in kittens \u0026gt; 7d can signal dehydration Sick kittens can be pale or cyanotic Septicemia is a common end result of many conditions. Stool: Normal neonate stool is yellow or tan colour, with a toothpaste consistency Yellow/green watery stool is a sign of overfeeding Blood in the stool can indicate coccidiosis, sepsis, parvo, E.coli or Salmonella infections X-rays in kittens # Reduce KVP by 50% because the low fat percentage results in low contrast.\n","date":"16 February 2023","permalink":"/posts/medical-care-of-neonatal-kittens-ontario-shelter/","section":"Posts","summary":"Medical-Care-of-Neonatal-Kittens-Ontario-Shelter # Nursing: Watch out for too much of a good thing # It may seem really obvious, but kitten carers often think that if a kitten is constantly nursing from the queen, all must be well.","title":"Medical-Care-of-Neonatal-Kittens-Ontario-Shelter"},{"content":"Microsoft-PowerPoint-Column-Oriented-Database-Sy # Microsoft PowerPoint - Column-Oriented Database Systems FINAL - Column_Store_Tutorial_VLDB09.pdf # Created: April 20, 2020 10:08 PM URL: https://web.cs.ucdavis.edu/~green/courses/ecs165b-s10/Column_Store_Tutorial_VLDB09.pdf Column_Store_Tutorial_VLDB09.pdf\n","date":"16 February 2023","permalink":"/posts/microsoft-powerpoint-column-oriented-database-sy/","section":"Posts","summary":"Microsoft-PowerPoint-Column-Oriented-Database-Sy # Microsoft PowerPoint - Column-Oriented Database Systems FINAL - Column_Store_Tutorial_VLDB09.","title":"Microsoft-PowerPoint-Column-Oriented-Database-Sy"},{"content":"Model-beats-Wall-Street-analysts-in-forecasting-bu # Combining alternative data with more traditional but infrequent ground-truth financial data — such as quarterly earnings, press releases, and stock prices — can paint a clearer picture of a company’s financial health on even a daily or weekly basis. Notably, the analysts had access to any available private or public data and other machine-learning models, while the researchers’ model used a very small dataset of the two data types. “We asked, ‘Can you combine these noisy signals with quarterly numbers to estimate the true financials of a company at high frequencies?’ Turns out the answer is yes.” The model could give an edge to investors, traders, or companies looking to frequently compare their sales with competitors. Credit card data for, say, every week over the same period is only roughly another 100 “noisy” data points, meaning they contain potentially uninterpretable information. Also, including alternative data to help understand how sales vary over a quarter complicates matters: Apart from being noisy, purchased credit card data always consist of some indeterminate fraction of the total sales. Then, it matches the observed, noisy credit card data to unknown daily sales. Using the quarterly numbers and some extrapolation, it estimates the fraction of total sales the credit card data likely represents.\n","date":"16 February 2023","permalink":"/posts/model-beats-wall-street-analysts-in-forecasting-bu/","section":"Posts","summary":"Model-beats-Wall-Street-analysts-in-forecasting-bu # Combining alternative data with more traditional but infrequent ground-truth financial data — such as quarterly earnings, press releases, and stock prices — can paint a clearer picture of a company’s financial health on even a daily or weekly basis.","title":"Model-beats-Wall-Street-analysts-in-forecasting-bu"},{"content":"Modeling-subscription-revenue # These data points help the business understand the health of your subscriber base, specifically:\nThe health of your subscriber base even when you have seasonality in renewals (since you amortize your revenue over months) The source of revenue changes: new customers, upgrades, downgrades, churns, or reactivations The value of a customer based on how long a given user keeps paying you money: customer lifetime value, average contract value Frequently, this is called Monthly Recurring Revenue (MRR) analysis, with the output looking something like this: ! As a result, the queries in our dashboard are really simple – here’s one to calculate the number of customers and total MRR for each month: select date_month, sum(is_active::integer) as customers, sum(mrr) as mrr, from analytics.fct_mrr group by 1 Building the dashboard on top of a data model, rather than raw data, offers a number of benefits:\nYour business logic is codified: Every business is unique, and the way your business defines a churn is likely to be subtly different to another business. Untitled The change categories are worth going through here: new: the customer is a new customer that has not had a previous subscription churn: last month the customer paid for a subscription, but this month is not. A customer can churn many times upgrade: the customer has increased their usage and is now paying you more money per month downgrade: the customer has decreased their usage and is now paying you less money per month reactivation: a previously churned customer has started subscribing again If you\u0026rsquo;re ready to dive straight into a dbt project to see how this is done, check out our sample MRR project here. Step 2: Date spine subscriptions to have one record per customer per month # Required SQL technique: date spining | Required dbt technique: packages Since we want to look at the change month-by-month, we need to fan out our subscriptions to have one record per month, rather than one record per active subscription: Untitled Typically, we do this by joining to a table of “months”:\nselect months.date_month, subscriptions.customer_id, subscriptions.subscription_id, susbcriptions.monthly_revenue from subscriptions inner join months -- all months after start date on months.date_month \u0026gt;= customers.date_month_start -- and before end date and months.date_month \u0026lt;= customers.date_month_end We use the date_spine macro from the dbt-utils package to generate a table of all months.\nwith customers as ( select customer_id, date_trunc(\u0026#39;month\u0026#39;, min(start_date)) as date_month_start, date_trunc(\u0026#39;month\u0026#39;, max(end_date)) as date_month_end from subscription_periods group by 1 ), customer_months as ( select customers.customer_id, months.date_month from customers inner join months -- all months after start date on months.date_month \u0026gt;= customers.date_month_start -- and before end date and months.date_month \u0026lt; customers.date_month_end ), -- join the customer_months spine to MRR base model joined as ( select customer_months.date_month, customer_months.customer_id, coalesce(subscription_periods.monthly_amount, 0) as mrr from customer_months left join subscription_periods on customer_months.customer_id = subscription_periods.customer_id -- month is after a subscription start date and customer_months.date_month \u0026gt;= subscription_periods.start_date -- month is before a subscription end date and customer_months.date_month \u0026lt; subscription_periods.end_date ) ... Step 3: Identify the first- and last-active months for a customer # Required SQL technique: Window functions Now that we have all the months for a customer, we can start building some fields to help us categorize the changes.\n... final as ( select date_month, customer_id, mrr, mrr \u0026gt; 0 as is_active, -- calculate first and last months min(case when is_active then date_month end) over ( partition by account_id ) as first_active_month, max(case when is_active then date_month end) over ( partition by account_id ) as last_active_month, -- calculate if this record is the first or last month first_active_month = date_month as is_first_month, last_active_month = date_month as is_last_month from joined ) select * from final Step 4: Generate a “churn” month # Our customer’s last subscription finished in July, so they should get marked as a churn in August.\n","date":"16 February 2023","permalink":"/posts/modeling-subscription-revenue/","section":"Posts","summary":"Modeling-subscription-revenue # These data points help the business understand the health of your subscriber base, specifically:","title":"Modeling-subscription-revenue"},{"content":"numerical-linear-algebra-README-md-at-master-fas # Course Logistics](https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/0.%20Course%20Logistics.ipynb) (Video 1)\nMy background Teaching Approach Importance of Technical Writing List of Excellent Technical Blogs Linear Algebra Review Resources [1. # Matrix and Tensor Products Matrix Decompositions Accuracy Memory use Speed Parallelization \u0026amp; Vectorization [2. # Topic Frequency-Inverse Document Frequency (TF-IDF) Singular Value Decomposition (SVD) Non-negative Matrix Factorization (NMF) Stochastic Gradient Descent (SGD) Intro to PyTorch Truncated SVD [3. # Load and View Video Data SVD Principal Component Analysis (PCA) L1 Norm Induces Sparsity Robust PCA LU factorization Stability of LU LU factorization with Pivoting History of Gaussian Elimination Block Matrix Multiplication [4. # Predicting Health Outcomes with Linear Regressions](http://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/5.%20Health%20Outcomes%20with%20Linear%20Regression.ipynb) (Video 8)\nLinear regression in sklearn Polynomial Features Speeding up with Numba Regularization and Noise [6. # Naive solution Normal equations and Cholesky factorization QR factorization SVD Timing Comparison Conditioning \u0026amp; Stability Full vs Reduced Factorizations Matrix Inversion is Unstable [7. # SVD DBpedia Dataset Power Method QR Algorithm Two-phase approach to finding eigenvalues Arnoldi Iteration [8. # ","date":"16 February 2023","permalink":"/posts/numerical-linear-algebra-readme-md-at-master-fas/","section":"Posts","summary":"numerical-linear-algebra-README-md-at-master-fas # Course Logistics](https://nbviewer.","title":"numerical-linear-algebra-README-md-at-master-fas"},{"content":"NVCA-medium-article # 1*ZJQ8rax1gAVjYGucTl46IA.jpeg\nOnly by returning to its roots can the VC industry thrive in the years ahead # Though I’ve worked in venture capital for the past four years, I still identify as an outsider. Many investors seem to think venture capital began with a philosophy rooted in the belief that operators (that is,those who have been founders or early employees of a venture-backed startup) know how to build companies and can help you build yours, too. Venture pioneers like Arthur Rock were essential to the building of the companies in which they invested, but Rock wasn’t an operator (as it is thought of today) before becoming a venture capitalist. Venture capital today differs so dramatically from 50 years ago not just in the value-add services we now see at so many firms, such as a16z’s services-on-steroids portfolio, which has focused largely on industries like enterprise software, cryptocurrency, and online marketplaces. In venture capital today, we now have more than double the number of funds compared to what we had just 10 years ago, and nearly 40% of all venture funding going into one thing for each of the past 10 years: software. Source: PitchBook Considering it takes about six years to get a sense of returns, industry averages seem to be going in the wrong direction as measured by DPI (Distribution to Paid in Capital), which is a multiple that represents the limited partners’ return on their investment with a venture fund. Jim Gaither said of Genentech about the time it was getting started that “Nobody had a clue whether they could pull that off, but if they could, it’d be big.”Whatever a firm’s purpose, we need to hear these words more often in the halls of venture firms today, and by “big” it should mean something that matters to human potential and planetary health.\n","date":"16 February 2023","permalink":"/posts/nvca-medium-article/","section":"Posts","summary":"NVCA-medium-article # 1*ZJQ8rax1gAVjYGucTl46IA.","title":"NVCA-medium-article"},{"content":"Objectified-Gary-Hustwit # Objectified — Gary Hustwit # Created: June 7, 2020 4:22 PM URL: https://www.hustwit.com/objectified\n","date":"16 February 2023","permalink":"/posts/objectified-gary-hustwit/","section":"Posts","summary":"Objectified-Gary-Hustwit # Objectified — Gary Hustwit # Created: June 7, 2020 4:22 PM URL: https://www.","title":"Objectified-Gary-Hustwit"},{"content":"Odds-Algorithm # Odds Algorithm # Created: December 7, 2019 10:57 PM Tags: Data, Statistics URL: https://en.m.wikipedia.org/wiki/Odds_algorithm The odds-algorithm is a mathematical method for computing optimal strategies for a class of problems that belong to the domain of optimal stopping problems. Formally, the objective in these problems is to maximize the probability of identifying in a sequence of sequentially observed independent events the last event satisfying a specific criterion (a \u0026ldquo;specific event\u0026rdquo;). At the same time it computes The output is The odds-strategy is the rule to observe the events one after the other and to stop on the first interesting event from index s onwards (if any), where s is the stopping threshold of output a. Applications reach from medical questions in clinical trials over sales problems, secretary problems, portfolio selection, (one-way) search strategies, trajectory problems and the parking problem to problems in on-line maintenance and others. Matsui \u0026amp; Ano 2017 discussed a problem of selecting out of the last successes and obtained a tight lower bound of win probability. A problem discussed by Tamaki 2010 is obtained by setting multiple choice problem: A player is allowed choices, and he wins if any choice is the last success. When , Ano, Kakinuma \u0026amp; Miyoshi 2010 showed that the tight lower bound of win probability is equal to For general positive integer , Matsui \u0026amp; Ano 2016 discussed the tight lower bound of win probability.\n","date":"16 February 2023","permalink":"/posts/odds-algorithm/","section":"Posts","summary":"Odds-Algorithm # Odds Algorithm # Created: December 7, 2019 10:57 PM Tags: Data, Statistics URL: https://en.","title":"Odds-Algorithm"},{"content":"Official-Ghost-GitHub-Integration # Official Ghost + GitHub Integration # Created: April 23, 2020 2:28 PM URL: https://ghost.org/integrations/github/ Set up simple continuous integration of your Ghost theme to deploy directly to your Ghost website with GitHub Actions.\nUse GitHub Actions to deploy your theme # GitHub Actions allow you to build simple automation on top of any repository, including running a build command on a theme and pushing the compiled zipfile to the Ghost Admin API. Official%20Ghost%20+%20GitHub%20Integration%205b27867598de4daa9a3b11611b3d6958/image-1.png\nSet your Ghost integration credentials in GitHub # Next, copy and paste your integration details into your GitHub repository\u0026rsquo;s environment variables. Copy and paste the following code into a new file in your repository under .github/workflows/main.yml - this will automatically use the official Ghost GitHub Action from GitHub\u0026rsquo;s Marketplace:\nname: Deploy Theme on: push: branches: - master jobs: deploy: runs-on: ubuntu-18.04 steps: - uses: actions/checkout@master - uses: TryGhost/action-deploy-theme@v1.0.0 with: api-url: ${{ secrets.GHOST_ADMIN_API_URL }} api-key: ${{ secrets.GHOST_ADMIN_API_KEY }} Now, every time you push changes to your theme repository, your theme will automatically build and deploy to Ghost Admin.\nUse Github Gists to share code snippets # GitHub Gists are a simple way of sharing code snippets with other developers. Official%20Ghost%20+%20GitHub%20Integration%205b27867598de4daa9a3b11611b3d6958/Creating-a-Gist.png To share a Gist in Ghost, locate the embed option from the dropdown in the top navigation and copy the HTML embed code to your clipboard: ! Official%20Ghost%20+%20GitHub%20Integration%205b27867598de4daa9a3b11611b3d6958/Gist-embed-code-1.png\nPaste it into a HTML card in the editor # Create a new HTML block in the Ghost editor on the post you would like to embed your code snippet and paste in the embed code.\n","date":"16 February 2023","permalink":"/posts/official-ghost-github-integration/","section":"Posts","summary":"Official-Ghost-GitHub-Integration # Official Ghost + GitHub Integration # Created: April 23, 2020 2:28 PM URL: https://ghost.","title":"Official-Ghost-GitHub-Integration"},{"content":"Old-CSS-new-CSS-fuzzy-notepad # Old CSS, new CSS / fuzzy notepad # Created: February 2, 2020 8:00 AM URL: https://eev.ee/blog/2020/02/01/old-css-new-css/ I first got into web design/development in the late 90s, and only as I type this sentence do I realize how long ago that was. I’ve been taking for granted that most folks doing web stuff still remember those days, or at least the decade that followed, but I think that assumption might be a wee bit out of date. Here’s a history of CSS and web design, as I remember it. You may want to try the W3C’s history of CSS, which is considerably shorter, has a better chance of matching reality, and contains significantly less swearing.) My favorite artifact of this era is the book that taught me HTML: O’Reilly’s HTML: The Definitive Guide, published in several editions in the mid to late 90s. I don’t have it any more and can’t readily find screenshots online, but here’s a page from HTML \u0026amp; XHTML: The Definitive Guide, which seems to be a revision (I’ll get to XHTML later) with much the same style. Partly because this is a guidebook introducing concepts one at a time; partly because the book was printed in black and white; and partly, I’m sure, because it reflected the reality that coloring anything was a huge pain in the ass.\n","date":"16 February 2023","permalink":"/posts/old-css-new-css-fuzzy-notepad/","section":"Posts","summary":"Old-CSS-new-CSS-fuzzy-notepad # Old CSS, new CSS / fuzzy notepad # Created: February 2, 2020 8:00 AM URL: https://eev.","title":"Old-CSS-new-CSS-fuzzy-notepad"},{"content":"OMNIEQ-Real-Time-High-Probability-Credit-Spread-Sc # OMNIEQ | Real-Time High Probability Credit Spread Scanner # Created: January 21, 2020 12:01 PM Tags: Finance URL: https://omnieq.com/\n","date":"16 February 2023","permalink":"/posts/omnieq-real-time-high-probability-credit-spread-sc/","section":"Posts","summary":"OMNIEQ-Real-Time-High-Probability-Credit-Spread-Sc # OMNIEQ | Real-Time High Probability Credit Spread Scanner # Created: January 21, 2020 12:01 PM Tags: Finance URL: https://omnieq.","title":"OMNIEQ-Real-Time-High-Probability-Credit-Spread-Sc"},{"content":"One-Page-Calendar-2020 # One Page Calendar 2020 # Created: March 5, 2020 8:11 AM URL: https://davebakker.io/onepagecalendar/ Untitled\n","date":"16 February 2023","permalink":"/posts/one-page-calendar-2020/","section":"Posts","summary":"One-Page-Calendar-2020 # One Page Calendar 2020 # Created: March 5, 2020 8:11 AM URL: https://davebakker.","title":"One-Page-Calendar-2020"},{"content":"OODA-loop-Wikipedia # OODA%20loop%20-%20Wikipedia%207586e3c43c104f478b528198e05ac5f3/500px-OODA.Boyd.svg.png Diagram of the OODA loop The OODA loop is the cycle observe–orient–decide–act, developed by military strategist and United States Air Force Colonel John Boyd. It is especially applicable to cyber security and cyberwarfare [1]\nOverview[edit] # The OODA loop has become an important concept in litigation,[2] business,[3] law enforcement,[4] and military strategy. As one of Boyd\u0026rsquo;s colleagues, Harry Hillaker, put it in \u0026ldquo;John Boyd, USAF Retired, Father of the F16\u0026rdquo;:\nThe key is to obscure your intentions and make them unpredictable to your opponent while you simultaneously clarify his intentions.\nThe OODA loop also serves to explain the nature of surprise and shaping operations in a way that unifies Gestalt psychology, cognitive science and game theory in a comprehensive theory of strategy.\nSee also[edit] # Control theory Decision cycle Double-loop learning Improvement cycle DMAIC PDCA Learning cycle Maneuver warfare Mental model Nursing process Problem solving Situation awareness SWOT analysis Notes[edit] # [edit] # Boyd, John R. (3 September 1976). Strategy, Planning \u0026amp; Litigating to Win Greene, Robert, OODA and You Hillaker, Harry, , \u0026ldquo;John Boyd, USAF Retired, Father of the F16\u0026rdquo;, July 1997. Code one magazine Linger, Henry, , p. 449 Constructing The Infrastructure for the Knowledge Economy: Methods and Tools, Theory and Practice Metayer, Estelle, Decision making: It\u0026rsquo;s all about taking off – and landing safely…, Competia, December 2011 Osinga, Frans, \u0026ldquo;Science, Strategy and War The Strategic Theory of John Boyd\u0026rdquo; Richards, Chet, (2004) ISBN 1-4134-5377-5 Certain to Win: the Strategy of John Boyd, Applied to Business Ullman, David G., “OO-OO-OO!” The Sound of a Broken OODA Loop, Crosstalk, April 2007 External links[edit] # Archived documents Video: The OODA Loop and Clausewitzian \u0026ldquo;Friction\u0026rdquo; Bazin, A. ","date":"16 February 2023","permalink":"/posts/ooda-loop-wikipedia/","section":"Posts","summary":"OODA-loop-Wikipedia # OODA%20loop%20-%20Wikipedia%207586e3c43c104f478b528198e05ac5f3/500px-OODA.","title":"OODA-loop-Wikipedia"},{"content":"Optimal-stopping-Wikipedia # A sequence of \u0026lsquo;reward\u0026rsquo; functions which depend on the observed values of the random variables in 1.: Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/c347dcdade06ab59406dacf0d929e03855856ee8 Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/4f93e8400461ac4d620680baee52030fa89911db Given those objects, the problem is as follows:\nYou are observing the sequence of random variables, and at each step , you can choose to either stop observing or continue Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/add78d8608ad86e54951b8c8bd6c8d8416533d20 If you stop observing at step , you will receive reward Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/67d30d30b6c2dbe4d6f150d699de040937ecc95f You want to choose a stopping rule to maximize your expected reward (or equivalently, minimize your expected loss) Continuous time case[edit] # Consider a gain processes Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/3145251bd7dcd62f06889457914d47d54447646a defined on a filtered probability space Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/5e3f1b6d200f2bc4fd12f17fcd4b9547da96ce09 and assume that Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/f5f3c8921a3b352de45446a6789b104458c9f90b is adapted to the filtration. The optimal stopping problem is to find the stopping time Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/0b4ac981f3c6efc49fbcb3ecd24f7bf152dad0a7 which maximizes the expected gain Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/c4da65227df8165056ee82f640793d8e4b37908f where Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/57a433d75842b2d6a28cd5f8ca9cf7dba459084f is called the value function. We consider an adapted strong Markov process Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/478bcaa73ef8daeb8bd07701b59c6384b689f131 defined on a filtered probability space Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/becca0fa5b0e6527db1e25d78299511b5320edbb where Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/03c8fe9e48980d22020c362b11762a216f8bee58 denotes the probability measure where the stochastic process starts at Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/87f9e315fd7e2ba406057a97300593c4802b53e4 .\nA jump diffusion result[edit] # Let Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/95734a78eb8407939c3496cbfd92763ced1e41e1 be a Lévy diffusion in Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/1bcd8908c9fa46eb979ef7b67d1bb65eb3692cbb given by the SDE Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/264bc8d76ca788b3eff6e45fa24b76c3201aba60 where Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/47136aad860d145f75f3eed3022df827cee94d7a is an Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/0a07d98bb302f3856cbabc47b2b9016692e3f7bc -dimensional Brownian motion, Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/b49f9e15c90b97d6d95aaf6bd1a4f520d66c2bb7 is an Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/829091f745070b9eb97a80244129025440a1cfac -dimensional compensated Poisson random measure, Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/aae4bec0dfe664f70a1b9cda15fd319fa1e454eb , Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/165b2ac51764fbee3ed5db71d915b53420333832 , and Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/454e9f9964b0205f0e19d54a5e902038bc1e095f are given functions such that a unique solution Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/5446d2e710df1848b39d3474304fa84dbdc60a05 exists. The optimal stopping problem is: Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/96e90fc8d59f61857be4ba95aff689714bfc5761 It turns out that under some regularity conditions,[5] the following verification theorem holds: If a function Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/7d9dd8e4893e28a7f6eabb88b72d49efc8ddeb39 satisfies\nwhere the continuation region is , Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/8a238ecdc084108386647a9f4928c99d54af39a4 Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/9b3f5a2a4a0459b28eb40706f67ea48f83d35b78 on , and Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/3a8073395c6ee55e9f384471412c9d453ced655f Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/2302a18e269dbecc43c57c0c2aced3bfae15278d on , where is the infinitesimal generator of Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/b4b891a2ffc1861ecef13412bb7c69dd7e794891 Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/bc33a22fb3a9e91084b653ef5e58815ff05aba06 Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/280ae03440942ab348c2ca9b8db6b56ffa9618f8 then Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/812e58cf6049240099f528ebf2c4b403f7a9ebc7 for all Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/45c9dab32dcebce045fc69264dc531a98b9bc6c9 . Moreover, if on Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/ab904cee10099523faefb28ada29590acb97c578 Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/f34a0c600395e5d4345287e21fb26efd386990e6 Then Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/76c271acf29d6893d8a17d35018cc7d8d840ac50 for all and Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/72be0ff341a0ca9b991ed0249f29a229b223903f is an optimal stopping time. The solution is known to be[7] (Perpetual call) where and Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/62363453bd2df75ecda55d5ef3dba9d954f679a5 Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/0b5c45a5b03cdca86ea8deb1ec6e2c10ed35d099 Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/fe2ccc20c0bf5cf1d671556648d75d76656fca3d (Perpetual put) where and Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/e77d9317d3a58cf30457e68bf232480b6afc4a4f Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/fb8e5648b16dcd2cd97fdd295d5ea25ee2224d52 Optimal%20stopping%20-%20Wikipedia%2036ca95b1506f4eae8314e9de4cd135fa/10134e66db0d440076b296492c842f996f485e14 On the other hand, when the expiry date is finite, the problem is associated with a 2-dimensional free-boundary problem with no known closed-form solution. ","date":"16 February 2023","permalink":"/posts/optimal-stopping-wikipedia/","section":"Posts","summary":"Optimal-stopping-Wikipedia # A sequence of \u0026lsquo;reward\u0026rsquo; functions which depend on the observed values of the random variables in 1.","title":"Optimal-stopping-Wikipedia"},{"content":"Optimization-Model-Basics-Optimization-Mathema # An Unconstrained optimization problem is an optimization problem where the objective function can be of any kind (linear or nonlinear) and there are no constraints. A linear program is an optimization problem with an objective function that is linear in the variables, and all constraints are also linear. A quadratic program is an optimization problem with an objective function that is quadratic in the variables (i.e. it may contain squares and cross products of the decision variables), and all constraints are linear. A nonlinear program is an optimization problem with an objective function that is an arbitrary nonlinear function of the decision variables, and the constraints can be linear or nonlinear. An optimization model\u0026rsquo;s variables can be accessed through its Variables property. For example, both linear and quadratic programs use variables of type LinearProgramVariable, which inherits from DecisionVariable and has an extra Cost property that represents the coefficient of the variable in the linear portion of the objective function. An optimization model\u0026rsquo;s constraints can be accessed through its Constraints property.\n","date":"16 February 2023","permalink":"/posts/optimization-model-basics-optimization-mathema/","section":"Posts","summary":"Optimization-Model-Basics-Optimization-Mathema # An Unconstrained optimization problem is an optimization problem where the objective function can be of any kind (linear or nonlinear) and there are no constraints.","title":"Optimization-Model-Basics-Optimization-Mathema"},{"content":"Options-v-cash # Cynical reasons # One possible answer, perhaps the simplest possible answer, is that options aren’t worth what startups claim they’re worth and startups prefer options because their lack of value is less obvious than it would be with cash. However, if the share price stays at $10 for the lifetime of the option, the options will end up being worth $0 because an option with a $10 strike price is an option to buy the stock at $10, which is not the same as a grant of actual shares worth $10 a piece. By the time the company was worth $1B, Mayhar’s share of the company was diluted by 8x, which made his share of the company worth less than $500k (minus the cost of exercising his options) instead of $4M (minus the cost of exercising his options).\nOptions are often free to the company # A large fraction of options get returned to the employee option pool when employees leave, either voluntarily or involuntarily.\nTax benefit of ISOs # In the U.S., Incentive stock options (ISOs) have the property that, if held for one year after the exercise date and two years after the grant date, the owner of the option pays long-term capital gains tax instead of ordinary income tax on the difference between the exercise price and the strike price. For nonstatutory options without a readily determinable fair market value, there\u0026rsquo;s no taxable event when the option is granted but you must include in income the fair market value of the stock received on exercise, less the amount paid, when you exercise the option. Conversely, the vast majority of startup option packages end up being worth little to nothing, but nearly none of the employees whose options end up being worthless were instrumental in causing their options to become worthless.\n","date":"16 February 2023","permalink":"/posts/options-v-cash/","section":"Posts","summary":"Options-v-cash # Cynical reasons # One possible answer, perhaps the simplest possible answer, is that options aren’t worth what startups claim they’re worth and startups prefer options because their lack of value is less obvious than it would be with cash.","title":"Options-v-cash"},{"content":"Orphaned-Kittens-Saving-the-Tiniest-Lives # Orphaned Kittens Saving the Tiniest Lives # Created: January 13, 2021 5:34 AM URL: https://www.maddiesfund.org/orphaned-kittens.htm\nAudience: Executive Leadership, Foster Caregivers, Public, Shelter/Rescue Staff \u0026amp; Volunteers, Veterinary Team # Orphaned kittens are the most fragile of homeless animals, and many shelters consider it too resource-intensive to care for them. put together a comprehensive kitten nursery program based on similar programs for orphaned wildlife. In conjunction with home-based foster care, the nursery program utilized existing resources in new ways, allowing them to save hundreds of kittens\u0026rsquo; lives and contributing to Austin becoming the largest no-kill city in America. Dr. Jefferson will walk you through nursery care and foster-based care, and Heidi will help foster caregivers with practical tips for orphaned kitten care in Orphaned Kittens: How Saving the Tiniest Lives has the Biggest Impact. Orphaned Kittens: How Saving the Tiniest Lives has the Biggest Impact is part of an ongoing series of educational programs from Maddie\u0026rsquo;s Institute, a program of Maddie\u0026rsquo;s Fund®, the nation\u0026rsquo;s leading funder of shelter medicine education. Maddie\u0026rsquo;s Institute brings cutting edge shelter medicine information from universities and animal welfare leaders to shelter veterinarians, managers and staff as well as private practice veterinarians, rescue groups and community members to increase the lifesaving of homeless dogs and cats community-wide. With 29 years as a practicing technician, and nearly 20 years of experience caring for orphaned kittens, Heidi has the passion and commitment to help these vulnerable babies.\n","date":"16 February 2023","permalink":"/posts/orphaned-kittens-saving-the-tiniest-lives/","section":"Posts","summary":"Orphaned-Kittens-Saving-the-Tiniest-Lives # Orphaned Kittens Saving the Tiniest Lives # Created: January 13, 2021 5:34 AM URL: https://www.","title":"Orphaned-Kittens-Saving-the-Tiniest-Lives"},{"content":"osmnx-examples-09-example-figure-ground-ipynb-at-m # osmnx-examples/09-example-figure-ground.ipynb at master · gboeing/osmnx-examples # Created: December 4, 2019 7:28 PM https://github.com/gboeing/osmnx-examples/blob/master/notebooks/09-example-figure-ground.ipynb 4977197\n","date":"16 February 2023","permalink":"/posts/osmnx-examples-09-example-figure-ground-ipynb-at-m/","section":"Posts","summary":"osmnx-examples-09-example-figure-ground-ipynb-at-m # osmnx-examples/09-example-figure-ground.","title":"osmnx-examples-09-example-figure-ground-ipynb-at-m"},{"content":"Outtake-Senior-Engineer-Resolving-Conflicts-Yo # [Outtake] Senior Engineer: Resolving Conflicts - YouTube # Created: June 15, 2020 12:55 AM URL: https://www.youtube.com/watch?v=FDoH15ylAeo https://www.youtube.com/watch?v=FDoH15ylAeo\n","date":"16 February 2023","permalink":"/posts/outtake-senior-engineer-resolving-conflicts-yo/","section":"Posts","summary":"Outtake-Senior-Engineer-Resolving-Conflicts-Yo # [Outtake] Senior Engineer: Resolving Conflicts - YouTube # Created: June 15, 2020 12:55 AM URL: https://www.","title":"Outtake-Senior-Engineer-Resolving-Conflicts-Yo"},{"content":"Overview-18F-Engineering-Hiring-Guide # Overview | 18F Engineering Hiring Guide # Created: March 29, 2020 8:53 AM URL: https://eng-hiring.18f.gov/ This guide covers the engineering selection process, from resume review to turning over a candidate to HR for an offer.\nStep-by-step process # The process surrounding the standard engineering hiring process varies depending on the hiring action used — but the standard process consists of: 1. The TTS Talent team schedules a phone screen with the candidate and a representative from the 18F Engineering Hiring team. The TTS Talent team schedules the candidate for interviews with members of the Interview team.\nThe 18F Engineering Hiring team, who perform resume reviews, phone screens, wrap-up interviews, and selection of qualified candidates for job offers. The 18F Engineering Hiring team collects volunteers for the Interview team and performs Interview training to prepare them to mitigate unconscious bias, among other things. You’ll find these events on the 18F Engineering calendar; here are links to the slide decks for each: Intro to Engineering Hiring (30 minutes) # An introductory session, covering the overall hiring process, how selection works, our standards, and the parts of hiring that are common for all roles.\n","date":"16 February 2023","permalink":"/posts/overview-18f-engineering-hiring-guide/","section":"Posts","summary":"Overview-18F-Engineering-Hiring-Guide # Overview | 18F Engineering Hiring Guide # Created: March 29, 2020 8:53 AM URL: https://eng-hiring.","title":"Overview-18F-Engineering-Hiring-Guide"},{"content":"Overview-SimPy-4-0-2-dev1-g2973dbe-documentation # Overview — SimPy 4.0.2.dev1+g2973dbe documentation # Created: June 19, 2020 11:21 AM URL: https://simpy.readthedocs.io/en/latest/ learn the basics of SimPy in just a couple of minutes for a complete overview SimPy is a process-based discrete-event simulation framework based on standard Python. SimPy also provides various types of shared resources to model limited capacity congestion points (like servers, checkout counters and tunnels). Simulations can be performed “as fast as possible”, in real time (wall clock time) or by manually stepping through the events. A short example simulating two clocks ticking in different time intervals looks like this:\n\u0026gt;\u0026gt;\u0026gt; import simpy \u0026gt;\u0026gt;\u0026gt; \u0026gt;\u0026gt;\u0026gt; def clock(env, name, tick): ... while True: ... print(name, env.now) ... yield env.timeout(tick) ... \u0026gt;\u0026gt;\u0026gt; env = simpy.Environment() \u0026gt;\u0026gt;\u0026gt; env.process(clock(env, \u0026#39;fast\u0026#39;, 0.5)) \u0026gt;\u0026gt;\u0026gt; env.process(clock(env, \u0026#39;slow\u0026#39;, 1)) \u0026gt;\u0026gt;\u0026gt; env.run(until=2) fast 0 slow 0 fast 0.5 slow 1 fast 1.0 fast 1.5 The documentation contains a tutorial, several guides explaining key concepts, a number of examples and the API reference. Simulation model developers are encouraged to share their SimPy modeling techniques with the SimPy community. Please post a message to the SimPy mailing list. There is an introductory talk that explains SimPy’s concepts and provides some examples: watch the video or get the slides.\n","date":"16 February 2023","permalink":"/posts/overview-simpy-4-0-2-dev1-g2973dbe-documentation/","section":"Posts","summary":"Overview-SimPy-4-0-2-dev1-g2973dbe-documentation # Overview — SimPy 4.","title":"Overview-SimPy-4-0-2-dev1-g2973dbe-documentation"},{"content":"OWL-S-Semantic-Markup-for-Web-Services # With OWL-S markup of services, the information necessary for Web service discovery could be specified as computer-interpretable semantic markup at the service Web sites, and a service registry or ontology-enhanced search engine could be used to locate the services automatically. For nontrivial services (those composed of several steps over time), this description may be used by a service-seeking agent in at least four different ways: (1) to perform a more in-depth analysis of whether the service meets its needs; (2) to compose service descriptions from multiple services to perform a specific task; (3) during the course of the service enactment, to coordinate the activities of the different participants; and (4) to monitor the execution of the service. Specifically, it specifies the inputs required by the service and the outputs generated; furthermore, since a service may require external conditions to be satisfied, and it has the effect of changing such conditions, the profile describes the preconditions required by the service and the expected effects that result from the execution of the service.\n4.1 Compiling a Profile: The Relation with Process Model # The Profile of a service provides a concise description of the service to a registry, but once the service has been selected the Profile is useless; rather, the client will use the Process Model to control the interaction with the service.\n4.2 Profile Properties # In the following we describe in detail the main parts of the profile model; we classify them into four sections: the first one (4.2.1) describes the properties that link the Service Profile class with the Service class and Process Model class; the second section (4.2.2) describes the form of contact information and the Description of the profile \u0026ndash; this is information usually intended for human consumption; in the third section (4.2.3), we discuss the functional representation in terms of IOPEs; finally, in the last section (4.2.4), we describe the attributes of the Profile. Specifically, OWL-S 1.1 defines a subclass of ServiceModel, Process, which draws upon well-established work in a variety of fields, including work in AI on standardizations of planning languages [6], work in programming languages and distributed systems [20,19], emerging standards in process modeling and workflow technology such as the NIST\u0026rsquo;s Process Specification Language (PSL) [22] and the Workflow Management Coalition effort (http://www.aiim.org/wfmc), work on modeling verb semantics and event structure [21], previous work on action-inspired Web service markup [18], work in AI on modeling complex actions [13], and work in agent communication languages [15,5]. OWL-S%20Semantic%20Markup%20for%20Web%20Services%20bd3854383c384f6bbadb130c2a33d256/Process-Model-1.1.gif # Top level of the process ontology Atomic processes correspond to the actions a service can perform by engaging it in a single interaction; composite processes correspond to actions that require multi-step protocols and/or multiple server actions; finally, simple processes provide an abstraction mechanism to provide multiple views of the same process.\n","date":"16 February 2023","permalink":"/posts/owl-s-semantic-markup-for-web-services/","section":"Posts","summary":"OWL-S-Semantic-Markup-for-Web-Services # With OWL-S markup of services, the information necessary for Web service discovery could be specified as computer-interpretable semantic markup at the service Web sites, and a service registry or ontology-enhanced search engine could be used to locate the services automatically.","title":"OWL-S-Semantic-Markup-for-Web-Services"},{"content":"Pandas-filtering # Pandas filtering # Created: January 21, 2020 6:43 PM Tags: Data URL: https://news.ycombinator.com/item?id=22106575 TileDB[1] offers an embedded SQL experience for python[2]. We use MariaDB built in embedded mode to allow running sql against TileDB Arrays. This can be combined with pandas.read_sql to load the results directly into a pandas array. I wrote this implementation for the embedded SQL, so happy to answer any questions on it. The embedded SQL in its early stages, but should be fully functional for any queries that MariaDB itself supports. TileDB and the embedded SQL are both open source, TileDB is MIT licensed and the embedded SQL is under GPL. [1] https://tiledb.com/ [2] https://docs.tiledb.com/developer/api-usage/embedded-sql (disclosure: TileDB, inc. employee)\n","date":"16 February 2023","permalink":"/posts/pandas-filtering/","section":"Posts","summary":"Pandas-filtering # Pandas filtering # Created: January 21, 2020 6:43 PM Tags: Data URL: https://news.","title":"Pandas-filtering"},{"content":"Parabola-Make-your-computer-work-for-you # Parabola%20-%20Make%20your%20computer%20work%20for%20you%20cacb394ba10143118160ca2654fb9f7e/5dcca6f130b4b1358b962493_broom_icon.png Build CRM Workflows ! Parabola%20-%20Make%20your%20computer%20work%20for%20you%20cacb394ba10143118160ca2654fb9f7e/5dcca6f11b8bff044837edc3_folder_icon.png Automate Ecommerce Tasks ! Parabola%20-%20Make%20your%20computer%20work%20for%20you%20cacb394ba10143118160ca2654fb9f7e/5dcca6f1137dcb07e023f99e_shirt_icon.png Send Metrics Reports ! Parabola%20-%20Make%20your%20computer%20work%20for%20you%20cacb394ba10143118160ca2654fb9f7e/5dcca6f15095d0924de2e994_mailbox_icon.png\nDescribe it in words, build it in Parabola # If you can explain your problem to another person, you can teach Parabola to do it for you. Parabola%20-%20Make%20your%20computer%20work%20for%20you%20cacb394ba10143118160ca2654fb9f7e/5db4cc9e72af1383f53d3b7f_screenshot-illustration.png ! Parabola%20-%20Make%20your%20computer%20work%20for%20you%20cacb394ba10143118160ca2654fb9f7e/5daa253e5197cf6c3cbde488_blue-robot-recieves-parachutes-compressor.png If your data is on the internet, you can use it in Parabola. Parabola%20-%20Make%20your%20computer%20work%20for%20you%20cacb394ba10143118160ca2654fb9f7e/5d9f730b50ffcb3f6855a25e_number-two-2.png Drag and drop our ready-made steps to tell Parabola how to process your data and what to do with the output.\n","date":"16 February 2023","permalink":"/posts/parabola-make-your-computer-work-for-you/","section":"Posts","summary":"Parabola-Make-your-computer-work-for-you # Parabola%20-%20Make%20your%20computer%20work%20for%20you%20cacb394ba10143118160ca2654fb9f7e/5dcca6f130b4b1358b962493_broom_icon.","title":"Parabola-Make-your-computer-work-for-you"},{"content":"","date":"16 February 2023","permalink":"/","section":"patricktrainer","summary":"","title":"patricktrainer"},{"content":"PDF-RFM-and-CLV-Using-iso-value-curves-for-custo # (PDF) RFM and CLV: Using iso-value curves for customer base analysis # Created: January 30, 2020 12:00 PM URL: https://www.researchgate.net/publication/228351678_RFM_and_CLV_Using_iso-value_curves_for_customer_base_analysis\n","date":"16 February 2023","permalink":"/posts/pdf-rfm-and-clv-using-iso-value-curves-for-custo/","section":"Posts","summary":"PDF-RFM-and-CLV-Using-iso-value-curves-for-custo # (PDF) RFM and CLV: Using iso-value curves for customer base analysis # Created: January 30, 2020 12:00 PM URL: https://www.","title":"PDF-RFM-and-CLV-Using-iso-value-curves-for-custo"},{"content":"Picture-hanging-knot-for-a-picture-frame-UK-Pict # Picture hanging knot (for a picture frame) | UK Picture Framing Supplies Blog # Created: May 22, 2020 4:17 PM URL: http://blog.ukpictureframingsupplies.co.uk/picture-framing-information/picture-framing-a-picture-cord-knot/ A commonly asked question is what picture cord knot works best for hanging a picture frame? The following are some desirable features of a picture knot –\nWon’t untie (self tightening) Won’t sag (so you can hang several pictures simultaneously in a line) Will look neat and tidy (for a professional finish) Won’t fray at the ends Sagging can be minimised by using low stretch picture cord, you can then decide on the choice of knot. http://blog.ukpictureframingsupplies.co.uk/wp-content/uploads/2017/08/How_To_Tie_A_Picture_Framing_Knot_1.jpg http://blog.ukpictureframingsupplies.co.uk/wp-content/uploads/2017/08/How_To_Tie_A_Picture_Framing_Knot_2.jpg 4. http://blog.ukpictureframingsupplies.co.uk/wp-content/uploads/2017/08/How_To_Tie_A_Picture_Framing_Knot_3.jpg 5. http://blog.ukpictureframingsupplies.co.uk/wp-content/uploads/2017/08/How_To_Tie_A_Picture_Framing_Knot_4.jpg 6. http://blog.ukpictureframingsupplies.co.uk/wp-content/uploads/2017/08/How_To_Tie_A_Picture_Framing_Knot_5a.jpg 7.\n","date":"16 February 2023","permalink":"/posts/picture-hanging-knot-for-a-picture-frame-uk-pict/","section":"Posts","summary":"Picture-hanging-knot-for-a-picture-frame-UK-Pict # Picture hanging knot (for a picture frame) | UK Picture Framing Supplies Blog # Created: May 22, 2020 4:17 PM URL: http://blog.","title":"Picture-hanging-knot-for-a-picture-frame-UK-Pict"},{"content":"Pmarchive-Home # Pmarchive - Home # Created: February 21, 2020 12:04 PM URL: https://pmarchive.com/\nThe Pmarca Guide to Startups # Other Startup Essentials # The Psychology of Entrepreneurial Misjudgment, part 1: Biases 1-6\nAdditional Pmarca Guides # Assorted Goodness # The Long Kiss Goodbye # Before he stopped posting, Marc tantalized readers with a \u0026ldquo;Coming Soon\u0026rdquo; list (reprinted below). I was particularly excited about the Guide to High-Tech Startups. Maybe someday. All we can do is hope.\nTop 10 books for high-tech entrepreneurs Top 10 ways to do personal outsourcing Software \u0026ndash; the velvet revolution and the multicore conundrum How to trick out a Typepad blog in 2007 Killer Windows Media Center apps for 2007 The truth about reporters: a multi-part series The Pmarca Guide to High-Tech Startups: a multi-part series Why Internet advertising is about to get humongous ","date":"16 February 2023","permalink":"/posts/pmarchive-home/","section":"Posts","summary":"Pmarchive-Home # Pmarchive - Home # Created: February 21, 2020 12:04 PM URL: https://pmarchive.","title":"Pmarchive-Home"},{"content":"Pollinator-Friendly-Native-Plant-Lists-Xerces-Soci # Pollinator-Friendly Native Plant Lists | Xerces Society # Created: March 31, 2020 7:50 AM URL: https://xerces.org/pollinator-conservation/pollinator-friendly-plant-lists ! Pollinator-Friendly%20Native%20Plant%20Lists%20Xerces%20Soci%208ae94bc893034a0cb5512b284393efa1/poll-plant-lists-baller.jpg (Photo: Xerces Society / Jennifer Hopwood) We\u0026rsquo;ve prepared the following lists of recommended native plants that are highly attractive to pollinators such as native bees, honey bees, butterflies, moths, and hummingbirds, and are well-suited for small-scale plantings in gardens, on business and school campuses, in urban greenspaces, and in farm field borders. These lists are not intended to be exhaustive, but reflect a \u0026ldquo;best-of\u0026rdquo; approach identifying plants known from various trials and research efforts to provide a number of benefits to a diversity of pollinators, are regionally appropriate, and highly adaptable to a range of growing conditions. For more plant suggestions, see all Plant Lists in our Publications Library, or check out our books 100 Plants to Feed the Bees and Gardening for Butterflies. Use our Pollinator Conservation Resource Center for everything from plant lists to habitat establishment guides.\n","date":"16 February 2023","permalink":"/posts/pollinator-friendly-native-plant-lists-xerces-soci/","section":"Posts","summary":"Pollinator-Friendly-Native-Plant-Lists-Xerces-Soci # Pollinator-Friendly Native Plant Lists | Xerces Society # Created: March 31, 2020 7:50 AM URL: https://xerces.","title":"Pollinator-Friendly-Native-Plant-Lists-Xerces-Soci"},{"content":"Porn-the-Low-Slung-Engine-of-Progress-The-New-Y # The erotic technological impulse dates back at least to some of the earliest works of art, the so-called Venus figurines of women with exaggerated breasts and buttocks, which were made by firing clay 27,000 years ago \u0026ndash; 15 millenniums before ceramics technology was used for anything utilitarian like pots. Government regulation kept sex off radio and television airwaves, but eventually pornographers helped establish new audio and visual technologies for getting into the home. The feisty academic Camille Paglia has argued in her book \u0026ldquo;Sexual Personae\u0026rdquo; that developments in art and technology are related to the male sex drive: \u0026ldquo;Phallic aggression and projection are intrinsic to Western conceptualization. \u0026quot; Lynn Hunt, the editor of a recent book of essays, \u0026ldquo;The Invention of Pornography: Obscenity and the Origins of Modernity, 1500-1800,\u0026rdquo; suggests another way of looking at the technology-pornography link: Not only do some men see technology in sexual terms, they see sex in technological terms. \u0026ldquo;Pornography attaches itself to a new technological medium partly because it\u0026rsquo;s a genre that\u0026rsquo;s very interested in technological means,\u0026rdquo; said Dr. Hunt, a professor of history at the University of Pennsylvania. The Cyberfuture Virtual Sex, Virtually Alone If sex is always at the technological frontier, then today\u0026rsquo;s true pioneers of communications are to be found at Interotica, the CD-ROM company responsible for \u0026ldquo;The Interactive Adventures of Seymore Butts.\u0026rdquo; The most visible is Lisa Palac, editor of what is billed as \u0026ldquo;the only erotic magazine for women and men that combines the two most popular and powerful subjects of our time: sex and technology.\u0026rdquo;\n","date":"16 February 2023","permalink":"/posts/porn-the-low-slung-engine-of-progress-the-new-y/","section":"Posts","summary":"Porn-the-Low-Slung-Engine-of-Progress-The-New-Y # The erotic technological impulse dates back at least to some of the earliest works of art, the so-called Venus figurines of women with exaggerated breasts and buttocks, which were made by firing clay 27,000 years ago \u0026ndash; 15 millenniums before ceramics technology was used for anything utilitarian like pots.","title":"Porn-the-Low-Slung-Engine-of-Progress-The-New-Y"},{"content":"PostgreSQL-command-line-cheatsheet # PostgreSQL command line cheatsheet # Created: April 28, 2020 10:31 AM Tags: Cheat sheet, How To, SQL, Tips URL: https://gist.github.com/Kartones/dd3ff5ec5ea238d4c546 https://gist.github.com/Kartones/dd3ff5ec5ea238d4c546\n","date":"16 February 2023","permalink":"/posts/postgresql-command-line-cheatsheet/","section":"Posts","summary":"PostgreSQL-command-line-cheatsheet # PostgreSQL command line cheatsheet # Created: April 28, 2020 10:31 AM Tags: Cheat sheet, How To, SQL, Tips URL: https://gist.","title":"PostgreSQL-command-line-cheatsheet"},{"content":"","date":"16 February 2023","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"Practical-Python-Programming-practical-python # Practical Python Programming | practical-python # Created: May 29, 2020 8:21 PM URL: https://dabeaz-course.github.io/practical-python/Notes/Contents.html\nTable of Contents # ","date":"16 February 2023","permalink":"/posts/practical-python-programming-practical-python/","section":"Posts","summary":"Practical-Python-Programming-practical-python # Practical Python Programming | practical-python # Created: May 29, 2020 8:21 PM URL: https://dabeaz-course.","title":"Practical-Python-Programming-practical-python"},{"content":"Preface-GeoGebra # Preface – GeoGebra # Created: February 21, 2020 12:05 PM URL: https://www.geogebra.org/m/x39ys4d7#material/phuyhqtw\n","date":"16 February 2023","permalink":"/posts/preface-geogebra/","section":"Posts","summary":"Preface-GeoGebra # Preface – GeoGebra # Created: February 21, 2020 12:05 PM URL: https://www.","title":"Preface-GeoGebra"},{"content":"Principal-Component-Analysis-explained-visually # Principal Component Analysis explained visually # Created: July 13, 2020 9:22 AM URL: https://setosa.io/ev/principal-component-analysis/ ! fb-thumb.png\nExplained Visually # with text by Lewis Lehe Principal component analysis (PCA) is a technique used to emphasize variation and bring out strong patterns in a dataset. First, consider a dataset in only two dimensions, like (height, weight). This dataset can be plotted as points in a plane. But if we want to tease out variation, PCA finds a new coordinate system in which every point has a new (x,y) value. The axes don\u0026rsquo;t actually mean anything physical; they\u0026rsquo;re combinations of height and weight called \u0026ldquo;principal components\u0026rdquo; that are chosen to give one axes lots of variation. Drag the points around in the following visualization to see PC coordinate system adjusts.\n","date":"16 February 2023","permalink":"/posts/principal-component-analysis-explained-visually/","section":"Posts","summary":"Principal-Component-Analysis-explained-visually # Principal Component Analysis explained visually # Created: July 13, 2020 9:22 AM URL: https://setosa.","title":"Principal-Component-Analysis-explained-visually"},{"content":"Problem-Solving-with-Algorithms-and-Data-Structure # Problem Solving with Algorithms and Data Structures using Python — Problem Solving with Algorithms and Data Structures # Created: May 20, 2020 11:23 PM URL: https://runestone.academy/runestone/books/published/pythonds/index.html ! Problem%20Solving%20with%20Algorithms%20and%20Data%20Structure%20eff08db16ebf476e8c804a62c32800da/PythonDScover.jpg By Brad Miller and David Ranum, Luther College\nAssignments There is a wonderful collection of YouTube videos recorded by Gerry Jenkins to support all of the chapters in this text. Analysis](https://runestone.academy/runestone/books/published/pythonds/AlgorithmAnalysis/toctree.html) 4. Recursion 6. Trees and Tree Algorithms 8. Graphs and Graph Algorithms Acknowledgements # We are very grateful to Franklin Beedle Publishers for allowing us to make this interactive textbook freely available. This online version is dedicated to the memory of our first editor, Jim Leisy, who wanted us to “change the world.”\nIndices and tables # ","date":"16 February 2023","permalink":"/posts/problem-solving-with-algorithms-and-data-structure/","section":"Posts","summary":"Problem-Solving-with-Algorithms-and-Data-Structure # Problem Solving with Algorithms and Data Structures using Python — Problem Solving with Algorithms and Data Structures # Created: May 20, 2020 11:23 PM URL: https://runestone.","title":"Problem-Solving-with-Algorithms-and-Data-Structure"},{"content":"Product-Overview-Materialize # All of the interactive queryable capabilities of a traditional SQL data warehouse, but connecting directly to your streaming data sources, and staying up-to-date within milliseconds. Materialize delivers SQL exploration for streaming events and real-time data. Product%20Overview%20%E2%80%93%20Materialize%20cf57caaf7a794979bf60da5471332f90/rest-of-SQL-01-1250.png Many streaming solutions require the denormalization of data in order to maintain low latency - thereby prohibiting any kind of data enrichment like joins. Product%20Overview%20%E2%80%93%20Materialize%20cf57caaf7a794979bf60da5471332f90/connect-to-kafka-01-1250.png\nConnects directly to event stream processors # Materialize connects directly to your streaming infrastructure, ingesting streams of data from event stream processors like Kafka. Product%20Overview%20%E2%80%93%20Materialize%20cf57caaf7a794979bf60da5471332f90/postgres-01-1250.png\nPresents as Postgres downstream # Materialize is wire compatible with PostgreSQL, presenting to downstream tools like any Postgres database, simplifying the development of custom applications and streamlining the process of connecting existing data analysis tools.\nBuild views on views on views - or export back to your event stream processor # Transformations on streaming data can be used to feed into other transformations of that data - say joining several sources, then further querying that information. Product%20Overview%20%E2%80%93%20Materialize%20cf57caaf7a794979bf60da5471332f90/read-replica-01-1250.png\nUse Materialize as an Optimized Read Replica Database # Traditional read replica databases are optimized for transactional loads – but are usually suboptimal for analytics or visualization.\n","date":"16 February 2023","permalink":"/posts/product-overview-materialize/","section":"Posts","summary":"Product-Overview-Materialize # All of the interactive queryable capabilities of a traditional SQL data warehouse, but connecting directly to your streaming data sources, and staying up-to-date within milliseconds.","title":"Product-Overview-Materialize"},{"content":"Programming-Rules-of-Thumb # Programming Rules of Thumb # Created: December 4, 2019 6:22 PM Tags: Programming URL: https://holub.com/goodies/rules.html The following list is essentially the table of contents for my book Enough Rope to Shoot Yourself in the Foot (McGraw-Hill, 1995). The book was written with C/C++ in mind, but most of the rules apply to programming in general and OO programming in other languages (such as Java) in particular. 1 The essentials of programming: No surprises, minimize coupling, and maximize cohesion 2 Stamp out the demons of complexity (Part 1) 2.1 Don\u0026rsquo;t solve problems that don\u0026rsquo;t exist 2.2 Solve the specific problem, not the general case 3 A user interface should not look like a computer program (the transparency principle) 4 Don\u0026rsquo;t confuse ease of learning with ease of use 5 Productivity can be measured in the number of keystrokes 6 If you can\u0026rsquo;t say it in English, you can\u0026rsquo;t say it in C/C++ 6.1 Do the comments first 7 Read code 7.1 There\u0026rsquo;s no room for prima donnas in a contemporary programming shop 8 Decompose complex problems into smaller tasks 9 Use the whole language (Use the appropriate tool for the job) 10 A problem must be thought through before it can be solved 11 Computer programming is a service industry 12 Involve users in the development process 13 The customer is always right 14 Small is Beautiful. (Big == slow)\nGeneral Development Issues # 15 First, do no harm 16 Edit your code 17 A program must be written at least twice 18 You can\u0026rsquo;t measure productivity by volume 19 You can\u0026rsquo;t program in isolation 20 Goof off 21 Write code with maintenance in mind�the maintenance programmer is you 21.1 Efficiency is often a bugaboo\nFormatting and Documentation # 22 Uncommented code has no value 23 Put the code and the documentation in the same place 24 Comments should be sentences 25 Run your code through a spelling checker 26 A comment shouldn\u0026rsquo;t restate the obvious 27 A comment should provide only information needed for maintenance 28 Comments should be in blocks 29 Comments should align vertically 30 Use neat columns as much as possible 31 Don\u0026rsquo;t put comments between the function name and the open brace 32 Mark the ends of long compound statements with something reasonable 33 Put only one statement per line 34 Put argument names in function prototypes 35 Use a �predicate� form to split up long expressions 36 A subroutine should fit on a screen 37 All code should be printable 38 Use lines of dashes for visual separation between subroutines 39 White space is one of the most effective comments 40 Use four-space indents 41 Indent statements associated with a flow-control statement 41.1.Comments should be at the same indent level as the surrounding code 42 Align braces vertically at the outer level 43 Use braces when more than one line is present under a flow-control statement\nNames and Identifiers # 44 Names should be common English words, descriptive of what the function, argument, or variable does 44.1.Do not clutter names with gibberish 45 Macro names should be ENTIRELY_CAPITALIZED 45.1 Do not capitalize members of an enum 45.2 Do not capitalize type names created with a typedef 46 Avoid the ANSI C name space 47 Avoid the Microsoft name space 48 Avoid unnecessary symbols 49 Symbolic constants for Boolean values are rarely necessary\nRules for General Programming # 50 Don\u0026rsquo;t confuse familiarity with readability 51 A function should do only one thing 52 Too many levels of abstraction or encapsulation are as bad as too few 53 A function should be called more than once, but� 53.1 Code used more than once should be put into a function 54 A function should have only one exit point 54.1 Always put a return at the outer level 55 Avoid duplication of effort 56 Don\u0026rsquo;t corrupt the global name space 56.1 Avoid global symbols 56.2 Never require initialization of a global variable to call a function 56.2.1 Make locals static in recursive functions if the value doesn\u0026rsquo;t span a recursive call 56.3 Use instance counts in place of initialization functions 56.4 If an if ends in return, don\u0026rsquo;t use else 57 Put the shortest clause of an if/else on top 58 Try to move errors from run time to compile time 59 Use C function pointers as selectors 60 Avoid do/while loops 60.1 Never use a do/while for a forever loop 61 Counting loops should count down if possible 62 Don\u0026rsquo;t do the same thing in two ways at the same time 63 Use for if any two of an initialization, test, or increment are present 64 If it doesn\u0026rsquo;t appear in the test, it shouldn\u0026rsquo;t appear in the other parts of for statement 65 Assume that things will go wrong 66 Computers do not know mathematics 66.1 Expect the impossible 66.2 Always check error-return codes 67 Avoid explicit temporary variables 68 No magic numbers 69 Make no assumptions about sizes 70 Beware of casts (C issues) 71 Handle special cases directly 72 Don\u0026rsquo;t try to make lint happy 73 Put memory allocation and deallocation code in the same place 74 Heap memory is expensive 75 Test routines should not be interactive 76 An error message should tell the user what\u0026rsquo;s right 77 Don\u0026rsquo;t print error messages if an error is recoverable 78 Don\u0026rsquo;t use system-dependent functions for error messages\nThe Preprocessor # 79 Everything in a .h file should be used in at least two .c files 80 Use nested #includes 81 You should always be able to replace a macro with a function 81.1 ? is not the same as if/else 81.2 Parenthesize macro bodies and arguments 82 enum and const are better than a macro 83 A parameterized-macro argument should not appear more than once on the right-hand side 83.1 Never use macros for character constants 84 When all else fails, use the preprocessor C-Related Rules # 85 Stamp out the demons of complexity (Part 2) 85.1 Eliminate clutter.\nOO Programming/Design (C++ and Java) # 90 Object-oriented and �structured\u0026quot; designs don\u0026rsquo;t mix 90.1 If it\u0026rsquo;s not object-oriented, use C 91 Expect to spend more time in design and less in development 92 C++ class libraries usually can\u0026rsquo;t be used in a naive way 93 Use checklists 94 Messages should exercise capabilities, not request information 95 You usually cannot convert an existing structured program to object-oriented 96 A derived class object is a base-class object 97 Derivation is the process of adding member data and methods 98 Design the objects first 99 Design the hierarchy next, from the bottom up 99.1 Base classes should have more than one derived class 100 The capabilities defined in the base class should be used by all derived classes 101 C++ is not Smalltalk�avoid a common object class 102 Mix-ins shouldn\u0026rsquo;t derive from anything in C++, in Java there\u0026rsquo;s no problem if you follow the next rule: 103 Mix-ins should be C++ virtual base classes (in Java they should be interfaces) 104 Initialize virtual base classes with the default constructor 105 Derivation is not appropriate if you never send a base-class message to a derived-class object 106 Choose containment over derivation whenever possible 107 Use private base classes only when you must provide virtual overrides (C++ only) 108 Design the data structures last 109 All data in a class definition must be private 110 Never provide public access to private data 110.1 Do not use get/set functions 111 Give up on C idioms when coding in C++ 112 Design with derivation in mind 112.1 A member function should usually use the private data of a class 113 Use const (final in Java) 114 Use struct only if everything\u0026rsquo;s public and there are no member functions (C++ only) 115 Don\u0026rsquo;t put function bodies into class definitions (C++ only) 116 Avoid function overloads and default arguments 117 Avoid friend classes (in Java, don\u0026rsquo;t use package access.) 118 Inheritance is a form of coupling 119 Don\u0026rsquo;t corrupt the global name space\nC++ Rules # References # 120 Reference arguments should always be const 121 Never use references as outputs, use pointers 122 Do not return references (or pointers) to local variables 123 Do not return references to memory that came from new\nConstructors, Destructors, and operator=() # 124 Operator=() should return a const reference 125 Assignment to self must work 126 Classes having pointer members should always define a copy constructor and operator=() 127 If you can access an object, it has been initialized 128 Use member-initialization lists 129 Assume that members and base classes are initialized in random order 130 Copy constructors must use member initialization lists 131 Derived classes should usually define a copy constructor and operator=() 132 Constructors not suitable for type conversion should have two or more arguments 133 Use instance counts for class-level initialization 134 Avoid two-part initialization 135 C++ wrappers around existing interfaces rarely work well\nVirtual Functions # 136 Virtual functions are those functions that you can\u0026rsquo;t write at the base-class level 137 A virtual function isn\u0026rsquo;t virtual when called from a constructor or destructor 138 Do not call pure virtual functions from constructors 139 Destructors should always be virtual 140 Base-class functions that have the same name as derived-class functions generally should be virtual 141 Don\u0026rsquo;t make a function virtual unless you want the derived class to get control of it 142 protected functions should usually be virtual 143 Beware of casts: C++ issues 144 Don\u0026rsquo;t call constructors from operator=()\nOperator Overloads # 145 An operator is an abbreviation (no surprises) 146 Use operator overloads only to define operations for which there is a C analog (no surprises) 147 Once you overload an operation, you must overload all similar operations 148 Operator overloads should work exactly like they would in C 149 It\u0026rsquo;s best for a binary-operator overload to be an inline alias for a cast 150 Don\u0026rsquo;t go bonkers with type-conversion operators 151 Do all type conversions with constructors if possible\nMemory Management # 152 Use new/delete rather than malloc()/free() 153 All memory allocated in a constructor should be freed in the destructor 154 Local overloads of new and delete are dangerous\nTemplates # 155 Use inline function templates instead of parameterized macros 156 Always be aware of the size of the expanded template 157 Class templates should usually define derived classes 158 Templates do not replace derivation; they automate it\nExceptions # 159 Intend for exceptions not to be caught 160 Throw error objects when possible 161 Throwing exceptions from constructors is tricky\n","date":"16 February 2023","permalink":"/posts/programming-rules-of-thumb/","section":"Posts","summary":"Programming-Rules-of-Thumb # Programming Rules of Thumb # Created: December 4, 2019 6:22 PM Tags: Programming URL: https://holub.","title":"Programming-Rules-of-Thumb"},{"content":"Protests-Today-Live-Updates-and-Video-The-New-Yo # Mayor Jim Kenney and Danielle Outlaw, the city’s police commissioner, both took a knee at the protests, as aerial footage showed a swarm of demonstrators packed together downtown. More than 100 people gathered on Saturday for a protest against racism in Vidor, Texas, a town about 10,000 that has a history of Ku Klux Klan and white supremacist activity. The demonstrators, many holding “Black Lives Matter” signs, stood in a grassy area, chanting, “I can’t breathe!” and “No justice, no peace!”\nLatest Updates: George Floyd Protests # Updated 2020-06-07T01:14:12.916Z\n[Passionate but peaceful crowds take to American streets in another weekend of protest. Protests%20Today%20Live%20Updates%20and%20Video%20-%20The%20New%20Yo%205e1b1aaf261f429a812694100b9cf01e/merlin_173248479_9c509821-e1ab-4617-82f3-6d358561e780-articleLarge.jpg Police departments across the United States are re-examining their use-of-force policies as protesters continue to express outrage over such tactics in the wake of George Floyd’s death as Democrats in Congress plan expansive legislation to address police brutality and racial bias. City officials also said officers would be required to intervene and report any use of unauthorized force, a move that comes after nearly two weeks of protests over the death of Mr. Floyd, a black man whom a white Minneapolis police officer pinned under his knee for nearly nine minutes. Protests over the death of George Floyd were held in cities around world on Saturday, magnifying the voices of those speaking out against racism and police brutality. Two Buffalo police officers were arraigned Saturday morning on charges of assault in the second degree for pushing a 75-year-old man who was protesting outside City Hall on Thursday night, according to the Erie County District Attorney. ","date":"16 February 2023","permalink":"/posts/protests-today-live-updates-and-video-the-new-yo/","section":"Posts","summary":"Protests-Today-Live-Updates-and-Video-The-New-Yo # Mayor Jim Kenney and Danielle Outlaw, the city’s police commissioner, both took a knee at the protests, as aerial footage showed a swarm of demonstrators packed together downtown.","title":"Protests-Today-Live-Updates-and-Video-The-New-Yo"},{"content":"Quadratic-Payments-A-Primer # Quadratic Payments: A Primer # Created: December 10, 2019 9:55 AM Tags: Data, Finance URL: https://vitalik.ca/general/2019/12/07/quadratic.html Special thanks to Karl Floersch and Jinglan Wang for feedback If you follow applied mechanism design or decentralized governance at all, you may have recently heard one of a few buzzwords: quadratic voting, quadratic funding and quadratic attention purchase. These ideas have been gaining popularity rapidly over the last few years, and small-scale tests have already been deployed: the Taiwanese presidential hackathon used quadratic voting to vote on winning projects, Gitcoin Grants used quadratic funding to fund public goods in the Ethereum ecosystem, and the Colorado Democratic party also experimented with quadratic voting to determine their party platform. Quadratic%20Payments%20A%20Primer%209d075cbbe1144314a4f35620d936c95d/Market10.png Now let\u0026rsquo;s look at all three beside each other: Untitled Notice that only quadratic voting has this nice property that the amount of influence you purchase is proportional to how much you care; the other two mechanisms either over-privilege concentrated interests or over-privilege diffuse interests.\nQuadratic Voting # See also the original paper: https://papers.ssrn.com/sol3/papers.cfm?abstract%5fid=2003531 Let us begin by exploring the first \u0026ldquo;flavor\u0026rdquo; of quadratic payments: quadratic voting. Quadratic%20Payments%20A%20Primer%209d075cbbe1144314a4f35620d936c95d/Market7.png Note that in the voting case, we\u0026rsquo;re deciding two options, so different people will favor A over B or B over A; hence, unlike the graphs we saw earlier that start from zero, here voting and preference can both be positive or negative (which option is considered positive and which is negative doesn\u0026rsquo;t matter; the math works out the same way) As shown above, because the n\u0026rsquo;th vote has a cost of n, the number of votes you make is proportional to how much you value one unit of influence over the decision (the value of the decision multiplied by the probability that one vote will tip the result), and hence proportional to how much you care about A being chosen over B or vice versa. We interpret the quadratic funding as being a special case of quadratic voting, where the contributors to a project are voting for that project and there is one imaginary participant voting against it: the subsidy pool. Quadratic funding is starting to be explored as a mechanism for funding public goods already; Gitcoin grants for funding public goods in the Ethereum ecosystem is currently the biggest example, and the most recent round led to results that, in my own view, did a quite good job of making a fair allocation to support projects that the community deems valuable.\n","date":"16 February 2023","permalink":"/posts/quadratic-payments-a-primer/","section":"Posts","summary":"Quadratic-Payments-A-Primer # Quadratic Payments: A Primer # Created: December 10, 2019 9:55 AM Tags: Data, Finance URL: https://vitalik.","title":"Quadratic-Payments-A-Primer"},{"content":"Quick-guide-How-to-run-Apache-Airflow-with-docker # We will create a docker-compose file with:\nairflow webserver airflow scheduler (with LocalExecutor) PostgreSQL DB (we will use it as DB for Airflow) For PostgreSQL, we will use the official docker image: https://hub.docker.com/_/postgres/ version 9.6.14, for Apache Airflow: https://hub.docker.com/r/puckel/docker-airflow/ version 1.10.6 (note: we will switch to use official docker image as soon as it will be released, now we can just check AIP and wait, or participate if you can and want in process of creation). At the start, for quick development we will use one DAG folder that we will share between airflow scheduler and airflow webserver, so you will not need to rebuild and re-run server each time when you want to add dags. 1.3 Next, create a file with name Dockerfile (this is a default name for docker files to build images, let use it) In Dockerfile: FROM/RUN--ENVAIRFLOW_HOME///COPY///// Short description: FROM — define a base image that we use to create our image, ENV — define environment variables in an image, COPY — copy file from the local path to an image, RUN — run command. So, if you will put a file in main_folder/airflow/dags (‘.’ — mean current work dir where docker-compose.yaml is placed, so this is main_folder) it will be shown in Airflow images in /usr/local/airflow/dags, so Airflow webserver and Airflow Scheduler will see it. This will be enough for this tutorial, but… If you don’t want to do it with way, you can spend a little time and create a .sh script to put in docker-compose airflow services wait till DB will not create, that is this you can see, for example, here: https://github.com/vishnubob/wait-for-it but of course you need to modify sh to check is DB available to use or not. 3.1 Let’s run:\nmain_folderdocker-compose up postgres 3.1.1 after that in another terminal window run initdb (we need it only once if we clean up DB and start all from scratch)\ndocker-compose up initdb 3.2 After that run in another terminal window:\ndocker-compose up scheduler webserver As you see postgres, schedule and webserver — names of our services, that we defined in docker-compose.yml On next runs you can simply use (don’t forget to comment initdb service in docker-compose.yml):\ndocker-compose up 3.3 Now enter a UI to check at http://localhost:8080, that all run successfully. 4.1 Take it here (you need the whole file as a DAG): And copy it in your dags folder, that was defined in the volumes section of docker-compose file So, you got a structure of the main folder:\n…/main_folder — airflow.cfg — Dockerfile - docker-compose.yml airflow_files/ dags/ - example_bash_operator.py 4.2 Wait for 10–15 sec and check the UI, refresh it and wait for more if it still empty.\n","date":"16 February 2023","permalink":"/posts/quick-guide-how-to-run-apache-airflow-with-docker/","section":"Posts","summary":"Quick-guide-How-to-run-Apache-Airflow-with-docker # We will create a docker-compose file with:","title":"Quick-guide-How-to-run-Apache-Airflow-with-docker"},{"content":"Quick-Start-Guide-Leaflet-a-JavaScript-library # Quick Start Guide - Leaflet - a JavaScript library for interactive maps # Created: February 15, 2020 8:22 AM URL: https://leafletjs.com/examples/quick-start/ This step-by-step guide will quickly get you started on Leaflet basics, including setting up a Leaflet map, working with markers, polylines and popups, and dealing with events. Untitled\nPreparing your page # Before writing any code for the map, you need to do the following preparation steps on your page: # Include Leaflet CSS file in the head section of your document:\nInclude Leaflet JavaScript file after Leaflet’s CSS:\nPut a div element with a certain id where you want your map to be:\nFirst we’ll initialize the map and set its view to our chosen geographical coordinates and a zoom level: var mymap = L.map(\u0026lsquo;mapid\u0026rsquo;).setView([51.505, -0.09], 13);\nBy default (as we didn’t pass any options when creating the map instance), all mouse and touch interactions on the map are enabled, and it has zoom and attribution controls. In this example we’ll use the `mapbox/streets-v11` tiles from [Mapbox’s Static Tiles API](https://docs.mapbox.com/api/maps/#static-tiles) (in order to use tiles from Mapbox, you must also [request an access token](https://www.mapbox.com/studio/account/tokens/)). ### Markers, circles and polygons Besides tile layers, you can easily add other things to your map, including markers, polylines, polygons, circles, and popups. Let’s add a marker: var marker = L.marker([51.5, -0.09]).addTo(mymap);\nAdding a circle is the same (except for specifying the radius in meters as a second argument), but lets you control how it looks by passing options as the last argument when creating the object: var circle = L.circle([51.508, -0.11], { color: \u0026lsquo;red\u0026rsquo;, fillColor: \u0026lsquo;#f03\u0026rsquo;, fillOpacity: 0.5, radius: 500 }).addTo(mymap);\nAdding a polygon is as easy: var polygon = L.polygon([ [51.509, -0.08], [51.503, -0.06], [51.51, -0.047] ]).addTo(mymap);\n### Working with popups Popups are usually used when you want to attach some information to a particular object on a map. ### Dealing with events Every time something happens in Leaflet, e.g. user clicks on a marker or map zoom changes, the corresponding object sends an event which you can subscribe to with a function. ","date":"16 February 2023","permalink":"/posts/quick-start-guide-leaflet-a-javascript-library/","section":"Posts","summary":"Quick-Start-Guide-Leaflet-a-JavaScript-library # Quick Start Guide - Leaflet - a JavaScript library for interactive maps # Created: February 15, 2020 8:22 AM URL: https://leafletjs.","title":"Quick-Start-Guide-Leaflet-a-JavaScript-library"},{"content":"Qwiklabs-Hands-On-Cloud-Training # Qwiklabs - Hands-On Cloud Training # Created: February 21, 2020 12:07 PM URL: https://google.qwiklabs.com/?qlcampaign=1j-cloud-27 We give you temporary credentials to Google Cloud Platform, so you can learn the cloud using the real thing – no simulations. From 30-minute individual labs to multi-day courses, from introductory level to expert, instructor-led or self-paced, with topics like machine learning, security, infrastructure, app dev, and more, we\u0026rsquo;ve got you covered.\n","date":"16 February 2023","permalink":"/posts/qwiklabs-hands-on-cloud-training/","section":"Posts","summary":"Qwiklabs-Hands-On-Cloud-Training # Qwiklabs - Hands-On Cloud Training # Created: February 21, 2020 12:07 PM URL: https://google.","title":"Qwiklabs-Hands-On-Cloud-Training"},{"content":"Radiant-based-HVAC # Radiant based HVAC systems by its implied definition, integrates the [interior surface temperatures](Mean%20Radiant_pg2.htm#Surface temp) of the building enclosure for the purposes of evaluating the mean radiant temperature for establishing thermal comfort based on human factor design. Background: The control over surface temperatures for thermal comfort is not new; to set the bookends as it were, are two quotes one from 1857 and another 2010 - both from internationally recognized authorities on health and buildings advising us to focus on the enclosure to control heat transfer by radiation: 1857 - \u0026ldquo;the Commissioners of the General Board of Health advocated as one of the requirements for comfort that the walls of a room be at least as high in temperature as the general temperature of the room, while they included cold walls or floors amongst the conditions which make for discomfort.”1 2010 - National Building Code of Canada v2010: Section A-5.3.1.2. (1) Use of Thermal Insulation or Mechanical Systems for Environmental Control states, “In addition to controlling condensation, interior surface temperatures must be warm enough to avoid occupant discomfort due to excessive heat loss by radiation.” Furthermore, recognized authorities in human factor design and thermal comfort are in agreement with the UK\u0026rsquo;s Health and Safety Executive which states, “The most commonly used indicator of thermal comfort is air temperature – it is easy to use and most people can relate to it. Mechanical solutions, like low temperature radiant heating and high temperature radiant cooling can then be applied if necessary to compensate for the buildings short comings. As illustrated below, low performing buildings using traditional heating systems need high fluid temperatures in heating (and low temperatures in cooling) but only a high performance building can use low temperatures in heating and high temperatures in cooling (applies to all systems). In fact with high performing buildings, using radiant cooling and heating, conductive floors and close tube spacing, fluid and surface temperatures operate close to the core body temperature and enable maximum efficiency from the heating and cooling plant. So you win when you improve the building and you win when you use low temperature radiant heating and high temperature radiant cooling.\n","date":"16 February 2023","permalink":"/posts/radiant-based-hvac/","section":"Posts","summary":"Radiant-based-HVAC # Radiant based HVAC systems by its implied definition, integrates the [interior surface temperatures](Mean%20Radiant_pg2.","title":"Radiant-based-HVAC"},{"content":"Remote-Companies # Remote Companies # Created: December 11, 2019 5:35 PM Tags: List, Startup URL: https://remotemasters.dev/fully-remote-companies Below is a list of fully remote, 100% distributed companies hiring software developers. We constantly update this list to make sure it’s the most current and comprehensive list of fully remote companies on the internet. If you know of any fully remote companies that aren’t on this list yet, or you believe a company was listed here by mistake, please email us at contact@remotemasters.dev. You can also browse a list of remote-first companies here: https://remotemasters.dev/remote-first-companies\n","date":"16 February 2023","permalink":"/posts/remote-companies/","section":"Posts","summary":"Remote-Companies # Remote Companies # Created: December 11, 2019 5:35 PM Tags: List, Startup URL: https://remotemasters.","title":"Remote-Companies"},{"content":"Rondam-Ramblings-A-catalog-of-wealth-creation-mech # And the reason for linking the east coast of the United States with China was that a ship full of goods from China (including, for example, high quality china with a lower-case \u0026lsquo;c\u0026rsquo;, which at the time was produced nowhere else) could be sold in the east-coast cities of the U.S. at about a 300% profit. Money and wealth often go together, but they are completely different things. You can transform money into wealth, and vice versa (which is the whole point of having money), but you can have money without wealth and vice versa. Just as moving things around is the basis for the shipping industry, storing things is the basis for the retail industry. Stores are called stores because they used to be places where things were stored, not necessarily places where things were sold. A lot of wealth gets created this way, but of all the ways to make money it is arguably the least effective. Once upon a time, information was strongly bound to physical objects like books or vinyl records, and you could make money by producing these things because they were instances of #3.\n","date":"16 February 2023","permalink":"/posts/rondam-ramblings-a-catalog-of-wealth-creation-mech/","section":"Posts","summary":"Rondam-Ramblings-A-catalog-of-wealth-creation-mech # And the reason for linking the east coast of the United States with China was that a ship full of goods from China (including, for example, high quality china with a lower-case \u0026lsquo;c\u0026rsquo;, which at the time was produced nowhere else) could be sold in the east-coast cities of the U.","title":"Rondam-Ramblings-A-catalog-of-wealth-creation-mech"},{"content":"Rowise-vs-Columnar-Database-Theory-and-in-Practice # Theory and in Practice Created: April 20, 2020 10:08 PM Tags: Database, SQL URL: https://medium.com/@mangatmodi/rowise-vs-columnar-database-theory-and-in-practice-53f54c8f6505 Columnar database stores are getting increasingly popular lately especially in analytical query systems as data warehouse solutions. Here comes to major difference about row and columnar DBs\nRow oriented database tries to store whole row of the database in the same block but columnar database stores the values of the columns of subsequent in the same block\nFollowing is an excellent diagram from Redshift’s documentation[1] to understand the difference. Easy to guess — Row oriented databases perform best for the operations on the whole row and columnar for the operations on the columns. Column Oriented Operations: Update columns, Aggregations, selecting a few columns, and any other operation, stored procedures, etc. Since disk block contributes a lot to the performance it is recommended to have block size which is enough to fit the whole row for row wise DBs and for columnar keep reasonably high block size so that you can operate of more columnar data at one read. Columnar DBs can optimize a lot of space as they keep homogenous data in a single block, so they can apply strategies to compress the data in a block. This is one of the reasons that Columnar DBs are popular as big data DBs because less storage means fewer costs, faster reads (read fewer blocks).\n","date":"16 February 2023","permalink":"/posts/rowise-vs-columnar-database-theory-and-in-practice/","section":"Posts","summary":"Rowise-vs-Columnar-Database-Theory-and-in-Practice # Theory and in Practice Created: April 20, 2020 10:08 PM Tags: Database, SQL URL: https://medium.","title":"Rowise-vs-Columnar-Database-Theory-and-in-Practice"},{"content":"Scale-Workspace # Scale Workspace # Created: May 27, 2020 12:11 PM URL: https://www.scaleworkspace.com/ 612 Andrew Higgins Blvd, New Orleans LA 70130 2019 © Scale Workspace. All rights reserved. Design By: Freret Digital Media™\n","date":"16 February 2023","permalink":"/posts/scale-workspace/","section":"Posts","summary":"Scale-Workspace # Scale Workspace # Created: May 27, 2020 12:11 PM URL: https://www.","title":"Scale-Workspace"},{"content":"Scaling-Zapier-to-Automate-Billions-of-Tasks-The # Scaling Zapier to Automate Billions of Tasks - The Zapier Engineering Blog | Zapier # Created: June 18, 2020 3:54 AM URL: https://zapier.com/engineering/automating-billions-of-tasks/ !\nThe Teams Behind the Curtains # It takes a lot to make Zapier tick, so we have four distinct teams in engineering:\nThe frontend team works on the very powerful workflow editor. Data # MySQL is our primary relational data store—you\u0026rsquo;ll find our users, Zaps and more inside MySQL.\nSome Rough Numbers # These numbers represent a rough minimum to help the reader gauge the general size and dimensions of Zapier\u0026rsquo;s architecture:\nover 8m tasks automated daily over 60m API calls daily over 10m inbound webhooks daily ~12 c3.2xlarge boxes running HTTP behind ELB ~100 m3.2xlarge background workers running Celery (split amongst polling, hooks, email, misc) ~3 m3.medium RabbitMQ nodes in a cluster ~4 r3.2xlarge Redis instances - one hot, two failover, one backup/imaging ~12 m2.xlarge Memcached instances behind ~6 c3.xlarge McRouter instances ~10 m3.xlarge ElasticSearch instances behind ~3 m3.xlarge no-data ElasticSearch instances ~6 m3.xlarge ElasticSearch instances behind ~1 c3.2xlarge Graylog server ~10 dc1.large Redshift nodes in a cluster 1 master db.m2.2xlarge RDS MySQL instance w/ ~2 more replicas for both production reads and analysis a handful of supporting RDS MySQL instance (more details below) …and tons of microservices and miscellaneous specialty services Improving the Architecture # While the broad strokes of the architecture remain the same—we\u0026rsquo;ve only performed a few massive migrations—a lot of work has been done to grow the product in two categories: 1. Every step has to consume some data (which folder to read from or which list ID to add to, for example), do some API magic, and return some data (say, the new file created or the new card added to a list), but otherwise be ignorant of its placement in the workflow. In the middle exists the omniscient workflow engine which coordinates independent Celery tasks by stringing together steps as tasks—one step feeding into the next as defined by the Zap\u0026rsquo;s directed rooted tree. In addition to Multi-Step Zaps, we\u0026rsquo;ve also launched the ability to write Python and JavaScript as Code steps in your workflow.\n","date":"16 February 2023","permalink":"/posts/scaling-zapier-to-automate-billions-of-tasks-the/","section":"Posts","summary":"Scaling-Zapier-to-Automate-Billions-of-Tasks-The # Scaling Zapier to Automate Billions of Tasks - The Zapier Engineering Blog | Zapier # Created: June 18, 2020 3:54 AM URL: https://zapier.","title":"Scaling-Zapier-to-Automate-Billions-of-Tasks-The"},{"content":"Screen-Home # Screen | Home # Created: March 24, 2020 4:24 PM URL: https://screen.so/#/home Fast screen sharing with multiplayer control, drawing \u0026amp; video. Available for Mac, Windows, Linux, iOS and Android With low-latency, multiplayer screen sharing and voice chat, both sides can switch seamlessly between driving and navigating with zero overhead. Perfect for remote pair programming, live debugging, and even code review. With Screen’s always-on drawing, you can quickly draw on the screen to highlight something, or better communicate an idea. Whiteboarding has never been easier. Doodle together at anytime, even from a smartphone or tablet. And you’ll never need to worry about dried-out markers.\n","date":"16 February 2023","permalink":"/posts/screen-home/","section":"Posts","summary":"Screen-Home # Screen | Home # Created: March 24, 2020 4:24 PM URL: https://screen.","title":"Screen-Home"},{"content":"Serverless-Slash-Commands-with-Python-Renzo-Luci # Serverless Slash Commands with Python - Renzo Lucioni # Created: February 21, 2020 12:03 PM URL: https://renzo.lucioni.xyz/serverless-slash-commands-with-python/ Slack’s slash commands allow you to perform actions by typing commands into Slack. Serverless%20Slash%20Commands%20with%20Python%20-%20Renzo%20Luci%2094a762f9efaa44ad83e6a523a512da4e/hello-there-app-credentials_hu183e95f920966d8fe95c9761fdfb32b9_29554_500x500_fit_lanczos_2.png You’re going to create a command invoked with /hello-there which responds with “General Kenobi!” whenever the command is run. Serverless%20Slash%20Commands%20with%20Python%20-%20Renzo%20Luci%2094a762f9efaa44ad83e6a523a512da4e/hello-there-team-id_hu41e0fba4d4b1057f776459be142ef28a_17546_500x500_fit_lanczos_2.png See Slack’s command validation docs for more details about what’s going on in is_request_valid(). Zappa creates a Lambda function containing your Flask app and sets up a wildcard API Gateway route to proxy requests from Slack to the app. Then, just like before, navigate back to your app’s “Slash Commands” section in your browser and edit the configuration of the /hello-there command, replacing the ngrok forwarding URL with the API Gateway URL. To be more faithful to the scene from Revenge of the Sith, you now want your /hello-there command to respond with “General Kenobi!”, followed 5 seconds later by “You are a bold one.” Your command’s backend has to do more work now: receive a POST request, verify that the request was issued by Slack, schedule a task that will issue the second message 5 seconds in the future, and respond immediately with the first message.\nSlash Command Best Practices # Slack lists a variety of slash command best practices in their docs.\n","date":"16 February 2023","permalink":"/posts/serverless-slash-commands-with-python-renzo-luci/","section":"Posts","summary":"Serverless-Slash-Commands-with-Python-Renzo-Luci # Serverless Slash Commands with Python - Renzo Lucioni # Created: February 21, 2020 12:03 PM URL: https://renzo.","title":"Serverless-Slash-Commands-with-Python-Renzo-Luci"},{"content":"Setosa-data-visualization-and-visual-explanations # Setosa%20data%20visualization%20and%20visual%20explanations%20e5a7b77eba1e4025aa699f7d7b82d9a0/ev.png Traffic Waves ! Setosa%20data%20visualization%20and%20visual%20explanations%20e5a7b77eba1e4025aa699f7d7b82d9a0/markov-chains.png Simpson\u0026rsquo;s Paradox ! Setosa%20data%20visualization%20and%20visual%20explanations%20e5a7b77eba1e4025aa699f7d7b82d9a0/simpsons.png ! Setosa%20data%20visualization%20and%20visual%20explanations%20e5a7b77eba1e4025aa699f7d7b82d9a0/pythagorean.png Woodcut Data Visualization ! Setosa%20data%20visualization%20and%20visual%20explanations%20e5a7b77eba1e4025aa699f7d7b82d9a0/bart.png Bart Salaries ! Setosa%20data%20visualization%20and%20visual%20explanations%20e5a7b77eba1e4025aa699f7d7b82d9a0/taxis.png Deceptions per Gallon ! Setosa%20data%20visualization%20and%20visual%20explanations%20e5a7b77eba1e4025aa699f7d7b82d9a0/parking.png\n","date":"16 February 2023","permalink":"/posts/setosa-data-visualization-and-visual-explanations/","section":"Posts","summary":"Setosa-data-visualization-and-visual-explanations # Setosa%20data%20visualization%20and%20visual%20explanations%20e5a7b77eba1e4025aa699f7d7b82d9a0/ev.","title":"Setosa-data-visualization-and-visual-explanations"},{"content":"Showcase-Information-is-Beautiful-Awards # Showcase — Information is Beautiful Awards # Created: February 21, 2020 12:03 PM URL: https://www.informationisbeautifulawards.com/showcase?action=index\u0026amp;award=2019\u0026amp;controller=showcase\u0026amp;page=1\u0026amp;pcategory=short-list\u0026amp;type=awards\n","date":"16 February 2023","permalink":"/posts/showcase-information-is-beautiful-awards/","section":"Posts","summary":"Showcase-Information-is-Beautiful-Awards # Showcase — Information is Beautiful Awards # Created: February 21, 2020 12:03 PM URL: https://www.","title":"Showcase-Information-is-Beautiful-Awards"},{"content":"Simple-Analytics # Simple Analytics # Created: December 14, 2019 10:38 AM Tags: Startup URL: https://docs.simpleanalytics.com/uniques Referral program At Simple Analytics we do things a bit differently. Traditional analytics tools will show you unique visits based on a cookie they place on the visitors computer.\nNo consent needed # With Simple Analytics you don’t need consent. That’s why we came up with our unique way of tracking unique visits. Simple%20Analytics%203bf12c1c35614c37890c8cbd562f7eab/referrer-visit.jpg When a user lands on your website without visiting another website (direct visit) we will record it as a non unique visit: ! Simple%20Analytics%203bf12c1c35614c37890c8cbd562f7eab/direct-visit.jpg\nSPAs # If you have a single-page application we automatically see all visits after the first visit as non unique. For the first visit we use above functionality to detect if a visit is unique.\n","date":"16 February 2023","permalink":"/posts/simple-analytics/","section":"Posts","summary":"Simple-Analytics # Simple Analytics # Created: December 14, 2019 10:38 AM Tags: Startup URL: https://docs.","title":"Simple-Analytics"},{"content":"Simple-Fast-Easy-Parallelism-in-Shell-Pipelines # Simple, Fast, Easy Parallelism in Shell Pipelines # Created: January 12, 2023 9:28 PM URL: https://catern.com/posts/pipes.html The typical shell1 pipeline looks something like this: Usually src will output lines of data, and worker acts as a filter, processing each line and sending some transformed lines to sink. So there is a possibility of parallel processing on the basis of lines: We could have multiple worker tasks2 which each process different lines of data from src and output transformed lines to sink. But the typical data-processing Unix command reads lines of input from stdin, not from its arguments on the command line, so many commands simply don\u0026rsquo;t make sense to use with xargs. A technique that allows a pool of worker tasks2, executing in parallel, to process incoming lines as they arrive on stdin, would be strictly more general. Writing a parallel pipeline in any shell looks like this:3\nsrc | { worker \u0026amp; worker \u0026amp; worker \u0026amp; } | sink This will start up three workers in the background, which will all read data from src in parallel, and write output to sink. 5 Since all the worker tasks are reading input at the same time, we might run into a common concurrency issue: one worker task might get the first part of a line, while the rest of the line goes to another worker task. We can control interleaving of the output with other techniques too, such as stdbuf -oL, to force output to be written in units of whole lines, which will be atomic as long as those lines are short enough.\n","date":"16 February 2023","permalink":"/posts/simple-fast-easy-parallelism-in-shell-pipelines/","section":"Posts","summary":"Simple-Fast-Easy-Parallelism-in-Shell-Pipelines # Simple, Fast, Easy Parallelism in Shell Pipelines # Created: January 12, 2023 9:28 PM URL: https://catern.","title":"Simple-Fast-Easy-Parallelism-in-Shell-Pipelines"},{"content":"Six-Governance-Topologies-for-Data-Mesh # Six Governance Topologies for Data Mesh # Created: December 15, 2022 9:12 PM URL: https://www.infoq.com/news/2022/06/data-mesh-topologies/ Piethein Strengholt, author of Data Management at Scale, recently published an article presenting six data-mesh governance topologies and domain granularity. Each topology adapts the data mesh strategy to balance requirements like data ownership, organization structure, pace of change, technology, and others. Data mesh is an enterprise data architecture that adapts and applies the learnings in building distributed architectures to the domain of data. Data mesh recommends creating self-serve data infrastructure, treating data as a product, and organizing teams and architecture based on business domains. Organizations must adapt the data mesh concept to implement what works best for them today and use one of the following topologies as a starting point: ! Organizations that value quality and compliance over agility can implement the fine-grained and fully governed mesh topology because it adds a central distribution layer to address distribution concerns. However, organizations with hard-to-maintain legacy systems or lacking highly skilled software engineers can implement the \u0026ldquo;hybrid federated mesh\u0026rdquo; topology where federation happens on the consuming side domains and centralization in the others.\n","date":"16 February 2023","permalink":"/posts/six-governance-topologies-for-data-mesh/","section":"Posts","summary":"Six-Governance-Topologies-for-Data-Mesh # Six Governance Topologies for Data Mesh # Created: December 15, 2022 9:12 PM URL: https://www.","title":"Six-Governance-Topologies-for-Data-Mesh"},{"content":"snippet-generator # snippet generator # Created: April 25, 2020 3:58 AM URL: https://snippet-generator.app/ ! snippet-generator.jpg\n\u0026#34;\u0026#34;: { \u0026#34;prefix\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;body\u0026#34;: [ \u0026#34;\u0026#34; ], \u0026#34;description\u0026#34;: \u0026#34;\u0026#34; } ","date":"16 February 2023","permalink":"/posts/snippet-generator/","section":"Posts","summary":"snippet-generator # snippet generator # Created: April 25, 2020 3:58 AM URL: https://snippet-generator.","title":"snippet-generator"},{"content":"So-the-time-value-of-money-is-more-or-less-zero-no # So the time value of money is more or less zero now and loan rates depend much m\u0026hellip; | Hacker News # Created: March 31, 2020 11:00 AM URL: https://news.ycombinator.com/item?id=22738370 So the time value of money is more or less zero now and loan rates depend much more on default risk than any opportunity cost in loaning the money. A savings account at 0% doesn\u0026rsquo;t build wealth, but it didn\u0026rsquo;t really do that 3 months ago at 1.5%. Personal loans at 9% aren\u0026rsquo;t much better than loans at 11%. I\u0026rsquo;m not saying bet on them (I don\u0026rsquo;t know if they\u0026rsquo;ll happen, nor can I predict the future and time them). As an example, everyone in this life needs a roof over their head, so if you don\u0026rsquo;t own anything real estate related, a bubble in real estate prices is a risk. A homeowner who plans to stay in their house 20 more years doesn\u0026rsquo;t have to care about annual changes in the real estate market\u0026hellip;but a renter does. If your time horizon is long enough, not being able to buy stocks at a decent price is a bigger threat to your retirement than a short term drop, so make sure you have some amount of money in the market while the market isn\u0026rsquo;t at all time highs.\n","date":"16 February 2023","permalink":"/posts/so-the-time-value-of-money-is-more-or-less-zero-no/","section":"Posts","summary":"So-the-time-value-of-money-is-more-or-less-zero-no # So the time value of money is more or less zero now and loan rates depend much m\u0026hellip; | Hacker News # Created: March 31, 2020 11:00 AM URL: https://news.","title":"So-the-time-value-of-money-is-more-or-less-zero-no"},{"content":"Software-Development-AntiPatterns # Software Development AntiPatterns # Created: June 16, 2020 2:17 PM Tags: Code, Design patterns URL: https://sourcemaking.com/antipatterns/software-development-antipatterns ! home-tb1.png\n","date":"16 February 2023","permalink":"/posts/software-development-antipatterns/","section":"Posts","summary":"Software-Development-AntiPatterns # Software Development AntiPatterns # Created: June 16, 2020 2:17 PM Tags: Code, Design patterns URL: https://sourcemaking.","title":"Software-Development-AntiPatterns"},{"content":"Some-SQL-Tricks-of-an-Application-DBA-Haki-Benita # To demonstrate, set up a small schema for a store:\nDROP TABLE IF EXISTS product CASCADE; CREATE TABLE product ( id serial PRIMARY KEY, name TEXT NOT NULL, price INT NOT NULL ); INSERT INTO product (name, price) SELECT random()::text, (random() * 1000)::int FROM generate_series(0, 10000); DROP TABLE IF EXISTS customer CASCADE; CREATE TABLE customer ( id serial PRIMARY KEY, name TEXT NOT NULL ); INSERT INTO customer (name) SELECT random()::text FROM generate_series(0, 100000); DROP TABLE IF EXISTS sale; CREATE TABLE sale ( id serial PRIMARY KEY, created timestamptz NOT NULL, product_id int NOT NULL, customer_id int NOT NULL ); The schema defines different types of constraints such as \u0026ldquo;not null\u0026rdquo; and unique constraints. To set a baseline, start by adding foreign keys to the sale table, and then load some data into it:\ndb=# ALTER TABLE sale ADD CONSTRAINT sale_product_fk db-# FOREIGN KEY (product_id) REFERENCES product(id); ALTER TABLE Time: 18.413 ms db=# ALTER TABLE sale ADD CONSTRAINT sale_customer_fk db-# FOREIGN KEY (customer_id) REFERENCES customer(id); ALTER TABLE Time: 5.464 ms db=# CREATE INDEX sale_created_ix ON sale(created); CREATE INDEX Time: 12.605 ms db=# INSERT INTO SALE (created, product_id, customer_id) db-# SELECT db-# now() - interval \u0026#39;1 hour\u0026#39; * random() * 1000, db-# (random() * 10000)::int + 1, db-# (random() * 100000)::int + 1 db-# FROM generate_series(1, 1000000); INSERT 0 1000000 Time: 15410.234 ms (00:15.410) After defining constraints and indexes, loading a million rows to the table took ~15.4s. Next, try to load the data into the table first, and only then add constraints and indexes: Loading data into a table without indexes and constraints was much faster, 2.27s compared to 15.4s before. Let\u0026rsquo;s populate the table with user data, where roughly 90% of the users are activated: To query for activated and unactivated users, you might be tempted to create an index on the column activated: When you try to query unactivated users, the database is using the index: The database estimated that the filter will result in 102,567 which are roughly 10% of the table. In PostgreSQL, there is a way to create an index on only a part of the table, using whats called a partial index: Using a WHERE clause, we restrict the rows indexed by the index. But, PostgreSQL provides other types of indexes such as BRIN:\nBRIN is designed for handling very large tables in which certain columns have some natural correlation with their physical location within the table\nBRIN stands for Block Range Index. If the number of adjacent pages is set to 3, the index will divide the table into the following ranges: For each range, the BRIN index keeps the minimum and maximum value: Using the index above, try to search for the value 5:\n[1–3] - Definitely not here [4–6] - Might be here [7–9] - Definitely not here Using the BRIN index we managed to limit our search to blocks 4–6. ","date":"16 February 2023","permalink":"/posts/some-sql-tricks-of-an-application-dba-haki-benita/","section":"Posts","summary":"Some-SQL-Tricks-of-an-Application-DBA-Haki-Benita # To demonstrate, set up a small schema for a store:","title":"Some-SQL-Tricks-of-an-Application-DBA-Haki-Benita"},{"content":"Southern-Exposure-Seed-Exchange-Saving-the-Past-f # Please note that we are not currently able to keep up with the call volume on our phones. If you have a pressing matter, e-mail is a better way to reach us: gardens@southernexposure.com If you’re going out for supplies, you may find our seeds in a local retail store. Please check out our seed saving guides and our other growing guides. Eastern seed companies: Heavenly Seed (SC) Sow True Seed (NC) Two Seeds in a Pod (WV) True Love Seed (PA) Fruition Seeds (NY) Hudson Valley Seed Company (NY) Northern seed companies: Meadowlark Hearth (NE) Small House Farm (MI) Prairie Garden Seed (Saskatchewan) North Circle Seeds (MN) Nature and Nurture Seeds (MI) Western seed companies: San Diego Seed Company (CA) Redwood Seeds (CA) Quail Seeds (CA) Refining Fire Chiles (CA) Restoration Seeds (OR) Thanks for thinking seeds and for thinking food and for thinking of us in these strange and difficult times. Southern%20Exposure%20Seed%20Exchange,%20Saving%20the%20Past%20f%20200790d608eb4d5ab2246de1a2565dbd/growing-guides-mac-marsden.jpg\nFeatured Customer Favorites # Pictured here, Danvers 126 carrots, Contender snap bush bean, and Cocozelle Italian zucchini. Other customer favorites include Lacinato kale, Cossack Pineapple ground cherries, Sugar Snap Peas, and Dark Green Italian parsley.\nNew varieties for 2020 # For 2020, we’re adding 25 new varieties to our listings including Yukina Savoy mustard greens, Alston Everlasting tomatoes, Jaloro hot peppers, Cateto Sulino flint corn, Renick Yellow watermelon, Pearl Millet, White Russian kale, Yellowstone carrots, and Callaloo amaranth.\n","date":"16 February 2023","permalink":"/posts/southern-exposure-seed-exchange-saving-the-past-f/","section":"Posts","summary":"Southern-Exposure-Seed-Exchange-Saving-the-Past-f # Please note that we are not currently able to keep up with the call volume on our phones.","title":"Southern-Exposure-Seed-Exchange-Saving-the-Past-f"},{"content":"Sprint-Test-Flight-5-Project-Recap-YouTube # Sprint - Test Flight 5 + Project Recap - YouTube # Created: May 31, 2020 12:49 PM URL: https://www.youtube.com/watch?v=eh8ic1-5wFo https://www.youtube.com/watch?v=eh8ic1-5wFo\n","date":"16 February 2023","permalink":"/posts/sprint-test-flight-5-project-recap-youtube/","section":"Posts","summary":"Sprint-Test-Flight-5-Project-Recap-YouTube # Sprint - Test Flight 5 + Project Recap - YouTube # Created: May 31, 2020 12:49 PM URL: https://www.","title":"Sprint-Test-Flight-5-Project-Recap-YouTube"},{"content":"SQL # SQL # Created: May 11, 2020 4:35 PM URL: https://news.ycombinator.com/item?id=23138297\n","date":"16 February 2023","permalink":"/posts/sql/","section":"Posts","summary":"SQL # SQL # Created: May 11, 2020 4:35 PM URL: https://news.","title":"SQL"},{"content":"SQL-Indexing-and-Tuning-e-Book-for-developers-Use # SQL Indexing and Tuning e-Book for developers: Use The Index, Luke covers Oracle, MySQL, PostgreSQL, SQL Server, \u0026hellip; # Created: May 11, 2020 4:34 PM Tags: Database, Design patterns, SQL URL: https://use-the-index-luke.com/ A site explaining SQL indexing to developers—no crap about administration. Use The Index, Luke explains SQL indexing from grounds up and doesn\u0026rsquo;t stop at ORM tools like Hibernate. Use The Index, Luke is the free web-edition of SQL Performance Explained.\nSQL Indexing in MySQL, Oracle, SQL Server, etc. # Use The Index, Luke presents indexing in a vendor agnostic fashion. Anatomy of an Index — What does an index look like?\n","date":"16 February 2023","permalink":"/posts/sql-indexing-and-tuning-e-book-for-developers-use/","section":"Posts","summary":"SQL-Indexing-and-Tuning-e-Book-for-developers-Use # SQL Indexing and Tuning e-Book for developers: Use The Index, Luke covers Oracle, MySQL, PostgreSQL, SQL Server, \u0026hellip; # Created: May 11, 2020 4:34 PM Tags: Database, Design patterns, SQL URL: https://use-the-index-luke.","title":"SQL-Indexing-and-Tuning-e-Book-for-developers-Use"},{"content":"SQLite # To put it another way, a recursive common table expression must look like the following: Call the table named by the cte-table-name in a recursive common table expression the \u0026ldquo;recursive table\u0026rdquo;. A limit of zero means that no rows are ever added to the recursive table, and a negative limit means an unlimited number of rows may be added to the recursive table. The recursive CTE is then used in the final query:\nWITH RECURSIVE parent_of(name, parent) AS (SELECT name, mom FROM family UNION SELECT name, dad FROM family), ancestor_of_alice(name) AS (SELECT parent FROM parent_of WHERE name=\u0026lsquo;Alice\u0026rsquo; UNION ALL SELECT parent FROM parent_of JOIN ancestor_of_alice USING(name)) SELECT family.name FROM ancestor_of_alice, family WHERE ancestor_of_alice.name=family.name AND died IS NULL ORDER BY born;\nA version control system (VCS) will typically store the evolving versions of a project as a directed acyclic graph (DAG).\nWITH RECURSIVE ancestor(id,mtime) AS ( SELECT id, mtime FROM checkin WHERE id=@BASELINE UNION SELECT derivedfrom.xfrom, checkin.mtime FROM ancestor, derivedfrom, checkin WHERE ancestor.id=derivedfrom.xto AND checkin.id=derivedfrom.xfrom ORDER BY checkin.mtime DESC LIMIT 20 ) SELECT * FROM checkin JOIN ancestor USING(id);\nThe \u0026ldquo;ORDER BY checkin.mtime DESC\u0026rdquo; term in the recursive-select makes the query run much faster by preventing it from following branches that merge checkins from long ago. To illustrate, we will use a variation on the \u0026ldquo;org\u0026rdquo; table from an example above, without the \u0026ldquo;height\u0026rdquo; column, and with some real data inserted:\nCREATE TABLE org( name TEXT PRIMARY KEY, boss TEXT REFERENCES org ) WITHOUT ROWID; INSERT INTO org VALUES(\u0026lsquo;Alice\u0026rsquo;,NULL); INSERT INTO org VALUES(\u0026lsquo;Bob\u0026rsquo;,\u0026lsquo;Alice\u0026rsquo;); INSERT INTO org VALUES(\u0026lsquo;Cindy\u0026rsquo;,\u0026lsquo;Alice\u0026rsquo;); INSERT INTO org VALUES(\u0026lsquo;Dave\u0026rsquo;,\u0026lsquo;Bob\u0026rsquo;); INSERT INTO org VALUES(\u0026lsquo;Emma\u0026rsquo;,\u0026lsquo;Bob\u0026rsquo;); INSERT INTO org VALUES(\u0026lsquo;Fred\u0026rsquo;,\u0026lsquo;Cindy\u0026rsquo;); INSERT INTO org VALUES(\u0026lsquo;Gail\u0026rsquo;,\u0026lsquo;Cindy\u0026rsquo;);\nHere is a query to show the tree structure in a breadth-first pattern:\nWITH RECURSIVE under_alice(name,level) AS ( VALUES(\u0026lsquo;Alice\u0026rsquo;,0) UNION ALL SELECT org.name, under_alice.level+1 FROM org JOIN under_alice ON org.boss=under_alice.name ORDER BY 2 ) SELECT substr(\u0026rsquo;\u0026hellip;\u0026hellip;\u0026hellip;.\u0026rsquo;,1,level*3) || name FROM under_alice;\nThe \u0026ldquo;ORDER BY 2\u0026rdquo; (which means the same as \u0026ldquo;ORDER BY under_alice.level+1\u0026rdquo;) causes higher levels in the organization chart (with smaller \u0026ldquo;level\u0026rdquo; values) to be processed first, resulting in a breadth-first search. The output is:\nAlice \u0026hellip;Bob \u0026hellip;Cindy \u0026hellip;\u0026hellip;Dave \u0026hellip;\u0026hellip;Emma \u0026hellip;\u0026hellip;Fred \u0026hellip;\u0026hellip;Gail\nBut if we change the ORDER BY clause to add the \u0026ldquo;DESC\u0026rdquo; modifier, that will cause lower levels in the organization (with larger \u0026ldquo;level\u0026rdquo; values) to be processed first by the recursive-select, resulting in a depth-first search:\nWITH RECURSIVE under_alice(name,level) AS ( VALUES(\u0026lsquo;Alice\u0026rsquo;,0) UNION ALL SELECT org.name, under_alice.level+1 FROM org JOIN under_alice ON org.boss=under_alice.name ORDER BY 2 DESC ) SELECT substr(\u0026rsquo;\u0026hellip;\u0026hellip;\u0026hellip;.\u0026rsquo;,1,level*3) || name FROM under_alice;\nThe output of this revised query is:\nAlice \u0026hellip;Bob \u0026hellip;\u0026hellip;Dave \u0026hellip;\u0026hellip;Emma \u0026hellip;Cindy \u0026hellip;\u0026hellip;Fred \u0026hellip;\u0026hellip;Gail\nWhen the ORDER BY clause is omitted from the recursive-select, the queue behaves as a FIFO, which results in a breadth-first search. Outlandish Recursive Query Examples The following query computes an approximation of the Mandelbrot Set and outputs the result as ASCII-art:\nWITH RECURSIVE xaxis(x) AS (VALUES(-2.0) UNION ALL SELECT x+0.05 FROM xaxis WHERE x\u0026lt;1.2), yaxis(y) AS (VALUES(-1.0) UNION ALL SELECT y+0.1 FROM yaxis WHERE y\u0026lt;1.0), m(iter, cx, cy, x, y) AS ( SELECT 0, x, y, 0.0, 0.0 FROM xaxis, yaxis UNION ALL SELECT iter+1, cx, cy, xx-yy + cx, 2.0xy + cy FROM m WHERE (xx + yy) \u0026lt; 4.0 AND iter\u0026lt;28 ), m2(iter, cx, cy) AS ( SELECT max(iter), cx, cy FROM m GROUP BY cx, cy ), a(t) AS ( SELECT group_concat( substr(\u0026rsquo; .+*#\u0026rsquo;, 1+min(iter/7,4), 1), \u0026lsquo;\u0026rsquo;) FROM m2 GROUP BY cy ) SELECT group_concat(rtrim(t),x'0a\u0026rsquo;) FROM a;\nIn this query, the \u0026ldquo;xaxis\u0026rdquo; and \u0026ldquo;yaxis\u0026rdquo; CTEs define the grid of points for which the Mandelbrot Set will be approximated.\n","date":"16 February 2023","permalink":"/posts/sqlite/","section":"Posts","summary":"SQLite # To put it another way, a recursive common table expression must look like the following: Call the table named by the cte-table-name in a recursive common table expression the \u0026ldquo;recursive table\u0026rdquo;.","title":"SQLite"},{"content":"Startup-Marketing-Advice-from-Balsamiq-Studio-The # Startup Marketing Advice from Balsamiq Studio | The Balsamiq Blog | Balsamiq # Created: May 25, 2020 9:58 PM URL: https://blog.balsamiq.com/startup-marketing-advice-from-balsamiq-studios/ First off, the title of this post is a bit pretentious for my taste, but I wanted to pay homage to Mike Speiser\u0026rsquo;s excellent post on A/B testing using AdWords, and take advantage of his $10.87 investment while I was at it! Still, I can\u0026rsquo;t deny that Balsamiq has received a very good amount of coverage in the blogosphere: I am timing this post to coincide with the 100th review of Balsamiq Mockups (the full list is here), the website has received over 32,000 unique visitors and sales are exceeding all my expectations. After reading this article I decided to add this section to my company page and to tweak my \u0026ldquo;direct email template\u0026rdquo; (see below), and after reading this one I created an OPML file for Balsamiq\u0026rsquo;s feeds, which in the end resulted in Marshall covering Balsamiq on ReadWriteWeb himself. Peldi \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; Giacomo \u0026lsquo;Peldi\u0026rsquo; Guilizzoni Founder, Balsamiq Studios, LLC http://www.balsamiq.com ph: +1 (415) 367-3531, Skype, GTalk, Facebook, Twitter, FriendFeed: balsamiq\nIf you are reading this and were on the receiving end of an email like this, forgive me for sending you a templated email\u0026hellip;but I sent 40+ of these puppies and efficiency is a must for a one-man startup like mine! I also have an RSS feed of a Twitter search for prototyping OR \u0026ldquo;UI AND design\u0026rdquo; OR wireframe OR mockup OR UX which is a great read in general, and useful for finding people who are looking for a tool like mine: here\u0026rsquo;s an example, and here\u0026rsquo;s another. Give to bloggers to give away Bloggers love to give away free stuff, as it makes their blog more popular. All I do is select the person\u0026rsquo;s name (from their email), copy it, paste it in the application, click \u0026ldquo;generate key\u0026rdquo;, and what I get is not only the key, but a pre-populated email like this one: Hi there, here\u0026rsquo;s your license info: Download URL: http://www.balsamiq.com/products/mockups/desktop#download Organization name: test test Serial Key: [redacted] *I have some assets you can use here: http://www.balsamiq.com/company#forbloggers if you\u0026rsquo;d like.\n","date":"16 February 2023","permalink":"/posts/startup-marketing-advice-from-balsamiq-studio-the/","section":"Posts","summary":"Startup-Marketing-Advice-from-Balsamiq-Studio-The # Startup Marketing Advice from Balsamiq Studio | The Balsamiq Blog | Balsamiq # Created: May 25, 2020 9:58 PM URL: https://blog.","title":"Startup-Marketing-Advice-from-Balsamiq-Studio-The"},{"content":"State-of-the-Industry-OpenTable # State of the Industry | OpenTable # Created: March 21, 2020 5:38 PM URL: https://www.opentable.com/state-of-industry\n","date":"16 February 2023","permalink":"/posts/state-of-the-industry-opentable/","section":"Posts","summary":"State-of-the-Industry-OpenTable # State of the Industry | OpenTable # Created: March 21, 2020 5:38 PM URL: https://www.","title":"State-of-the-Industry-OpenTable"},{"content":"Storytelling-tips-for-technical-interviews # Given the importance of stories to the human mind, it seems there is no better way to explore it. You must assume the candidate will tell you the story you wish to hear; although most of the time, they won’t. Storytelling%20tips%20for%20technical%20interviews%201fa72afa1a72497cbfac10bb09a00fcd/worldbuilding.png Start the story at the point you think necessary, giving a recap on what happened until that point in which the story is taking place. By the time the candidate completes the story, from thousand possible paths, only one is going to remain explored. Most candidates don’t know how the story is supposed to advance and what places they need to explore. This gives the candidate time to adapt and be prepared to build the story further. But by the time the story is wrapping up, you should understand not only some of what the candidate knows, but you should also have connected with them at some level.\n","date":"16 February 2023","permalink":"/posts/storytelling-tips-for-technical-interviews/","section":"Posts","summary":"Storytelling-tips-for-technical-interviews # Given the importance of stories to the human mind, it seems there is no better way to explore it.","title":"Storytelling-tips-for-technical-interviews"},{"content":"Strategic-SEO-for-Startups-Kalzumeus-Software # Why Startup SEO Is Different # Essentially every business on the Internet from multi-billion dollar giants like Bank of America down to a one-man software business is dependent on SEO, because Google has become the primary navigation tool for the Internet. This is partially because search engine spam is indeed a problem and partially because Google is very good at influencing the culture of technically adept people, and it is in Google’s best interest to make people think that their algorithms are the authoritative voice of God.\nSEO Is A Feedback Loop # Sites tend to built self-reinforcing authority: the site at the top of the rankings for teddy bears (almost certainly Wikipedia, I can tell you without looking) is the first people go for teddy bears and the most likely to collect another citation when someone is writing about teddy bears. Strategies centering around user generated content really devolve into two things: one, you hope people will steal hand-crafted content from elsewhere and put it on your site while you look the other way long enough to build traction (hello, Youtube, Scribd, etc) and two, you generate vast amounts of mostly excruciatingly worthless content which happens to match an equally vast amount of search terms. Mass Semi-Amateur Content Creation: The Demand Media model is capturing quite a bit of attention these days: take an authority domain like eHow, use sophisticated algorithms to generate article ideas for it, pay an army of underemployed freelancers miniscule wages to write uninspired content about the suggested titles, collect hundreds of millions in AdSense revenue. Focus on evergreen content: A lot of people like blogs as content generation engines, and indeed, I think every startup should probably have a blog.\nGood SEO Can Make Your Startup # Your startup can succeed at SEO via the sweat of your brow and a bit of focused creativity, without having to spend hundreds of thousands to do so.\n","date":"16 February 2023","permalink":"/posts/strategic-seo-for-startups-kalzumeus-software/","section":"Posts","summary":"Strategic-SEO-for-Startups-Kalzumeus-Software # Why Startup SEO Is Different # Essentially every business on the Internet from multi-billion dollar giants like Bank of America down to a one-man software business is dependent on SEO, because Google has become the primary navigation tool for the Internet.","title":"Strategic-SEO-for-Startups-Kalzumeus-Software"},{"content":"styleguide-Style-guides-for-Google-originated-open # All new code should contain the following and existing code should be updated to be compatible when possible:\nfrom __future__ import absolute_import from __future__ import division from __future__ import print_function If you are not already familiar with those, read up on each here: absolute imports, new / division behavior, and the print function.\n2.21.1 Definition # Type annotations (or “type hints”) are for function or method arguments and return values:\ndef func(a: int) -\u0026gt; List[int]: You can also declare the type of a variable using a special comment:\na = SomeFunc() # type: SomeType 2.21.2 Pros # Type annotations improve the readability and maintainability of your code.\nimport collections import queue import sys from absl import app from absl import flags import bs4 import cryptography import tensorflow as tf from book.genres import scifi from myproject.backend.hgwells import time_machine from myproject.backend.state_machine import main_loop from otherproject.ai import body from otherproject.ai import mind from otherproject.ai import soul # Older style code may have these imports down here instead: #from myproject.backend.hgwells import time_machine #from myproject.backend.state_machine import main_loop 3.14 Statements # Generally only one statement per line.\nYes: if foo: bar(foo) No: if foo: bar(foo) else: baz(foo) try: bar(foo) except ValueError: baz(foo) try: bar(foo) except ValueError: baz(foo) 3.15 Accessors # If an accessor function would be trivial, you should use public variables instead of accessor functions to avoid the extra cost of function calls in Python. Example:\nfrom typing import List, TypeVar T = TypeVar(\u0026#34;T\u0026#34;) ... def next(l: List[T]) -\u0026gt; T: return l.pop() A TypeVar can be constrained:\nAddableType = TypeVar(\u0026#34;AddableType\u0026#34;, int, float, Text) def add(a: AddableType, b: AddableType) -\u0026gt; AddableType: return a + b A common predefined type variable in the typing module is AnyStr.\nfrom typing import Text, Union ... def py2_compatible(x: Union[bytes, Text]) -\u0026gt; Union[bytes, Text]: ... def py3_only(x: Union[bytes, str]) -\u0026gt; Union[bytes, str]: ... If all the string types of a function are always the same, for example if the return type is the same as the argument type in the code above, use AnyStr. Ex:\nfrom typing import Any, Dict, Optional Given that this way of importing from typing adds items to the local namespace, any names in typing should be treated similarly to keywords, and not be defined in your Python code, typed or not.\n","date":"16 February 2023","permalink":"/posts/styleguide-style-guides-for-google-originated-open/","section":"Posts","summary":"styleguide-Style-guides-for-Google-originated-open # All new code should contain the following and existing code should be updated to be compatible when possible:","title":"styleguide-Style-guides-for-Google-originated-open"},{"content":"Support-Vector-Machines-A-Visual-Explanation-with # Support Vector Machines: A Visual Explanation with Sample Python Code - YouTube # Created: July 13, 2020 11:14 PM URL: https://www.youtube.com/watch?v=N1vOgolbjSc https://www.youtube.com/watch?v=N1vOgolbjSc\n","date":"16 February 2023","permalink":"/posts/support-vector-machines-a-visual-explanation-with/","section":"Posts","summary":"Support-Vector-Machines-A-Visual-Explanation-with # Support Vector Machines: A Visual Explanation with Sample Python Code - YouTube # Created: July 13, 2020 11:14 PM URL: https://www.","title":"Support-Vector-Machines-A-Visual-Explanation-with"},{"content":"Syncing-Redshift-PostgreSQL-in-real-time-with-Ka # Before showing Kafka Connect, we’ll walk through some setup\nSetting up a distributed Kafka cluster Setting up a PostgreSQL database on AWS RDS Setting up an AWS Redshift instance Setting up Confluent’s open source platform If you’re curious about how Kafka Connect works, I highly recommend reading the concepts and architecture and internals of Kafka Connect on Confluent’s platform documentation. We can see the data in the table as below: Now that we have some data in our PostgreSQL table, we can use Kafka Connect to get these rows as messages in a Kafka topic and have a process listening for any inserts/updates on this table. timestamp.column.name: the column name which has the timestamps incrementing.column.name: the column which has incremental IDs topic.prefix: to identify the Kafka topics ingested from PostgreSQL we can specify a prefix value which will be appended to all the table names and the topic name will be prefix+table name The source-postgres.properties should look like this: Schema Registry # The JDBC connector from Confluent uses Schema Registry to store schema for the messages. We can start schema registry as follows:\n~$ /usr/local/confluent/bin/schema-registry-start /usr/local/confluent/etc/schema-registry/schema-registry.properties \u0026amp; Ingest data from PostgreSQL tables to Kafka topics # Let’s create a topic in which we want to consume the updates from PostgreSQL.\n~$ /usr/local/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic postgres_users You can check that the topic exists using the following command:\n~$ /usr/local/kafka/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic postgres_users We will be using Kafka Connect in stand-alone mode and we can start the stand-alone job to start consuming data from PostgreSQL table as follows:\n~$ /usr/local/confluent/bin/connect-standalone /usr/local/confluent/etc/schema-registry/connect-avro-standalone.properties /usr/local/confluent/etc/kafka-connect-jdbc/source-postgres.properties The jdbc connector serializes the data using Avro and we can use the Avro console consumer provided by Confluent to consume these messages from Kafka topic. Change the following properties:\nname: name for the connector topics: Kafka topic to write data from connection.url: JDBC endpoint for Redshift auto.create: it is true by default and we will change it to false as we’ve already created the table in Redshift that this data should be written to. The sink-redshift.properties should look as follows: Send messages from the Kafka topic to Redshift # We are all set to have messages from the Kafka topic write to the Redshift table.\n","date":"16 February 2023","permalink":"/posts/syncing-redshift-postgresql-in-real-time-with-ka/","section":"Posts","summary":"Syncing-Redshift-PostgreSQL-in-real-time-with-Ka # Before showing Kafka Connect, we’ll walk through some setup","title":"Syncing-Redshift-PostgreSQL-in-real-time-with-Ka"},{"content":"Technical-Interviews-Interesting-is-normally-more # Technical%20Interviews%20Interesting%20is%20normally%20more%20%2064050097a96c43ddbc9d3e07b4346cd2/Untitled-11-825x510.jpg We’ve already covered that technical interviews are often primarily about not being too weird and that you should really try and show your working when solving programming questions in interviews. When you’re being asked technical questions, the point is not seeing whether or not you know the right answer — the point is to get you to show off your technical skills. Here’s a plausible answer: “I’ve used MySQL for three years, I’ve used PostgreSQL, and I have used SQLite for testing” An answer that’s short and to the point. But most technical questions in an interview are simply exploratory, and exist solely to get you to try to show off your technical experience. And when you’re talking about technical problems you’ve solved in the past, or talking directly from your experience, you’re showing off your technical experience. When you stop talking, you’ll get asked another question designed to show off your technical experience, and perhaps it’ll be on a topic you’re not so comfortable on. If your monologuing about your experiences is boring them, or there’s something specific they want to talk about, they’ll cut you off and talk about it, but otherwise, keep segueing into other things you think they’d like to hear about, and that you’re good at talking about.\n","date":"16 February 2023","permalink":"/posts/technical-interviews-interesting-is-normally-more/","section":"Posts","summary":"Technical-Interviews-Interesting-is-normally-more # Technical%20Interviews%20Interesting%20is%20normally%20more%20%2064050097a96c43ddbc9d3e07b4346cd2/Untitled-11-825x510.","title":"Technical-Interviews-Interesting-is-normally-more"},{"content":"Technical-Writing-Google-Developers # Technical%20Writing%20Google%20Developers%2031c0e595e19845c0830cdc316fc97755/TechWritingCoursesLogo_2880.png Every engineer is also a writer. This collection of courses and learning resources aims to improve your technical documentation. You can also learn about the role of technical writers at Google. Technical%20Writing%20Google%20Developers%2031c0e595e19845c0830cdc316fc97755/StudyTW_2880.png ! Technical%20Writing%20Google%20Developers%2031c0e595e19845c0830cdc316fc97755/TWResources_2880.png We\u0026rsquo;ve aimed these courses at people in the following roles:\nprofessional software engineers computer science students engineering-adjacent roles, such as product managers You need at least a little writing proficiency in English, but you don\u0026rsquo;t need to be a strong writer to take these courses. These courses focus on technical writing, not on general English writing or business writing. Technical%20Writing%20Google%20Developers%2031c0e595e19845c0830cdc316fc97755/FindingInformation_2880.png You can find facilitator guides and related resources for each of the courses on this site. ","date":"16 February 2023","permalink":"/posts/technical-writing-google-developers/","section":"Posts","summary":"Technical-Writing-Google-Developers # Technical%20Writing%20Google%20Developers%2031c0e595e19845c0830cdc316fc97755/TechWritingCoursesLogo_2880.","title":"Technical-Writing-Google-Developers"},{"content":"Technical-Writing-One-introduction-Google-Develope # Technical Writing One introduction | Google Developers # Created: March 29, 2020 9:16 AM Tags: Communication, Technical writing URL: https://developers.google.com/tech-writing/one ! white.png Technical Writing One teaches you how to write clearer technical documentation.\nConvert passive voice sentences to active voice. Identify three ways in which active voice is superior to passive voice. This course will improve your technical writing but will not instantly transform you into a great technical writer. Pre-class and in-class components # The course consists of the following two components:\npre-class in-class You are currently viewing the start of the pre-class component. Optional units # We\u0026rsquo;ve marked a few units as optional.\n","date":"16 February 2023","permalink":"/posts/technical-writing-one-introduction-google-develope/","section":"Posts","summary":"Technical-Writing-One-introduction-Google-Develope # Technical Writing One introduction | Google Developers # Created: March 29, 2020 9:16 AM Tags: Communication, Technical writing URL: https://developers.","title":"Technical-Writing-One-introduction-Google-Develope"},{"content":"Tell-HN-How-to-run-a-startup-for-6-a-year-Hacker # Tell HN: How to run a startup for $6 a year | Hacker News # Created: July 20, 2020 2:17 AM URL: https://news.ycombinator.com/item?id=22354060 Use this stack. DynamoDB for database 2. AWS Lambda for backend 3. Netlify / Now / Surge for frontend 4. Cloudinary for image hosting 6. Porkbun for $6 .com domains 10. Cloudflare for DNS This setup is enough to handle ~1M/requests month, more or less, depending on the application.\n","date":"16 February 2023","permalink":"/posts/tell-hn-how-to-run-a-startup-for-6-a-year-hacker/","section":"Posts","summary":"Tell-HN-How-to-run-a-startup-for-6-a-year-Hacker # Tell HN: How to run a startup for $6 a year | Hacker News # Created: July 20, 2020 2:17 AM URL: https://news.","title":"Tell-HN-How-to-run-a-startup-for-6-a-year-Hacker"},{"content":"That-Time-I-Tried-to-Buy-an-Actual-Barrel-of-Crude # That Time I Tried to Buy an Actual Barrel of Crude Oil - Bloomberg # Created: April 20, 2020 3:53 PM URL: https://www.bloomberg.com/news/articles/2015-11-03/that-time-i-tried-to-buy-some-crude-oil\n","date":"16 February 2023","permalink":"/posts/that-time-i-tried-to-buy-an-actual-barrel-of-crude/","section":"Posts","summary":"That-Time-I-Tried-to-Buy-an-Actual-Barrel-of-Crude # That Time I Tried to Buy an Actual Barrel of Crude Oil - Bloomberg # Created: April 20, 2020 3:53 PM URL: https://www.","title":"That-Time-I-Tried-to-Buy-an-Actual-Barrel-of-Crude"},{"content":"The-Anatomy-of-a-Great-Decision # The Anatomy of a Great Decision # Created: February 21, 2020 12:17 PM URL: https://fs.blog/2019/04/decision-anatomy/ *Making better decisions is one of the best skills we can develop. In 1947, Secretary of State General George Marshall put forward a plan that has since carried his name, a plan to give a massive amount of money to several European nations. From there came the principles, things like: strong economies minimize social unrest; countries that work toward mutual goals are less likely to fight each other; let’s not have another war in Europe anytime soon. To invite Russia to partake\nUsing Multiple Lenses # The decision to give rather than lend the majority of the aid was the result of looking at the situation through economic, political, and humanitarian lenses.\nLetting the World Do the Work For You # The decision to have the participating nations allocate the aid among themselves was the answer to what the historical, political and psychological lenses revealed. Even though there was very little expectation that Russia would participate, and possibly even less desire to give them money, Russia and its allied countries were invited by both the US and the European nations to participate in the talks involving the implementation of the plan. The Marshall Plan is remembered as a great decision, not strictly because of its outcomes—though it did contribute to the debatably successful reconstruction of Europe, it did not succeed in preventing the deterioration of relations with Russia—but because it was firmly grounded in principles that were identified and executed through a multidisciplinary lens.\n","date":"16 February 2023","permalink":"/posts/the-anatomy-of-a-great-decision/","section":"Posts","summary":"The-Anatomy-of-a-Great-Decision # The Anatomy of a Great Decision # Created: February 21, 2020 12:17 PM URL: https://fs.","title":"The-Anatomy-of-a-Great-Decision"},{"content":"The-API-as-a-marketplace-Version-One # The API-as-a-marketplace - Version One # Created: May 28, 2020 10:17 AM URL: https://versionone.vc/api-as-a-marketplace/ As we’ve been spending time on B2B marketplaces and dev tools/platforms, we’ve come to realize that there’s an interesting sub-category that combines elements of both: the API-as-a-marketplace. Here are two examples from e-commerce use cases – both are V1 portfolio companies:\nShippo: developers can use this API to connect businesses to shipping carriers Patch: developers can use this API to connect businesses to carbon offset providers Note that we aren’t talking about API marketplaces here. API marketplaces (like RapidAPI) are traditional marketplaces that allow API providers to publish their APIs for developers to discover them. An API-as-a-marketplace is more of a dev tool that connects pools of suppliers and buyers and uses an API to connect the two sides. Choice, defensibility and simplicity** APIs are the building blocks of today’s digital world: developers use them to quickly integrate features, data, services and functions into their own apps, removing the need to build and scale all those elements from scratch themselves. For example, on Shippo, businesses may want to optimize the carrier (supplier) based on price, visibility on tracking, etc.. On Patch, businesses can choose a carbon offset provider based on the type of project (e.g. forestry) that most aligns with their business ethos. ** A data marketplace, which is an online store where people can buy data, is not necessarily an API-as-a-marketplace. ","date":"16 February 2023","permalink":"/posts/the-api-as-a-marketplace-version-one/","section":"Posts","summary":"The-API-as-a-marketplace-Version-One # The API-as-a-marketplace - Version One # Created: May 28, 2020 10:17 AM URL: https://versionone.","title":"The-API-as-a-marketplace-Version-One"},{"content":"The-Art-of-Bargaining-Positional-vs-Interest-Base # The Art of Bargaining, Positional vs Interest-Based Negotiation # Created: December 11, 2019 7:27 PM Tags: Negotiation, Self URL: https://f3fundit.com/the-art-of-bargaining-positional-vs-interest-based-negotiation/ ! We’ll cover the types of negotiation, and how to utilize them, specifically Positional vs Interest-Based Negotiation approaches. ** A “successful” negotiation in the advocacy approach is when the negotiator is able to obtain all or most of the outcomes their party desires, but without driving the other party to permanently break off negotiations.\nPart 1: Positional Bargaining # Positional bargaining is a negotiation strategy that involves holding on to a fixed idea, or position, of what you want, and arguing for it and it alone, regardless of any underlying interests. Therefore, positional bargaining is often considered a less constructive and less efficient strategy for negotiation than integrative negotiation.\nPart 2: Interest-based bargaining # The win/win negotiator’s approach or Interest-Based Bargaining # Integrative bargaining (also called “interest-based bargaining,” “win-win bargaining”) is a negotiation strategy in which parties collaborate to find a “win-win” solution to their dispute. Therefore, it is necessary when negotiating to convince the opposite party that they are getting a better deal than they really are, a person with low self-esteem will tend to push negotiations too far and to allow his own ego to dictate the course of negotiations.\n","date":"16 February 2023","permalink":"/posts/the-art-of-bargaining-positional-vs-interest-base/","section":"Posts","summary":"The-Art-of-Bargaining-Positional-vs-Interest-Base # The Art of Bargaining, Positional vs Interest-Based Negotiation # Created: December 11, 2019 7:27 PM Tags: Negotiation, Self URL: https://f3fundit.","title":"The-Art-of-Bargaining-Positional-vs-Interest-Base"},{"content":"The-Bash-Guide # The Bash Guide # Created: March 21, 2020 10:12 PM URL: https://guide.bash.academy/ This guide is an introduction to basic and advanced concepts of the bash shell. You can give back by providing feedback on the guide at the GitHub project page, making corrections by hitting the edit buttons on the chapter pages, sharing this guide with any of your friends that have similar interests so that they too may learn as you have, or contributing financially in gratitude for my work, time and effort put into writing and maintaining this guide. I\u0026rsquo;m done Commands And Arguments: How do I give bash instructions? beta About what a command is, and how to issue them; interactive mode and scripts; command syntax, searching commands and programs by name; arguments and word splitting as well as input and output redirection. alpha Bash parameters and variables; environment variables, special parameters and array parameters; expanding parameters, expansion operators, command substitution and process substitution; pathname expansion, tilde expansion and brace expansion. I\u0026rsquo;m done Colors And Terminal Commands: Advanced control over the terminal display. todo Terminals and terminal sequences, terminal identifiers, terminfo and terminal capabilities, outputting colors, moving the cursor and querying the terminal\u0026rsquo;s state.\n","date":"16 February 2023","permalink":"/posts/the-bash-guide/","section":"Posts","summary":"The-Bash-Guide # The Bash Guide # Created: March 21, 2020 10:12 PM URL: https://guide.","title":"The-Bash-Guide"},{"content":"The-Beergame-App # The Beergame App # Created: March 23, 2020 6:10 PM URL: https://beergameapp.com/\nSupply-Chain principles explained with a visual simulation # Multi-player # You may host sessions remotely, students can join wherever the are.Each user controls its own supply-chain stage : Retailer, Wholesaler, Distributor or Manufacturer.Missing some participants ? # The%20Beergame%20App%201f287e5c83fa4fe4b683f82558cbfbdc/mini-game-desktop.dc36976e.png Game Introduction ! The%20Beergame%20App%201f287e5c83fa4fe4b683f82558cbfbdc/mini-stage-desktop.65e38298.png Stage\nPlay on Mobile # Students can simply open the web page on their phone to play. # The design will adapt to fit the screen nicely. Game Introduction Stage\nInstructor Dashboard # You need to sign up in order to create new games. # The%20Beergame%20App%201f287e5c83fa4fe4b683f82558cbfbdc/mini-dashboard-mygames.b24f28b3.png My Games ! The%20Beergame%20App%201f287e5c83fa4fe4b683f82558cbfbdc/mini-dashboard-newgame.1ddff78c.png New Game\nAnalytics and Debriefing material # Follow your performance data during the game. # ","date":"16 February 2023","permalink":"/posts/the-beergame-app/","section":"Posts","summary":"The-Beergame-App # The Beergame App # Created: March 23, 2020 6:10 PM URL: https://beergameapp.","title":"The-Beergame-App"},{"content":"The-contribution-of-neural-networks-and-genetic-al # The contribution of neural networks and genetic algorithms to business decision support # Created: November 29, 2019 8:50 PM Status: Finished Tags: Data, Finance URL: https://www.emeraldinsight.com/doi/abs/10.1108/00251740410518534 Author(s):Abstract:Managing large amounts of information and efficiently using this information in improved decision making has become increasingly challenging as businesses collect terabytes of data. Intelligent solutions, based on neural networks (NNs) and genetic algorithms (GAs), to solve complicated practical problems in various sectors are becoming more and more widespread nowadays. The current study provides an overview for the operations researcher of the neural networks and genetic algorithms methodology, as well as their historical and current use in business. The main aim is to present and focus on the wide range of business areas of NN and GA applications, avoiding an in‐depth analysis of all the applications – with varying success – recorded in the literature. This review reveals that, although still regarded as a novel methodology, NN and GA are shown to have matured to the point of offering real practical benefits in many of their applications.Keywords: Type: Literature reviewPublisher: Emerald Group Publishing LimitedCopyright:Citation: Kostas Metaxiotis, John Psarras, (2004) \u0026ldquo;The contribution of neural networks and genetic algorithms to business decision support: Academic myth or practical solution? \u0026ldquo;, Management Decision, Vol. 42 Issue: 2, pp.229-242, https://doi.org/10.1108/00251740410518534Downloads: The fulltext of this document has been downloaded 1671 times since 2012\n","date":"16 February 2023","permalink":"/posts/the-contribution-of-neural-networks-and-genetic-al/","section":"Posts","summary":"The-contribution-of-neural-networks-and-genetic-al # The contribution of neural networks and genetic algorithms to business decision support # Created: November 29, 2019 8:50 PM Status: Finished Tags: Data, Finance URL: https://www.","title":"The-contribution-of-neural-networks-and-genetic-al"},{"content":"The-Cookbook-ANDREAS-KRETZ # The Cookbook | ANDREAS KRETZ # Created: April 15, 2020 12:30 PM URL: https://andreaskretz.com/the-cookbook/\n","date":"16 February 2023","permalink":"/posts/the-cookbook-andreas-kretz/","section":"Posts","summary":"The-Cookbook-ANDREAS-KRETZ # The Cookbook | ANDREAS KRETZ # Created: April 15, 2020 12:30 PM URL: https://andreaskretz.","title":"The-Cookbook-ANDREAS-KRETZ"},{"content":"The-COVID-Tracking-Project-Homepage # card.png The COVID Tracking Project collects information from 50 US states, the District of Columbia, and 5 other US territories to provide the most comprehensive testing data we can collect for the novel coronavirus, SARS-CoV-2. We attempt to include positive and negative results, pending tests, and total people tested for each state or district currently reporting that data. From here, you can also learn about our methodology, see who makes this, and find out what information states provide and how we handle it.\nBest Practices for Data Release # State Testing Data Release - Best Practices—Our recommendations on what data state public health authorities should be releasing, and how.\nIn the Press # Comparing COVID-19 in Minnesota and its neighbors in Upper Midwest, Minnesota Public Radio, 23 March 2020 Coronavirus cases top 300,000 worldwide as US becomes one of worst hit countries, CNBC, 21 March 2020 Coronavirus Deaths Spike Abroad As New York City Becomes U.S. Virus Epicenter, NPR, 21 March 2020 Huge Testing Discrepancies Among States Muddles the Meaning of Results, Washington Post, 20 March 2020 U.S. Lags in Coronavirus Testing After Slow Response to Outbreak, The New York Times, 17 March 2020 Live tracker: How many coronavirus cases have been reported in each U.S. , Politico, 16 March 2020 The 4 Key Reasons the U.S. Is So Behind on Coronavirus Testing, The Atlantic, 14 March 2020 America Isn’t Testing for the Most Alarming Coronavirus Cases, The Atlantic, 13 March 2020 Mike Pence and the Farce of Trusting Donald Trump on the Coronavirus, The New Yorker, 13 March 2020 ","date":"16 February 2023","permalink":"/posts/the-covid-tracking-project-homepage/","section":"Posts","summary":"The-COVID-Tracking-Project-Homepage # card.","title":"The-COVID-Tracking-Project-Homepage"},{"content":"The-data-model-behind-Notion-s-flexibility # Notion supports many types of blocks, most of which you can see in the “new block” menu that appears when you press the + button or in the / menu: In addition to the attributes that describe the block itself, every block has attributes that define their relationship with other blocks:\nContent — an array (or ordered set) of block IDs representing the content inside this block, like nested bullet items in a bulleted list or the text inside a toggle. The block type is what specifies how the block is rendered in Notion’s UI — and depending on that type, we interpret the block’s properties and content differently. The “checked” property of the To-do list block is ignored when the block is transformed into Heading and Callout block types — but by the time we come full circle to turn the block back into a To-do list block, it is still checked. In the to-do list example, we have a To-do list block (“Write a blog post about blocks”) with three block IDs in its content array. We think of these IDs as “downward pointers,” and call the blocks that they refer to “content” or “render children.” Each block defines the position and order in which its content blocks are rendered. For example, pressing indent in a content block tries to add that block to the content of the nearest sibling block in the content tree. The API method for loading the data for a page is called loadPageChunk — it descends from a starting point (likely the block ID of a page block) down the content tree, and returns the blocks in the content tree plus any dependent records needed to properly render those blocks. ","date":"16 February 2023","permalink":"/posts/the-data-model-behind-notion-s-flexibility/","section":"Posts","summary":"The-data-model-behind-Notion-s-flexibility # Notion supports many types of blocks, most of which you can see in the “new block” menu that appears when you press the + button or in the / menu: In addition to the attributes that describe the block itself, every block has attributes that define their relationship with other blocks:","title":"The-data-model-behind-Notion-s-flexibility"},{"content":"The-five-minute-feedback-fix # Overview # The core idea of formal specifications is simple: Create a blueprint of your design, write some properties the blueprint should maintain, and test that you have those properties.\nDecision tables # Decision tables are a way of specifying choices your system can make in a given context. That’s where the formal part of the formal specification comes in: If we don’t have exactly six rows, we immediately know the table is somehow wrong. For simplicity, we’ll assume I only need to implement just the first two options, “How many participants can share at the same time?” and “Who can share?” Under what cases can I share my screen? There are four possible inputs in our decision table:\nCan only the host share? Here, the invariant is *If \u0026ldquo;only the host can share” is enabled then someone who isn’t the host cannot share. That means there’s no missing rows indicating a missing requirement, and no duplicate rows indicating a requirement contradiction. ","date":"16 February 2023","permalink":"/posts/the-five-minute-feedback-fix/","section":"Posts","summary":"The-five-minute-feedback-fix # Overview # The core idea of formal specifications is simple: Create a blueprint of your design, write some properties the blueprint should maintain, and test that you have those properties.","title":"The-five-minute-feedback-fix"},{"content":"The-four-priorities-for-an-analytics-team-of-one-l # At the start of his tenure at Lola, Sagar laid out his analytics strategy, a 10-page document that described his plan to make Lola a “lean, fact-driven organization.” His plan described an approach that started with enabling business users to self-serve in the near-term–meeting the data needs of the organization today–while he implemented an analytics engineering workflow that would scale to meet future data needs.\nPriority #1: Enable business users to understand what is happening in their department today # Sagar took inspiration from Carl Anderson’s book, Creating a Data Driven Organization, which outlines the six types of questions that can be answered using analytics: ! Instead of spending 80% of my time cleaning data, I spend my time building tools that enable business users to do it themselves, and generating real insights that can help scale the business.” So instead of churning out monthly Excel reports, Sagar’s job is now to: 1.\nPriority #3: Empower business users to do their own data exploration # In Sagar’s strategy doc he wrote: “Eventually, the organization will need to explore data and deliver insights at a rate that isn’t possible if all queries must filter through a centralized analytics team. Looker is currently the best business intelligence platform on the market that enables analysts to build data tools for business users, and enable them to support themselves.” In Sagar’s view, it makes sense for analysts to build some reporting for business users, but long term, “Our goal is to have each vice president be able to do their own data work. “As an analyst, this is a huge relief.”\nPriority #4: Plan for scale # Even with best-in-class technology and code-based processes, the analytics needs of the organization will eventually grow to a point where “it becomes important to optimize for cost and efficiency.” When Lola reaches that stage, Sagar says that’s when it makes sense to hire data engineers, data scientists, and other business intelligence professionals. A few tips:\nStaging and mart models: Sagar follows the convention of using staging models for data cleaning, and marts for storing the business logic of a given business function. ","date":"16 February 2023","permalink":"/posts/the-four-priorities-for-an-analytics-team-of-one-l/","section":"Posts","summary":"The-four-priorities-for-an-analytics-team-of-one-l # At the start of his tenure at Lola, Sagar laid out his analytics strategy, a 10-page document that described his plan to make Lola a “lean, fact-driven organization.","title":"The-four-priorities-for-an-analytics-team-of-one-l"},{"content":"The-Hardest-Lessons-for-Startups-to-Learn # The Hardest Lessons for Startups to Learn # Created: March 28, 2020 9:21 AM URL: http://www.paulgraham.com/startuplessons.html Untitled\n","date":"16 February 2023","permalink":"/posts/the-hardest-lessons-for-startups-to-learn/","section":"Posts","summary":"The-Hardest-Lessons-for-Startups-to-Learn # The Hardest Lessons for Startups to Learn # Created: March 28, 2020 9:21 AM URL: http://www.","title":"The-Hardest-Lessons-for-Startups-to-Learn"},{"content":"The-Knapsack-Problem-in-Computer-Science-Explained # The Knapsack Problem in Computer Science Explained | Science | Smithsonian Magazine # Created: March 9, 2020 8:30 PM URL: https://www.smithsonianmag.com/science-nature/why-knapsack-problem-all-around-us-180974333/ ! istock-1152723691.jpg\n","date":"16 February 2023","permalink":"/posts/the-knapsack-problem-in-computer-science-explained/","section":"Posts","summary":"The-Knapsack-Problem-in-Computer-Science-Explained # The Knapsack Problem in Computer Science Explained | Science | Smithsonian Magazine # Created: March 9, 2020 8:30 PM URL: https://www.","title":"The-Knapsack-Problem-in-Computer-Science-Explained"},{"content":"The-Most-Underutilized-Function-in-SQL # #1: Building Yourself a Unique ID # I’m going to make a really strong statement here, but it’s one that I really believe in: every single data model in your warehouse should have a rock solid unique ID. One reason is that your source data doesn’t have a unique key—if you’re syncing advertising performance data from Facebook Ads via Stitch or Fivetran, the source data in your ad_insights table doesn’t have a unique key you can rely on. Using that knowledge, you can build yourself a unique id using md5():\nselect md5(date_start::varchar || ad_id::varchar) as insight_id from stitch_fb_ads.facebook_ads_insights limit 5; insight_id ---------------------------------- 6d475ea96f23b097b51ed500116d8c5e 822c9429eabb28ccbcd7286836d7cd60 8b7fcd2aff879772ccac4f0f8bcb6a45 8a2cfd7eb1a723c49db47232e73ca29c 10338719dfadb3d4c9d44c608063998a (5 rows) The resulting hash is a meaningless string of alphanumeric text that functions as a unique identifier for your record. Of course, you could just as easily just create a single concatenated varchar field that performed the same function, but it’s actually important to obfuscate the underlying logic behind the hash: you will innately treat the field differently if it looks like an id versus if it looks like a jumble of human-readable text. We create unique keys for every table and then test uniqueness on this key using dbt schema tests. You have the same Facebook Ads dataset as referenced earlier but this time you have a new challenge: join that data to data in your web analytics sessions table so that you can calculate Facebook ROAS. Here’s a code snippet from a client project where we did exactly this: You can see that this code is actually building the id on top of even more fields: in this example we’re actually unioning together advertising spend data from 7 different ad channels, and the data from Bing and Adwords is identified by ad_group_id and keyword_id instead of by UTM parameters.\n","date":"16 February 2023","permalink":"/posts/the-most-underutilized-function-in-sql/","section":"Posts","summary":"The-Most-Underutilized-Function-in-SQL # #1: Building Yourself a Unique ID # I’m going to make a really strong statement here, but it’s one that I really believe in: every single data model in your warehouse should have a rock solid unique ID.","title":"The-Most-Underutilized-Function-in-SQL"},{"content":"The-Number-One-Goal-is-Getting-Started-Avni-Pate # The Number One Goal is Getting Started - Avni Patel Thompson of Poppy - YouTube # Created: March 28, 2020 9:23 AM URL: https://www.youtube.com/watch?v=EOw6izSfzSA https://www.youtube.com/watch?v=EOw6izSfzSA\n","date":"16 February 2023","permalink":"/posts/the-number-one-goal-is-getting-started-avni-pate/","section":"Posts","summary":"The-Number-One-Goal-is-Getting-Started-Avni-Pate # The Number One Goal is Getting Started - Avni Patel Thompson of Poppy - YouTube # Created: March 28, 2020 9:23 AM URL: https://www.","title":"The-Number-One-Goal-is-Getting-Started-Avni-Pate"},{"content":"The-Unix-Shell # The Unix Shell # Created: March 22, 2020 12:46 AM URL: http://swcarpentry.github.io/shell-novice/ The Unix shell has been around longer than most of its users have been alive. It has survived so long because it’s a power tool that allows people to do complex things with just a few keystrokes. More importantly, it helps them combine existing programs in new ways and automate repetitive tasks so they aren’t typing the same things over and over again. Use of the shell is fundamental to using a wide range of other powerful tools and computing resources (including “high-performance computing” supercomputers).\nPrerequisites This lesson guides you through the basics of file systems and the shell. If you have stored files on a computer at all and recognize the word “file” and either “directory” or “folder” (two common words for the same thing), you’re ready for this lesson. If you’re already comfortable manipulating files and directories, searching for files with grep and find, and writing simple loops and scripts, you probably want to explore the next lesson: shell-extras.\n","date":"16 February 2023","permalink":"/posts/the-unix-shell/","section":"Posts","summary":"The-Unix-Shell # The Unix Shell # Created: March 22, 2020 12:46 AM URL: http://swcarpentry.","title":"The-Unix-Shell"},{"content":"The-unreasonable-importance-of-data-preparation # When executives ask me how to approach an AI transformation, I show them Monica Rogati’s AI Hierarchy of Needs, which has AI at the top, and everything is built upon the foundation of data (Rogati is a data science and AI advisor, former VP of data at Jawbone, and former LinkedIn data scientist): !\nData professionals spend an inordinate amount on time cleaning, repairing, and preparing data # Before you even think about sophisticated modeling, state-of-the-art machine learning, and AI, you need to make sure your data is ready for analysis—this is the realm of data preparation. No data analysts/scientists work on this data pipeline as everything must happen in real time, requiring an automated data preparation and data quality workflow (e.g., to resolve if I say “eye-talian” instead of “it-atian”). Understanding the importance of general automation and democratization of all parts of the DS/ML/AI workflow, it’s important to recognize that we’ve done pretty well at democratizing data collection and gathering, modeling[8], and data reporting[9], but what remains stubbornly difficult is the whole process of preparing the data.\nModern tools for automating data cleaning and data preparation # We’re seeing the emergence of modern tools for automated data cleaning and preparation, such as HoloClean and Snorkel coming from Christopher Ré’s group at Stanford. HoloClean decouples the task of data cleaning into error detection (such as recognizing that the location “cicago” is erroneous) and repairing erroneous data (such as changing “cicago” to “Chicago”), and formalizes the fact that “data cleaning is a statistical learning and inference problem.” All data analysis and data science work is a combination of data, assumptions, and prior knowledge. Snorkel provides a way to automate labeling, using a modern paradigm called data programming, in which users are able to “inject domain information [or heuristics] into machine learning models in higher level, higher bandwidth ways than manually labeling thousands or millions of individual data points.” Researchers at Google AI have adapted Snorkel to label data at industrial/web scale and demonstrated its utility in three scenarios: topic classification, product classification, and real-time event classification.\n","date":"16 February 2023","permalink":"/posts/the-unreasonable-importance-of-data-preparation/","section":"Posts","summary":"The-unreasonable-importance-of-data-preparation # When executives ask me how to approach an AI transformation, I show them Monica Rogati’s AI Hierarchy of Needs, which has AI at the top, and everything is built upon the foundation of data (Rogati is a data science and AI advisor, former VP of data at Jawbone, and former LinkedIn data scientist): !","title":"The-unreasonable-importance-of-data-preparation"},{"content":"This-entire-thread-is-the-most-amazing-Rorsach-tes # The first issue worth noting is that the definition of \u0026rsquo;employed\u0026rsquo;(not discussed in the article or the original paper) vs self-employed incorporated vs self-employed non-incorporated are based on self-reporting from the census [5]: \u0026ldquo;CURRENT OR MOST RECENT JOB ACTIVITY. If this person had no job or business last week, give information for his/her last job or business. Once you understand where the data came from, you might not infer the same meaning to the underlying paper, whose abstract is here: \u0026ldquo;We disaggregate the self-employed into incorporated and unincorporated to distinguish between \u0026ldquo;entrepreneurs\u0026rdquo; and other business owners. We show that the incorporated self-employed and their businesses engage in activities that demand comparatively strong nonroutine cognitive abilities, while the unincorporated and their firms perform tasks demanding relatively strong manual skills. Let\u0026rsquo;s see if that makes sense, and if their \u0026rsquo;entrepreneur\u0026rsquo; is the same as what we might call a \u0026lsquo;startup founder\u0026rsquo;:\nThe ratio of unincorporated to incorporated is 2:1 (approx 10 million vs 5 million in 2009 [1]) The SBA says 600,000 new ventures are started each year (that they know about) [2] There are around 70000 angel deals in one year (2013) [3] There are about 4000 venture deals in one year (2014) [4] We can probably agree that the angel and venture deals are \u0026lsquo;real\u0026rsquo; startup entrepreneurs. However, doesn\u0026rsquo;t it seem a little silly to analyze the 5 million incorporated self-employed vs the 10 million non-incorporated self-employed, and assume it\u0026rsquo;s telling you anything about start-up entrepreneurs ? [1] http://www.bls.gov/opub/mlr/2010/09/art2full.pdf [2] https://www.sba.gov/sites/default/files/FAQ_Sept_2012.pdf [3] http://www.angelcapitalassociation.org/data/Documents/Resour\u0026hellip; [4] http://nvca.org/?ddownload=1868 (2015 NVCA Yearbook) [5] http://www2.census.gov/programs-surveys/acs/methodology/ques\u0026hellip; Edited to fix formatting issues ","date":"16 February 2023","permalink":"/posts/this-entire-thread-is-the-most-amazing-rorsach-tes/","section":"Posts","summary":"This-entire-thread-is-the-most-amazing-Rorsach-tes # The first issue worth noting is that the definition of \u0026rsquo;employed\u0026rsquo;(not discussed in the article or the original paper) vs self-employed incorporated vs self-employed non-incorporated are based on self-reporting from the census [5]: \u0026ldquo;CURRENT OR MOST RECENT JOB ACTIVITY.","title":"This-entire-thread-is-the-most-amazing-Rorsach-tes"},{"content":"Tiny-Beautiful-Things # Tiny Beautiful Things # Created: March 22, 2020 12:49 PM URL: https://fs.blog/2014/11/cheryl-strayed-tiny-beautiful-things/ She claimed she would offer a combination of “the by-the-book common sense of Dear Abby and the earnest spiritual cheesiness of Cary Tennis and the butt-pluggy irreverence of Dan Savage and the closeted Upper East Side nymphomania of Miss Manners.” It became clear after a while that she was an advice columnist unlike others: intimate and frank, dispensing advice built on a foundation drawn of deep personal experience. In a way Sugar’s advice columns — combined into the amazing collection Tiny Beautiful Things: Advice on Love and Life from Dear Sugar — represents an ad hoc memoir. “But it’s a memoir with an agenda,” Strayed’s friend Steve Almond writes in the introduction, “With great patience, and eloquence, (Sugar) assures her readers that within the chaos of our shame and disappointment and rage there is meaning, and within that meaning is the possibility of rescue.”\nInexplicable sorrows await all of us. The sort of people worthy of your love will love you more for this, sweet pea. Leaving doesn’t mean you’re incapable of real love or that you’ll never love anyone else again. One Christmas at the very beginning of your twenties when your mother gives you a warm coat that she saved for months to buy, don’t look at her skeptically after she tells you she thought the coat was perfect for you. Yours, Sugar\n“Tiny Beautiful Things will endure as a piece of literary art,” Almond writes, “as will Cheryl’s other books (Torch and Wild), because they do the essential work of literary art: they make us more human than we were before.”\n","date":"16 February 2023","permalink":"/posts/tiny-beautiful-things/","section":"Posts","summary":"Tiny-Beautiful-Things # Tiny Beautiful Things # Created: March 22, 2020 12:49 PM URL: https://fs.","title":"Tiny-Beautiful-Things"},{"content":"Tips-to-avoid-burnout # Tips to avoid burnout # Created: December 29, 2019 10:17 AM Tags: How To, Productivity URL: https://news.ycombinator.com/item?id=21904549\nBurn out happens when - the company or team is under duress, imagine running a manufacturing software system and your factory lines are down. But, that\u0026rsquo;s not because of the reasons listed here. I wouldn\u0026rsquo;t say that\u0026rsquo;s the case at all. The first time I struggled with burnout was because the organisation I worked within was built in such a way that the rank and file are irrelevant and take some sort of grim pleasure in being low status. In that situation there doesn\u0026rsquo;t have to be any crisis or external pressure. The further I get away from that, the happier I am. IME the company, your life, whatever, can be falling apart as long as you know that those around you are actually playing the same game as you and care about the outcome.\n","date":"16 February 2023","permalink":"/posts/tips-to-avoid-burnout/","section":"Posts","summary":"Tips-to-avoid-burnout # Tips to avoid burnout # Created: December 29, 2019 10:17 AM Tags: How To, Productivity URL: https://news.","title":"Tips-to-avoid-burnout"},{"content":"Tradeshift-raises-240M-and-appears-to-put-its-exp # Tradeshift%20raises%20$240M%20and%20appears%20to%20put%20its%20exp%20a554111d847b4eb08246034ac92257e3/tradeshift.png Tradeshift — the startup which set out to disrupt the traditional arena of supply chain payments and marketplaces when it first appeared in 2008 — has today announced a new funding round of $240 million in equity and debt, raised from a combination of existing and new investors. The funding will be used to help accelerate its growth and, it says, set the company “on a direct path to profitability in the near future.” That last line is telling, as the new funding comes in the context of what was widely held to be a window of opportunity for Tradeshift to head toward an IPO. What this new funding means it that Tradeshift is effectively delaying its IPO to get its “house in order” in the context of a new economic environment that has become skeptical toward tech IPOs in the wake of the WeWork debacle, which saw public investors cool toward new tech company listings. In a statement, the company said it has reported more than two years of strong growth in quarterly revenue, and recorded its best-ever year in 2019, including more than 60% revenue growth, with more than 250 deals closed (the average deal size was doubled). Tradeshift said the additional capital will be used to further momentum it has seen across core product lines, including Tradeshift Pay, which was ranked in 2019 as the strongest ePayables SaaS solution in the industry by analyst firm Ardent Partners, and Tradeshift Go, with more than 200 new customers signed in 2019. But it’s also important that we manage that growth responsibly.” I asked him what he meant by “networked.” Lang believes we are moving “from cloud businesses to networked businesses,” where, instead of companies, like Microsoft having one single solution but also offering a variety of other products (such as LinkedIn and Skype), people and businesses will opt more for single-use tools. Previous investors in the company have included Goldman Sachs, the Public Sector Pension Investment Board (PSP Investments), HSBC, H14, GP Bullhound, Gray Swan, a venture company established by Tradeshift’s founders, American Express Ventures, the CreditEase Fintech Investment Fund, Notion Capital, Santander InnoVentures and others.\n","date":"16 February 2023","permalink":"/posts/tradeshift-raises-240m-and-appears-to-put-its-exp/","section":"Posts","summary":"Tradeshift-raises-240M-and-appears-to-put-its-exp # Tradeshift%20raises%20$240M%20and%20appears%20to%20put%20its%20exp%20a554111d847b4eb08246034ac92257e3/tradeshift.","title":"Tradeshift-raises-240M-and-appears-to-put-its-exp"},{"content":"Two-lessons-on-reducing-sign-up-friction # Two lessons on reducing sign-up friction # Created: May 13, 2020 11:49 AM URL: https://bbirnbaum.com/two-lessons-on-reducing-sign-up-friction/ photo-1533582437341-dfdc01630b05\n","date":"16 February 2023","permalink":"/posts/two-lessons-on-reducing-sign-up-friction/","section":"Posts","summary":"Two-lessons-on-reducing-sign-up-friction # Two lessons on reducing sign-up friction # Created: May 13, 2020 11:49 AM URL: https://bbirnbaum.","title":"Two-lessons-on-reducing-sign-up-friction"},{"content":"Two-methods-that-have-helped-me-Have-an-ideas-jour # Two methods that have helped me: * Have an ideas journal. # Write new ideas down \u0026hellip; | Hacker News Created: May 10, 2020 11:09 AM URL: https://news.ycombinator.com/item?id=23133502 Two methods that have helped me:\nHave an ideas journal. Write new ideas down there, and don\u0026rsquo;t start with any of them in less than two weeks. This lets you get over the initial enthusiasm - and perhaps new better ideas will push less useful ones out of the way in that time. If you are having trouble deciding between a small number of fixed options, roll the dice. The very fact that you are having trouble deciding means that (within the information available to you right now) all choices are equally good. And sometimes when you see the dice rolling and know that the decision will happen now you realise which one you want. ","date":"16 February 2023","permalink":"/posts/two-methods-that-have-helped-me-have-an-ideas-jour/","section":"Posts","summary":"Two-methods-that-have-helped-me-Have-an-ideas-jour # Two methods that have helped me: * Have an ideas journal.","title":"Two-methods-that-have-helped-me-Have-an-ideas-jour"},{"content":"Understanding-C-Store-Columnar-Databases-and-Data # Understanding C-Store: Columnar Databases and Data Intensive Analytics # Created: April 20, 2020 8:26 AM URL: https://medium.com/@aakashpydi/understanding-c-store-columnar-databases-and-big-data-analytics-aa669bb60f0 ! 1*xNYKI1azVyfzhXB-cIvKUw.png From what I understand, the first prototype for a columnar database was introduced in a 2005 paper from MIT’s school of computer science and artificial intelligence, called C-Store: A Column Oriented Database. The key idea from this paper seems to be that, in columnar databases we store each column’s values sequentially in a database, as opposed to storing each row’s values sequentially.\nNote that a data block is the smallest unit of data used by a database. This is a ‘read optimized’ database, particularly in a big data analytics context (which is what we use the large datasets in our columnar database for). The original CSAIL paper notes that write optimized database systems are naturally ideal write heavy applications (such as Online Transaction Processing applications) whereas read optimized database systems are naturally ideal for read heavy applications (such as Data Warehouses). My Director, Dan Woicke wrote about the value of using a columnar database to our team’s work here: [Cerner Advances Big Data Analytic Capabilities. ","date":"16 February 2023","permalink":"/posts/understanding-c-store-columnar-databases-and-data/","section":"Posts","summary":"Understanding-C-Store-Columnar-Databases-and-Data # Understanding C-Store: Columnar Databases and Data Intensive Analytics # Created: April 20, 2020 8:26 AM URL: https://medium.","title":"Understanding-C-Store-Columnar-Databases-and-Data"},{"content":"Understanding-SaaS-Why-the-Pundits-Have-It-Wrong # Take a look at the cumulative cash flow for a single customer under a SaaS model — the company doesn’t even break even on that customer until after a year: Company A, which is spending $6,000 to acquire the customer and billing them at a rate of $500 per month, doesn’t break even on the customer until month 13. ** Because once a SaaS company has generated enough cash from its installed customer base to cover the cost of acquiring new customers, those customers stay for a long time. While this is a good measure to understand a company’s ability to satisfy and retain its customers, it is more telling to look at revenue churn, the % of revenue lost due to churned customers as a % of total recurring revenue. There are several ways that a company can offset or overcome churn: add new customers at a faster and faster pace; have “negative churn” (which happens when expansion revenue is larger than the revenue lost from churned customers); and reduce churn itself — that is, retain customers! As the company starts to recognize revenue from the SaaS service, it reduces its deferred revenue balance and increases revenue — so for a 24-month deal, as each month goes by deferred revenue drops by 1/24th and revenue increases by 1/24th. Billings is a much better forward-looking indicator of the health of a SaaS company than simply looking at revenue for two reasons: (1) Revenue understates the true value of the customer because it gets recognized ratably; and (2) Because of the recurring nature of revenue, a SaaS company could show stable revenue for a long time (just by working off its billings backlog) which could make the business seem healthier than it truly is. CAC/LTV Workday does not disclose customer acquisition costs, so the proxy we used to get to CAC was sales and marketing spend for new customers: Assumes 70% of total sales and marketing spend for the year is for new customer acquisition, divided by the total new customers in the year.\n","date":"16 February 2023","permalink":"/posts/understanding-saas-why-the-pundits-have-it-wrong/","section":"Posts","summary":"Understanding-SaaS-Why-the-Pundits-Have-It-Wrong # Take a look at the cumulative cash flow for a single customer under a SaaS model — the company doesn’t even break even on that customer until after a year: Company A, which is spending $6,000 to acquire the customer and billing them at a rate of $500 per month, doesn’t break even on the customer until month 13.","title":"Understanding-SaaS-Why-the-Pundits-Have-It-Wrong"},{"content":"Universal-Palindrome-Day-Susam-Pal # Universal Palindrome Day - Susam Pal # Created: February 3, 2020 9:12 AM URL: https://susam.in/blog/universal-palindrome-day/ There are three popular date formats followed worldwide: big-endian, little-endian, and middle-endian. Here is how today\u0026rsquo;s date looks like in these three formats if the full year is written: The ISO 8601 standard specifies the big-endian date format. The little-endian date format is popular in a huge number of countries including Germany, United Kingdom, India, Italy, etc. The middle-endian date format is primarily popular in the United States. Today\u0026rsquo;s date is a palindrome in all three date formats. If we define universal palindrome day to be a day when its date is a palindrome in all three date formats (YYYY-MM-DD, DD-MM-YYYY, and MM-DD-YYYY), then today is a universal palindrome day! Here is a list of all universal palindrome days between the years 1000 and 9999:\n1010-01-01 1111-11-11 2020-02-02 2121-12-12 3030-03-03 4040-04-04 5050-05-05 6060-06-06 7070-07-07 8080-08-08 9090-09-09 The previous universal palindrome day was over 908 years ago! ","date":"16 February 2023","permalink":"/posts/universal-palindrome-day-susam-pal/","section":"Posts","summary":"Universal-Palindrome-Day-Susam-Pal # Universal Palindrome Day - Susam Pal # Created: February 3, 2020 9:12 AM URL: https://susam.","title":"Universal-Palindrome-Day-Susam-Pal"},{"content":"Unlikely-Optimism-The-Conjunctive-Events-Bias # The probability of a series of conjunctive events happening is lower than the probability of any individual event. The majority of students (85% to 95%) chose the latter statement, seeing the conjunctive events (that she is both a bank teller and a feminist activist) as more probable.\nWhy the best laid plans often fail # The conjunctive events bias makes us underestimate the effort required to accomplish complex plans. As Max Bazerman and Don Moore explain in Judgment in Managerial Decision Making, “The overestimation of conjunctive events offers a powerful explanation for the problems that typically occur with projects that require multistage planning. What is more likely:\nThe building permits get delayed The building permits get delayed and the electrical goes in on schedule You know a bit about the electrical schedule. Humans make mistakes, equipment fails, technologies don’t work as planned, unrealistic expectations, biases including sunk cost-syndrome, inexperience, wrong incentives, changing requirements, random events, ignoring early warning signals are reasons for delays, cost overruns, and mistakes. In the housing example above, asking what is the frequency of having building permits delayed in every hundred houses, versus the frequency of having permits delayed and electrical going in on time for the same hundred demonstrates more easily the higher frequency of option one. ","date":"16 February 2023","permalink":"/posts/unlikely-optimism-the-conjunctive-events-bias/","section":"Posts","summary":"Unlikely-Optimism-The-Conjunctive-Events-Bias # The probability of a series of conjunctive events happening is lower than the probability of any individual event.","title":"Unlikely-Optimism-The-Conjunctive-Events-Bias"},{"content":"Unsure-Calculator # Unsure Calculator # Created: April 12, 2020 7:07 PM URL: https://filiph.github.io/unsure/\nCalculate with numbers you’re not sure about # Hi, I\u0026rsquo;m Filip, and I\u0026rsquo;d like to introduce to you an early version of an uncertainty calculator. The range notation says the following to the calculator: *I am not sure about the exact number here, but I am 95% sure it\u0026rsquo;s somewhere in this range. *\u0026ldquo;Well, I don\u0026rsquo;t know this number exactly, so I\u0026rsquo;ll just pick the first number that seems plausible and calculate with that. Unsure%20Calculator%20184175d7166640d99984c3ba269dc2d4/shrug.png Unsure Calculator to the rescue! Unsure%20Calculator%20184175d7166640d99984c3ba269dc2d4/alien.png\nOther use cases # Here are some ideas of how to use this calculator and its notation. For example: 1000~1500 x 10~12 x (30~50 / 100)\nEstimate time saved by a dishwasher (or any other piece of technology) given uncertain number of times used per week, uncertain time saving per use, uncertain lifetime and uncertain installation costs. For example: 100 x tan(70 ~ 80) Estimate return on investment of a marketing campaign, given an uncertain number of views, uncertain click through rate, uncertain conversion rate, and uncertain spend. ","date":"16 February 2023","permalink":"/posts/unsure-calculator/","section":"Posts","summary":"Unsure-Calculator # Unsure Calculator # Created: April 12, 2020 7:07 PM URL: https://filiph.","title":"Unsure-Calculator"},{"content":"Upflow-turbocharges-your-invoices-TechCrunch # Upflow%20turbocharges%20your%20invoices%20TechCrunch%208cd6b5df2d9f444e952719e098a7be19/rawpixel-780494-unsplash.jpg Meet Upflow, a French startup that wants to help you deal with your outstanding invoices — the company first started at eFounders. Most companies currently manage invoices using Excel spreadsheets, outdated banking interfaces and unnecessary conversations. If everything I described resonates with you, Upflow wants to manage your invoices for you. Upflow%20turbocharges%20your%20invoices%20TechCrunch%208cd6b5df2d9f444e952719e098a7be19/illu-header-home.png After signing up, you can send invoices to your client and cc Upflow in your email thread. Upflow also generates banking information with the help of Treezor. This way, you can put your Upflow banking information on your invoices. Upflow%20turbocharges%20your%20invoices%20TechCrunch%208cd6b5df2d9f444e952719e098a7be19/Upflow-sketch.jpg In other words, Upflow has created a brick that sits between your company’s back office and your customers.\n","date":"16 February 2023","permalink":"/posts/upflow-turbocharges-your-invoices-techcrunch/","section":"Posts","summary":"Upflow-turbocharges-your-invoices-TechCrunch # Upflow%20turbocharges%20your%20invoices%20TechCrunch%208cd6b5df2d9f444e952719e098a7be19/rawpixel-780494-unsplash.","title":"Upflow-turbocharges-your-invoices-TechCrunch"},{"content":"Using-Docker-to-explore-Airflow-and-other-open-sou # Using Docker to explore Airflow and other open source projects # Created: January 30, 2020 12:17 PM URL: https://medium.com/@segal.levi/using-docker-to-explore-airflow-and-other-open-source-projects-e6349ffadf5a I’m in love with the idea that open source software has, and will continue to, change the world as we know it. It’s amazing how much of the society we live in today is built on top of free resources that are accessible to the public. It’s easy to take for granted, but when I try to explain the concept to a friend who knows nothing about the software world, it blows their mind. But open source software grows at an overwhelming pace, and reading about a piece a software is not the same as trying it out. Recently, I’ve been using to speed through testing out new pieces of software that I’d like to try, and since it’s worked nicely for me I thought it could help some people on here.\n","date":"16 February 2023","permalink":"/posts/using-docker-to-explore-airflow-and-other-open-sou/","section":"Posts","summary":"Using-Docker-to-explore-Airflow-and-other-open-sou # Using Docker to explore Airflow and other open source projects # Created: January 30, 2020 12:17 PM URL: https://medium.","title":"Using-Docker-to-explore-Airflow-and-other-open-sou"},{"content":"Using-Self-Joins-To-Calculate-Your-Retention-Chur # Here’s the query:\nwith monthly_activity as ( select distinct date_trunc(\u0026#39;month\u0026#39;, created_at) as month, user_id from events ) select this_month.month, count(distinct user_id) from monthly_activity this_month join monthly_activity last_month on this_month.user_id = last_month.user_id and this_month.month = add_months(last_month.month,1) group by month Our two join conditions are:\nthis_month.month = add_months(last_month.month,1): This sets up how the join works. Here’s the query: with monthly_activity as ( select distinct date_trunc(\u0026#39;month\u0026#39;, created_at) as month, user_id from events ) select last_month.month + add_months(last_month.month,1), count(distinct last_month.user_id) from monthly_activity last_month left join monthly_activity this_month on this_month.user_id = last_month.user_id and this_month.month = add_months(last_month.month,1) where this_month.user_id is null group by 1 We’ve changed our query in two ways:\nleft join: This includes every row from last month, not just the ones with users who were active this month. This leaves us with a table of users who were active last month but not this month, which once again we can group and count over! Here it is: with first_activity as ( select user_id, date(min(created_at)) as month from events group by 1 ) We’re going to include all active users each month for whom: 1. Here’s how we do it:\nwith monthly_activity as ( select distinct date_trunc(\u0026#39;month\u0026#39;, created_at) as month, user_id from events ), first_activity as ( select user_id, date(min(created_at)) as month from events group by 1 ) select this_month.month, count(distinct user_id) from monthly_activity this_month left join monthly_activity last_month on this_month.user_id = last_month.user_id and this_month.month = add_months(last_month.month,1) join first_activity on this_month.user_id = first_activity.user_id and first_activity.month != this_month.month where last_month.user_id is null group by 1 Similar to our Churn query, we employ a couple things in tandem:\nleft join: We want every activity from the current month, even if they weren’t active last month. We want only users who were active this month and not last month. Combined together, we get users who were active this month, were not active last month, and are not new: Reactivated users! ","date":"16 February 2023","permalink":"/posts/using-self-joins-to-calculate-your-retention-chur/","section":"Posts","summary":"Using-Self-Joins-To-Calculate-Your-Retention-Chur # Here’s the query:","title":"Using-Self-Joins-To-Calculate-Your-Retention-Chur"},{"content":"Using-SQL-to-find-my-best-photo-of-a-pelican-accor # Using SQL to find my best photo of a pelican according to Apple Photos # Created: May 22, 2020 8:02 PM URL: https://simonwillison.net/2020/May/21/dogsheep-photos/ According to the Apple Photos internal SQLite database, this is the most aesthetically pleasing photograph I have ever taken of a pelican: ! Using%20SQL%20to%20find%20my%20best%20photo%20of%20a%20pelican%20accor%202abe187079964d1284b0f800b448a8e2/cbfe463f1a67e37a1d36c5db44f0159ef6f86a0d64a987b129b63b52e555f1af.jpeg Here’s the SQL query that found me my best ten pelican photos:\nselect sha256, ext, uuid, date, ZOVERALLAESTHETICSCORE from photos_with_apple_metadata where uuid in ( select uuid from labels where normalized_string = \u0026#39;pelican\u0026#39; ) order by ZOVERALLAESTHETICSCORE desc limit 10 You can try it out here (with some extra datasette-json-html magic to display the actual photos). Using%20SQL%20to%20find%20my%20best%20photo%20of%20a%20pelican%20accor%202abe187079964d1284b0f800b448a8e2/a444857c4ac71ceae6af5192c8acc5ac35934ed589259136df0ed11295dbb085.jpeg\nHow this works # Apple Photos keeps photo metadata in a SQLite database.\nAn aside: Why I love Apple Photos # The Apple Photos app—on both macOS and iOS—is in my opinion Apple’s most underappreciated piece of software.\nQuerying the Apple Photos SQLite database # If you run Apple Photos on a Mac (which will synchronize with your phone via iCloud) then most of your photo metadata can be found in a database file that lives here:\n~/Pictures/Photos\\ Library.photoslibrary/database/Photos.sqlite Mine is 752MB, for aroud 40,000 photos. Here’s a full list, each one linking to my public photos sorted by that score:\nZBEHAVIORALSCORE ZFAILURESCORE ZHARMONIOUSCOLORSCORE ZIMMERSIVENESSSCORE ZINTERACTIONSCORE ZINTERESTINGSUBJECTSCORE ZINTRUSIVEOBJECTPRESENCESCORE ZLIVELYCOLORSCORE ZLOWLIGHT ZNOISESCORE ZPLEASANTCAMERATILTSCORE ZPLEASANTCOMPOSITIONSCORE ZPLEASANTLIGHTINGSCORE ZPLEASANTPATTERNSCORE ZPLEASANTPERSPECTIVESCORE ZPLEASANTPOSTPROCESSINGSCORE ZPLEASANTREFLECTIONSSCORE ZPLEASANTSYMMETRYSCORE ZSHARPLYFOCUSEDSUBJECTSCORE ZTASTEFULLYBLURREDSCORE ZWELLCHOSENSUBJECTSCORE ZWELLFRAMEDSUBJECTSCORE ZWELLTIMEDSHOTSCORE I’m not enormously impressed with the results I get from these. It took some work to figure out how to match those labels with their corresponding photos, mainly because the psi.sqlite database stores photo UUIDs as a pair of signed integers whereas the Photos.sqlite database stores a UUID string. ","date":"16 February 2023","permalink":"/posts/using-sql-to-find-my-best-photo-of-a-pelican-accor/","section":"Posts","summary":"Using-SQL-to-find-my-best-photo-of-a-pelican-accor # Using SQL to find my best photo of a pelican according to Apple Photos # Created: May 22, 2020 8:02 PM URL: https://simonwillison.","title":"Using-SQL-to-find-my-best-photo-of-a-pelican-accor"},{"content":"Using-Squarespace-Forms-with-Dropbox-to-Receive-Up # Using Squarespace Forms with Dropbox to Receive Uploaded Files — Amazing Andrea # Created: January 27, 2020 10:16 PM Tags: How To URL: https://amazingandrea.com/blog/using-squarespace-forms-with-dropbox-to-receive-uploaded-files © 2019 Andrea Buchtel All Rights Reserved | Web \u0026amp; Graphic Design by Andrea Buchtel\n","date":"16 February 2023","permalink":"/posts/using-squarespace-forms-with-dropbox-to-receive-up/","section":"Posts","summary":"Using-Squarespace-Forms-with-Dropbox-to-Receive-Up # Using Squarespace Forms with Dropbox to Receive Uploaded Files — Amazing Andrea # Created: January 27, 2020 10:16 PM Tags: How To URL: https://amazingandrea.","title":"Using-Squarespace-Forms-with-Dropbox-to-Receive-Up"},{"content":"Valentin-Perez-valentin-pd-Tweeted-okaypol-No # Valentin Perez (@valentin_pd) Tweeted: # @okaypol @NotionHQ Unfortunately the talk wasn’t recorded but I made a YouTube video with the same content plus screensharing video of my workspace, condensed into 5mins: https://t.co/0D8GixvpwQ Created: April 4, 2020 6:07 PM URL: https://twitter.com/valentin_pd/status/1217321188176752646?s=20 https://twitter.com/valentin_pd/status/1217321188176752646?s=20\n","date":"16 February 2023","permalink":"/posts/valentin-perez-valentin-pd-tweeted-okaypol-no/","section":"Posts","summary":"Valentin-Perez-valentin-pd-Tweeted-okaypol-No # Valentin Perez (@valentin_pd) Tweeted: # @okaypol @NotionHQ Unfortunately the talk wasn’t recorded but I made a YouTube video with the same content plus screensharing video of my workspace, condensed into 5mins: https://t.","title":"Valentin-Perez-valentin-pd-Tweeted-okaypol-No"},{"content":"Valuable-things-in-one-hour # Valuable things in one hour # Created: January 1, 2020 3:06 PM Tags: Self URL: https://news.ycombinator.com/item?id=21913129 Taking this also as a bit of an Ask HN: Easily the Tiny Habits habit formation regime created by Stanford researcher BJ Fogg. Quite nefarious use of psychology for advertising/“engagement”, but the plus side is you can use the same strategies to build habits you want to build. Step 1: Consider the habit you want to build, e.g. “I want to meditate 10 minutes every day” Step 2: Make it the absolute smallest possible version of itself; so small that it requires zero motivation/willpower, e.g. “I want to close my eyes and take 3 deep breaths every day” Step 3: Place this habit immediately following an existing habit, e.g. “After I brush my teeth in the morning, I will close my eyes and take 3 deep breaths” Step 4: Do this activity, and after each time you do it, reward yourself with a small celebration. I’ve used this method to pick up daily meditation, journaling, and flossing (acquired simultaneously!) I spent a month of the year just flossing 1 tooth each night (ridiculous, I know!\nPart of the trick is finding a good habit to put your new habit after. You probably already have a lot more habits than you know, since the whole point of a habit is to be automatic. ","date":"16 February 2023","permalink":"/posts/valuable-things-in-one-hour/","section":"Posts","summary":"Valuable-things-in-one-hour # Valuable things in one hour # Created: January 1, 2020 3:06 PM Tags: Self URL: https://news.","title":"Valuable-things-in-one-hour"},{"content":"Valuing-User-Subscriptions-Over-Time-with-SQL-Sise # Starting from the raw data Our primary table is a subscription table that has the company id, plan id, and the start and end dates of each plan: This data structure allows us to easily analyze some date slices of our data. For example, we can look at the number of plans started on a given date: But it doesn’t allow us to see inside of our start and end ranges, such as how many plans were active on a specific date. Since we only want a limited series of data, we can perform a check using a nested select to make sure that our dates only cover our periods with a plan.\nwith dates as ( select ( getdate()::date - row_number() over(order by true) )::date as plan_date from subscriptions ) , plan_dates as ( select plan_date from dates where plan_date \u0026gt;= ( select min(plan_start) from subscriptions ) ) Accounting for null dates # In our subscriptions table we currently have the dates where they have started and ended. We can accomplish this by using a coalesce statement to evaluate the plan_end value and inserting the current date in the event that it is null:\n, cleaned_subs as ( select company_id , plan_id , plan_start , coalesce(plan_end, getdate()::date) as plan_end from subscriptions ) Joining on inequalities # Now that we have our cleaned subscription data in one temp table and our relevant dates in another, we can join them together.\nAggregating our data # Now that our data is joined across date ranges, we can perform the next step in our analysis and aggregate the data! To get this picture of our data, we can join our aggregated data with our plan data to understand the value within each plan:\nselect plan_date , plan_id , plan , (active_subs * amount) as mrr from aggregated_data join plans on aggregated_data.plan_id = plans.id Now we are able to see that our users are in fact moving from our Starter plan to a Pro Plan over time and that the growth in revenue is promising!\n","date":"16 February 2023","permalink":"/posts/valuing-user-subscriptions-over-time-with-sql-sise/","section":"Posts","summary":"Valuing-User-Subscriptions-Over-Time-with-SQL-Sise # Starting from the raw data Our primary table is a subscription table that has the company id, plan id, and the start and end dates of each plan: This data structure allows us to easily analyze some date slices of our data.","title":"Valuing-User-Subscriptions-Over-Time-with-SQL-Sise"},{"content":"Water-Powered-Weeder-Lee-Valley-Tools # Water-Powered Weeder - Lee Valley Tools # Created: April 27, 2020 9:03 PM URL: https://www.leevalley.com/en-us/shop/garden/garden-care/weeders/10418-water-powered-weeder\n","date":"16 February 2023","permalink":"/posts/water-powered-weeder-lee-valley-tools/","section":"Posts","summary":"Water-Powered-Weeder-Lee-Valley-Tools # Water-Powered Weeder - Lee Valley Tools # Created: April 27, 2020 9:03 PM URL: https://www.","title":"Water-Powered-Weeder-Lee-Valley-Tools"},{"content":"We-re-All-Using-Airflow-Wrong-and-How-to-Fix-It # When a DAG is executed, the Worker will execute the work of each Operator, whether it is an HTTPOperator, a BigQueryOperator, or any other Operator, on the Airflow worker itself. The DAG for this workflow would look something like this: First, because each step of this DAG is a different functional task, each step is created using a different Airflow Operator. Developer after developer moved a previously-working workflow over to Airflow only to have it brought down by an issue with an Airflow Operator itself. This will happen for every Operator that it executes: This means that all Python package dependencies from each workflow will need to be installed on each Airflow Worker for Operators to be executed successfully. * When we took the time to enumerate our problems with Airflow, it was evident that the Airflow Operators, the things that were supposed to make Airflow powerful and flexible, were providing the wrong abstraction to the Airflow developer. * Here’s how the new Kubernetes Operator works: The Airflow Worker, instead of executing any work itself, spins up Kubernetes resources to execute the Operator’s work at each step. Instead of executing work on the Airflow Worker itself, the Kubernetes Operator will spin up a Kubernetes resource to execute the work (shown above).\n","date":"16 February 2023","permalink":"/posts/we-re-all-using-airflow-wrong-and-how-to-fix-it/","section":"Posts","summary":"We-re-All-Using-Airflow-Wrong-and-How-to-Fix-It # When a DAG is executed, the Worker will execute the work of each Operator, whether it is an HTTPOperator, a BigQueryOperator, or any other Operator, on the Airflow worker itself.","title":"We-re-All-Using-Airflow-Wrong-and-How-to-Fix-It"},{"content":"Weakening-the-Volcker-Rule-Is-in-Nobody-s-Interest # Weakening the Volcker Rule Is in Nobody\u0026rsquo;s Interest - Bloomberg # Created: May 20, 2020 8:41 PM URL: https://www.bloomberg.com/opinion/articles/2019-08-27/weakening-the-volcker-rule-is-in-nobody-s-interest\n","date":"16 February 2023","permalink":"/posts/weakening-the-volcker-rule-is-in-nobody-s-interest/","section":"Posts","summary":"Weakening-the-Volcker-Rule-Is-in-Nobody-s-Interest # Weakening the Volcker Rule Is in Nobody\u0026rsquo;s Interest - Bloomberg # Created: May 20, 2020 8:41 PM URL: https://www.","title":"Weakening-the-Volcker-Rule-Is-in-Nobody-s-Interest"},{"content":"Welcome-to-pdfminer-six-s-documentation-pdfmine # — pdfminer.six 20191020 documentation Created: January 28, 2020 9:01 PM Tags: Tools URL: https://pdfminersix.readthedocs.io/en/latest/index.html\nFeatures # Parse all objects from a PDF document into Python objects. Analyze and group text in a human-readable way. Extract text, images (JPG, JBIG2 and Bitmaps), table-of-contents, tagged contents and more. Support for (almost all) features from the PDF-1.7 specification Support for Chinese, Japanese and Korean CJK) languages as well as vertical writing. Support for various font types (Type1, TrueType, Type3, and CID). Installation instructions # Before using it, you must install it using Python 3.4 or newer.\n$ pip install pdfminer.six Contributing # We welcome any contributors to pdfminer.six!\n","date":"16 February 2023","permalink":"/posts/welcome-to-pdfminer-six-s-documentation-pdfmine/","section":"Posts","summary":"Welcome-to-pdfminer-six-s-documentation-pdfmine # — pdfminer.","title":"Welcome-to-pdfminer-six-s-documentation-pdfmine"},{"content":"What-is-Next-Architecture-O-Reilly # We’re already there # Today, many organizations are already thinking and developing software in consonance with Next Architecture’s foundational priorities and principles, even if they are not consciously (or conscientiously) “doing” Next Architecture. Next Architecture is likewise premised on the understanding that software (and software architecture) is not merely a digital twin of your business—i.e., its virtual complement or mirror—but is, in an essential sense, your business. Let’s briefly consider a few of the specific business benefits of using the concepts, technologies, and practices of Next Architecture to build software:\nFeature agility — You can quickly develop features and add them to your digital presence. Next Architecture challenges # While the data shows many organizations directionally adopting Next Architecture, these organizations face challenges that span across training/hiring, culture, distributed data integrity, new cost regimes, migration, managing complexity, and decomposition. For organizations addressing the move from legacy monolith architectures, the migration to Next Architecture requires a close look at what makes sense, both from a technology and cost perspective, to cleave off as microservices that can be deployed in the cloud. Next Architecture harnesses containers, service orchestration (via Kubernetes, Swarm, or similar platforms) and other commodity technologies in what could nominally be described as a microservice architecture. Next Architecture’s overarching goals are two-fold: first, it aims to deliver an improved overall service experience for customers; second, it aims to produce software architecture that is more flexible and resilient—in other words, that has a capacity for adaptability.\n","date":"16 February 2023","permalink":"/posts/what-is-next-architecture-o-reilly/","section":"Posts","summary":"What-is-Next-Architecture-O-Reilly # We’re already there # Today, many organizations are already thinking and developing software in consonance with Next Architecture’s foundational priorities and principles, even if they are not consciously (or conscientiously) “doing” Next Architecture.","title":"What-is-Next-Architecture-O-Reilly"},{"content":"What-is-the-front-controller-design-pattern # Created: June 17, 2020 4:45 PM URL: https://www.educative.io/edpresso/what-is-the-front-controller-design-pattern The front controller design pattern is used to refine the structure of an application which: 1. The front controller receives a request from the client, authenticates it, and forwards the request to the dispatcher.\nUML diagram # What%20is%20the%20front%20controller%20design%20pattern%20d9380051288b472a831d4d2b541efd19/6238711576526848\n#include using namespace std; // Views: class View1{ public: void update() { cout \u0026lt;\u0026lt; \u0026#34;Bills payed.\u0026#34; \u0026lt;\u0026lt; endl; } }; class View2{ public: void update() { cout \u0026lt;\u0026lt; \u0026#34;Username updated.\u0026#34; \u0026lt;\u0026lt; endl; } }; class View3{ public: void update() { cout \u0026lt;\u0026lt; \u0026#34;Password changed.\u0026#34; \u0026lt;\u0026lt; endl; } }; // Handlers: class Handler1{ private: View1* view1; public: Handler1() { view1 = new View1(); } void updateView() { view1-\u0026gt;update(); } }; class Handler2{ private: View2* view2; public: Handler2() { view2 = new View2(); } void updateView() { view2-\u0026gt;update(); } }; class Handler3{ private: View3* view3; public: Handler3() { view3 = new View3(); } void updateView() { view3-\u0026gt;update(); } }; // Dispatcher: class Dispatcher{ private: Handler1* handler1; Handler2* handler2; Handler3* handler3; public: Dispatcher(){ handler1 = new Handler1(); handler2 = new Handler2(); handler3 = new Handler3(); } void dispatch(string req) { if(req == \u0026#34;pay bills\u0026#34;) handler1-\u0026gt;updateView(); else if(req == \u0026#34;update username\u0026#34;) handler2-\u0026gt;updateView(); else if(req == \u0026#34;change password\u0026#34;) handler3-\u0026gt;updateView(); else cout \u0026lt;\u0026lt; \u0026#34;Invalid request!\u0026#34; \u0026lt;\u0026lt; endl; } }; // Controller: class Controller{ private: Dispatcher* myDispatcher; bool authUser(string user) { if(user == \u0026#34;authentic\u0026#34;) return true; else return false; } public: Controller(){ myDispatcher = new Dispatcher(); } void forwardRequest(string u, string req) { if(authUser(u)) myDispatcher-\u0026gt;dispatch(req); } }; int main(){ Controller ctrl; ctrl.forwardRequest(\u0026#34;authentic\u0026#34;, \u0026#34;pay bills\u0026#34;); return 0; } ","date":"16 February 2023","permalink":"/posts/what-is-the-front-controller-design-pattern/","section":"Posts","summary":"What-is-the-front-controller-design-pattern # Created: June 17, 2020 4:45 PM URL: https://www.","title":"What-is-the-front-controller-design-pattern"},{"content":"What-procurement-managers-should-expect-from-a-bu # In the world of supply chain, this is known as the bullwhip effect — and we\u0026rsquo;re seeing it play out in real-time a the coronavirus leads consumers to panic buying and forces hospitals to scramble for supplies. What%20procurement%20managers%20should%20expect%20from%20a%20\u0026rsquo;bu%20c577c33b950843b1906f93b5721364cd/Online_grocery_orders_surge_with_spread_of_coronavirus_4.svg \u0026ldquo;Producers are definitely ramping up to help retailers restock their shelves and make sure there aren\u0026rsquo;t any outages of whatever it is there has been a run on,\u0026rdquo; Pete Guarraia, the global head of supply chain for Bain and Company, told Supply Chain Dive in an interview. Order batching: When demand does not change, but multiple retailers place new orders on different days then it can appear as a change in demand that can be hard for manufacturers to interpret. Subscribe to Supply Chain Dive: Both ends of the supply chain will need to make responsible decisions. A central part of the bullwhip theory relies on the expectation that there is a next step in the supply chain — another company to interpret the demand signal. \u0026ldquo;What I\u0026rsquo;ve been spending all morning doing,\u0026rdquo; Rathke said earlier this week, \u0026ldquo;is assessing to what extent our clients constitute essential businesses, to what extent their suppliers also constitute essential businesses, and their customers constitute essential businesses, and making sure everybody along the supply chain of an essential business will operate to the extent necessary to service the essential business.\u0026rdquo; Whether a procurement manager is trying to figure out the best way to buy surgical masks or toilet paper, one thing is for sure: it is a challenging environment for supply chain professionals to operate in.\n","date":"16 February 2023","permalink":"/posts/what-procurement-managers-should-expect-from-a-bu/","section":"Posts","summary":"What-procurement-managers-should-expect-from-a-bu # In the world of supply chain, this is known as the bullwhip effect — and we\u0026rsquo;re seeing it play out in real-time a the coronavirus leads consumers to panic buying and forces hospitals to scramble for supplies.","title":"What-procurement-managers-should-expect-from-a-bu"},{"content":"What-s-a-design-pattern # What’s a design pattern? # Created: June 12, 2020 2:25 PM URL: https://refactoring.guru/design-patterns/what-is-pattern\n","date":"16 February 2023","permalink":"/posts/what-s-a-design-pattern/","section":"Posts","summary":"What-s-a-design-pattern # What’s a design pattern?","title":"What-s-a-design-pattern"},{"content":"WhatIsaBliki # WhatIsaBliki # Created: June 7, 2020 3:15 PM URL: https://martinfowler.com/bliki/WhatIsaBliki.html I\u0026rsquo;ve been watching the blog scene develop for a while, and it\u0026rsquo;s impossible to not want to join in. For a start the name, as my colleague Mike Two puts it, \u0026ldquo;blog sounds like something I should pay a physician to remove\u0026rdquo;. Beyond the name, however, there\u0026rsquo;s the very ephemeral nature of blog postings. Short bursts of writing that might be interesting when they are read - but quickly age. And I do like the fact that blogs make it easy to see what\u0026rsquo;s really changed recently - thanks to the hooks into RSS and aggregators. So I decided I wanted something that was a cross between a wiki and a blog - which Ward Cunningham immediately dubbed a bliki. Like a blog, it allows me to post short thoughts when I have them.\n","date":"16 February 2023","permalink":"/posts/whatisabliki/","section":"Posts","summary":"WhatIsaBliki # WhatIsaBliki # Created: June 7, 2020 3:15 PM URL: https://martinfowler.","title":"WhatIsaBliki"},{"content":"Why-procrastination-is-about-managing-emotions-no # Why procrastination is about managing emotions, not time # Created: January 23, 2020 10:31 AM Tags: Productivity, Self URL: about:reader?url=https%3A%2F%2Fwww.bbc.com%2Fworklife%2Farticle%2F20200121-why-procrastination-is-about-managing-emotions-not-time\n","date":"16 February 2023","permalink":"/posts/why-procrastination-is-about-managing-emotions-no/","section":"Posts","summary":"Why-procrastination-is-about-managing-emotions-no # Why procrastination is about managing emotions, not time # Created: January 23, 2020 10:31 AM Tags: Productivity, Self URL: about:reader?","title":"Why-procrastination-is-about-managing-emotions-no"},{"content":"Why-Robinhood-uses-Airflow-Robinhood-Engineering # We started off with using cron to schedule these jobs but with their growing number and complexity, it became increasingly challenging for us to manage them using cron:\nManaging dependencies between jobs was difficult. Dependency Management # Airflow uses Operators as the fundamental unit of abstraction to define tasks, and uses a DAG (Directed Acyclic Graph) to define workflows using a set of operators. It provides historical views of the jobs and tools to control the state of jobs — such as kill a running job or manually re-running a job. We also use Airflow sensors to run jobs right after market close, while handling market half-days.\nThe Scheduler works separately for scheduled jobs and backfill jobs. Airflow was built primarily for data batch processing due to which the Airflow designers made a decision to always schedule jobs for the previous interval. Hence, a job scheduled to run daily at midnight will pass in the execution date “2016–12–31 00:00:00” to the job’s context when run on “2017–01–01 00:00:00”. ","date":"16 February 2023","permalink":"/posts/why-robinhood-uses-airflow-robinhood-engineering/","section":"Posts","summary":"Why-Robinhood-uses-Airflow-Robinhood-Engineering # We started off with using cron to schedule these jobs but with their growing number and complexity, it became increasingly challenging for us to manage them using cron:","title":"Why-Robinhood-uses-Airflow-Robinhood-Engineering"},{"content":"Why-you-should-have-a-startup-accountability-email # Why you should have a startup accountability email list | Dan Moore! # Created: March 8, 2020 2:52 PM URL: http://www.mooreds.com/wordpress/archives/3324 If you\u0026rsquo;re new here, you may want to subscribe to my RSS feed or get my posts in your inbox. I think if you are starting a company, you should absolutely have an email list of interested folks. Mailchimp makes this free. You can email to this list:\nProgress you’ve made Help you need (hiring, funding, finding customers, anything else) Things you are proud of Things you are bummed about Interesting topics in your space We used this at a startup I was at a couple of years ago and found it super useful. You may share financials or pretty vulnerable asks on this list, so I’d avoid anyone who you wouldn’t ask for assistance from face to face. But if you are chatting with someone at a party and they are interested in and seem like they could help your startup, just say “Hey, I send out a monthly newsletter about our progress, would you mind if I added you?” ","date":"16 February 2023","permalink":"/posts/why-you-should-have-a-startup-accountability-email/","section":"Posts","summary":"Why-you-should-have-a-startup-accountability-email # Why you should have a startup accountability email list | Dan Moore!","title":"Why-you-should-have-a-startup-accountability-email"},{"content":"Won-t-You-Be-My-Neighbor-Quickly-Finding-Who-is-Ne # If we want to find the three friends who were closest to us on October 1, 2012 between 7:00am and 9:00am, we could construct a query like this:\nSELECT visitor, visited_at, geocode FROM visits WHERE visited_at BETWEEN \u0026#39;2012-10-01 07:00\u0026#39; AND \u0026#39;2012-10-01 09:00\u0026#39; ORDER BY POINT(40.7127263,-74.0066592) geocode LIMIT 3; The “K-nearest neighbor” portion of the query can be seen in ORDER BY POINT(40.7127263,-74.0066592) geocode LIMIT 3 which is another of saying “order by the shortest distance between my current location and all the other recorded locations, but find the 3 closest locations to me.” How does this perform? Let’s pretend that we still have the covering indexes in place from the previous article:\nEXPLAIN ANALYZE SELECT visitor, visited_at, geocode FROM visits WHERE visited_at BETWEEN \u0026#39;2012-10-01 07:00\u0026#39; AND \u0026#39;2012-10-01 09:00\u0026#39; ORDER BY POINT(40.7127263,-74.0066592) geocode LIMIT 3; Limit (cost=53755.18..53755.54 rows=3 width=48) (actual time=120.890..128.781 rows=3 loops=1) -\u0026gt; Gather Merge (cost=53755.18..53794.45 rows=328 width=48) (actual time=120.889..128.778 rows=3 loops=1) Workers Planned: 4 Workers Launched: 4 -\u0026gt; Sort (cost=52755.12..52755.32 rows=82 width=48) (actual time=115.623..115.625 rows=2 loops=5) Sort Key: ((\u0026#39;(40.7127263,-74.0066592)\u0026#39;::point geocode)) Sort Method: top-N heapsort Memory: 25kB Worker 0: Sort Method: top-N heapsort Memory: 25kB Worker 1: Sort Method: top-N heapsort Memory: 25kB Worker 2: Sort Method: top-N heapsort Memory: 25kB Worker 3: Sort Method: quicksort Memory: 25kB -\u0026gt; Parallel Seq Scan on visits (cost=0.00..52754.06 rows=82 width=48) (actual time=65.256..115.476 rows=50 loops=5) Filter: ((visited_at \u0026gt;= \u0026#39;2012-10-01 07:00:00-04\u0026#39;::timestamp with time zone) AND (visited_at \u0026lt;= \u0026#39;2012-10-01 09:00:00-04\u0026#39;::timestamp with time zone)) Rows Removed by Filter: 805600 Planning Time: 0.112 ms Execution Time: 128.808 ms Looking at this query plan, PostgreSQL determined that none of the indexes could be used, and does a full (parallelized) sequential scan on the visits table in order to find the 3 closest people. You can use a KNN-GiST index simply by creating a GiST index on a supported data type, which in this case, is the geocode column:\nCREATE INDEX visits_geocode_gist_idx ON visits USING gist(geocode); VACUUM ANALYZE visits; To demonstrate its power, let’s see what happens if we try to find the 3 nearest points to a given location:\nEXPLAIN ANALYZE SELECT visitor, visited_at, geocode FROM visits ORDER BY POINT(40.7127263,-74.0066592) geocode LIMIT 3; Limit (cost=0.41..0.73 rows=3 width=48) (actual time=0.237..0.244 rows=3 loops=1) -\u0026gt; Index Scan using visits_geocode_gist_idx on visits (cost=0.41..423200.97 rows=4028228 width=48) (actual time=0.236..0.242 rows=3 loops=1) Order By: (geocode \u0026#39;(40.7127263,-74.0066592)\u0026#39;::point) Planning Time: 0.089 ms Execution Time: 0.265 ms Wow! Now let’s try running our original query to find who is closest to us on October 1, 2012 between 7am and 9am and see if this index speeds up:\nEXPLAIN ANALYZE SELECT visitor, visited_at, geocode FROM visits WHERE visited_at BETWEEN \u0026#39;2012-10-01 07:00\u0026#39; AND \u0026#39;2012-10-01 09:00\u0026#39; ORDER BY POINT(40.7127263,-74.0066592) geocode LIMIT 3; Limit (cost=0.41..4012.19 rows=3 width=48) (actual time=184.327..184.332 rows=3 loops=1) -\u0026gt; Index Scan using visits_geocode_gist_idx on visits (cost=0.41..433272.35 rows=324 width=48) (actual time=184.326..184.330 rows=3 loops=1) Order By: (geocode \u0026#39;(40.7127263,-74.0066592)\u0026#39;::point) Filter: ((visited_at \u0026gt;= \u0026#39;2012-10-01 07:00:00-04\u0026#39;::timestamp with time zone) AND (visited_at \u0026lt;= \u0026#39;2012-10-01 09:00:00-04\u0026#39;::timestamp with time zone)) Rows Removed by Filter: 499207 Planning Time: 0.140 ms Execution Time: 184.361 ms No luck: in this case, because we need also need to filter out our data for the given date/time range, PostgreSQL is unable to take advantage of the KNN-GiST index. You can install this extension by executing the following:\nCREATE EXTENSION btree_gist; Before we try creating the multicolumn index, first, let’s drop the previous index:\nDROP INDEX visits_geocode_gist_idx; Now, let’s create the multicolumn index on (visited_at,geocode):\nCREATE INDEX visits_visited_at_geocode_gist_idx ON visits USING gist(visited_at, geocode); VACUUM ANALYZE visits; What happens to the execution time?\nEXPLAIN ANALYZE SELECT visitor, visited_at, geocode FROM visits WHERE visited_at BETWEEN \u0026#39;2012-10-01 07:00\u0026#39; AND \u0026#39;2012-10-01 09:00\u0026#39; ORDER BY POINT(40.7127263,-74.0066592) geocode LIMIT 3; Limit (cost=0.41..12.64 rows=3 width=48) (actual time=0.047..0.049 rows=3 loops=1) -\u0026gt; Index Scan using visits_visited_at_geocode_gist_idx on visits (cost=0.41..1348.69 rows=331 width=48) (actual time=0.046..0.048 rows=3 loops=1) Index Cond: ((visited_at \u0026gt;= \u0026#39;2012-10-01 07:00:00-04\u0026#39;::timestamp with time zone) AND (visited_at \u0026lt;= \u0026#39;2012-10-01 09:00:00-04\u0026#39;::timestamp with time zone)) Order By: (geocode \u0026#39;(40.7127263,-74.0066592)\u0026#39;::point) Planning Time: 0.133 ms Execution Time: 0.068 ms Excellent! They are not without cost: KNN-GiST indexes are larger than regular GiST indexes, but the speedup KNN-GiST indexes provide is significantly (if not orders of magnitude) larger than the additional space and should be in your toolbox for any location-aware application you build.\n","date":"16 February 2023","permalink":"/posts/won-t-you-be-my-neighbor-quickly-finding-who-is-ne/","section":"Posts","summary":"Won-t-You-Be-My-Neighbor-Quickly-Finding-Who-is-Ne # If we want to find the three friends who were closest to us on October 1, 2012 between 7:00am and 9:00am, we could construct a query like this:","title":"Won-t-You-Be-My-Neighbor-Quickly-Finding-Who-is-Ne"},{"content":"YAML-CUE # YAML | CUE # Created: October 26, 2022 1:04 AM URL: https://cuelang.org/docs/integrations/yaml/\nYAML # Intro # Unlike with JSON, CUE is not a superset of YAML.\nCommand line tool # Validate YAML files # The vet command of the cue command line tool can validate YAML files using a CUE schema.\n$ cue vet ranges.yaml check.cue max: invalid value 5 (out of bound \u0026gt;10): ./check.cue:2:16 ./ranges.yaml:5:7 Import YAML # The import command of the cue command line tool can convert YAML files into CUE.\nYAML in CUE # The encoding/yaml builtin package provides various builtins to parse, generate, or validate YAML from within CUE. #Phrases: { phrases: [string]: #Phrase #Phrase: { lang: #LanguageTag text: !=\u0026quot;\u0026quot; attribution? phrases: yaml.Validate(#Phrases) phrases: \u0026quot;\u0026quot;\u0026quot; phrases:\nA quote from Mark Twain. # $ cue vet dim.cue phrases: error in call to encoding/yaml.Validate: conflicting values false and LanguageTag (mismatched types bool and string): ./dim.cue:18:10 Create # The builtin encoding/yaml.Marshal generates YAML from within CUE.\n","date":"16 February 2023","permalink":"/posts/yaml-cue/","section":"Posts","summary":"YAML-CUE # YAML | CUE # Created: October 26, 2022 1:04 AM URL: https://cuelang.","title":"YAML-CUE"},{"content":"YC-s-Series-A-Guide # YC’s Series A Guide # Created: February 26, 2020 12:35 PM URL: https://blog.ycombinator.com/ycs-series-a-guide/ ! YC-logo-1200-1.png We started YC’s Series A Program two years ago to fix a problem faced by every startup raising an A: VCs understand how Series A’s work and founders do not. While we couldn’t work directly with every company in the world trying to raise an A, we realized that publicly distributing our learnings would help all founders. We’ve published some of what we learned:\nIn Standard and Clean Series A Term Sheet, we analyzed dozens of term sheets to create the fairest one possible. For instance, we learned that: Founders on average have to meet with 30 investors to produce a single term sheet. The guide works best in conjunction with the internal programs and resources we’ve built out for YC companies. Our hope is that the Guide will remove information asymmetry from the Series A process, and help level the playing field between founders and VCs. ","date":"16 February 2023","permalink":"/posts/yc-s-series-a-guide/","section":"Posts","summary":"YC-s-Series-A-Guide # YC’s Series A Guide # Created: February 26, 2020 12:35 PM URL: https://blog.","title":"YC-s-Series-A-Guide"},{"content":"YCAdvice # YCAdvice # Created: March 28, 2020 9:22 AM URL: https://ycadvice.com/ Kevin Hale on how investors think about ideas. Prior YC Kevin was the cofounder of Wufoo, which was funded by Y Combinator in 2006 and acquired by SurveyMonkey in 2011. https://twitter.com/ilikevests Learn more about YC and apply for funding here: https://www.ycombinator.com/apply/ Craig Cannon is the Director of Marketing at Y Combinator. He usually hosts the YC podcast but today is the guest on this episode about podcasting. https://twitter.com/craigcannon Adora Cheung is a Partner at YC. https://twitter.com/nolimits A quick word about competitors: competitors are a startup ghost story. Worry instead about all of your internal problems. If you fail, it will very likely be because you failed to make a great product and/or failed to make a great company.\n","date":"16 February 2023","permalink":"/posts/ycadvice/","section":"Posts","summary":"YCAdvice # YCAdvice # Created: March 28, 2020 9:22 AM URL: https://ycadvice.","title":"YCAdvice"},{"content":"You-re-all-calculating-churn-rates-wrong-The-Sta # If the churn probability gets lower the longer the customer has been subscribed, you could model that as c/(t+1), where t is the timestep (e.g. number of days the customer has been subscribed), and c is some constant. In this case, this implies that customer lifetimes comes from a Lomax distribution. Keep in mind, in each of the examples below we simulate lifetimes from the same customer lifetime distribution, and this distribution does not change over time. Multiply this by what you make per customer per day, and you have your Customer Lifetime Value. The distributions will both have the same median customer lifetime, but one has a larger mean lifetime than the other. Keep in mind that the typical customers (found by the median) stick around equally long in either company, but it’s the rare long term customers that shift the Lifetime Customer Value massively in favor of the orange company. ](http://fooledbyrandomness.com/DarwinCollege.pdf) So if you have Pareto 80/20 distributed customer lifetimes, **you need 100 billion customers before the sample mean lifetime is accurate.\n","date":"16 February 2023","permalink":"/posts/you-re-all-calculating-churn-rates-wrong-the-sta/","section":"Posts","summary":"You-re-all-calculating-churn-rates-wrong-The-Sta # If the churn probability gets lower the longer the customer has been subscribed, you could model that as c/(t+1), where t is the timestep (e.","title":"You-re-all-calculating-churn-rates-wrong-The-Sta"},{"content":"Your-guide-to-creating-a-website-wireframe-Cacoo # This guide is useful for:\nAnyone interested in learning more about UX design, UX Designers just starting their careers, UX Designers struggling to deliver quality wireframes, and UX design veterans wanting to brush up on the fundamentals and stay up-to-date on new trends. Achieving this outcome requires an effective design process and a skilled UX Designer who understands how to create the right type of wireframe with the right design elements at the right time. Types of wireframes # People use many terms to describe wireframes, but essentially there are three types:\nContent wireframes, Annotated wireframes, and Interactive wireframes. By removing any graphic design from these initial wireframes, the UX Designer can focus on what matters most: laying out content in a way that best achieves business goals and user needs while keeping elements consistent across different pages and devices. Interactive wireframes # Sometimes called prototypes, interactive wireframes require digital design tools that allow for the addition of interactivity into your wireframes to test with users. Here are a few examples of common design processes\nContent Wireframe \u0026gt; Annotated Wireframe \u0026gt; Prototype \u0026gt; Code Content Wireframe \u0026gt; Annotated Wireframe \u0026gt; Lo-fi Wireframe \u0026gt; Prototype \u0026gt; Code Content Wireframe \u0026gt; Annotated Wireframe \u0026gt; Hi-fi Mockup \u0026gt; Prototype \u0026gt; Code Content Wireframe \u0026gt; Annotated Wireframe \u0026gt; Lo-fi Wireframe \u0026gt; Hi-fi Mockup \u0026gt; Prototype \u0026gt; Code Content Wireframe \u0026gt; Code There are many common objects and symbols used within wireframing templates. This will include: Interaction feedback, i.e. information that tells the user what happened or what will happen next, such as: Validation responses for forms Intermediary messages and modals Interaction states, i.e. potential states for all UI components, such as: Dropdown lists Titles, labels, and names Dynamic content Icons with numbers Form fields File uploads Gestures, i.e. the way your users will physically interact with your UI on a given device, such as: Click Double-click Right-click Swipe or flick Pinch and spread Press Hover Drag and drop Keyboard input Keyboard shortcuts Cross-device interactions, i.e. gestures that may not translate to different devices: Hovering Dragging and dropping Uploading files User types, i.e. what different users see based on their previous interactions with the page, such as: New users Guests First-time visitors Returning visitors Existing or logged-in users Admins and super-users As we stated earlier in this guide, UX and UI often overlap. ","date":"16 February 2023","permalink":"/posts/your-guide-to-creating-a-website-wireframe-cacoo/","section":"Posts","summary":"Your-guide-to-creating-a-website-wireframe-Cacoo # This guide is useful for:","title":"Your-guide-to-creating-a-website-wireframe-Cacoo"},{"content":"","date":"1 January 0001","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"1 January 0001","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"}]